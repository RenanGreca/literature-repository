[
    {
        "type": "primary",
        "year": "2016",
        "authors": "Srikanth, Hema; Hettiarachchi, Charitha; Do, Hyunsook",
        "author_keys": [
            "srikanth_hema",
            "hettiarachchi_charitha",
            "do_hyunsook"
        ],
        "title": "Requirements Based Test Prioritization Using Risk Factors",
        "bibtex": "srikanth_requirements_2016",
        "abstract": "Context Software testing is an expensive and time-consuming process. Software engineering teams are often forced to terminate their testing efforts due to budgetary and time constraints, which inevitably lead to long term issues with quality and customer satisfaction. Test case prioritization (TCP) has shown to improve test effectiveness. Objective The results of our prior work on requirements-based test prioritization showed improved rate of fault detection on industrial projects; the customer priority (CP) and the fault proneness (FP) were the biggest contributing factors to test effectiveness. The objective of this paper is to further investigate these two factors and apply prioritization based on these factors in a different domain: an enterprise level cloud application. We aim to provide an effective prioritization scheme that practitioners can implement with minimum effort. The other objective is to compare the results and the benefits of these two factors with two risk-based prioritization approaches that extract risks from the system requirements categories. Method Our approach involved analyzing and assigning values to each requirement based on two important factors, CP and FP, so that the test cases for high-value requirements are prioritized earlier for execution. We also proposed two requirements-based TCP approaches that use risk information of the system. Results Our results indicate that the use of CP and FP can improve the effectiveness of TCP. The results also show that the risk-based prioritization can be effective in improving the TCP. Conclusion We performed an experiment on an enterprise cloud application to measure the fault detection rate of different test suites that are prioritized based on CP, FP, and risks. The results depict that all approaches outperform the random prioritization approach, which is prevalent in the industry. Furthermore, the proposed approaches can easily be used in the industry to address the schedule and budget constraints at the testing phase.",
        "published_in": "Information and Software Technology",
        "publisher": "Elsevier",
        "doi": "10.1016/j.infsof.2015.09.002",
        "date": "2015-09-26",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "IBM analytics application \n\nIndustrial proprietary, medium scale (1700+ TCs)",
        "prog_language": "Unclear",
        "ind_partner": "IBM (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Requirements-based",
        "info_approach": "Requirements-based",
        "alg_approach": "",
        "metrics": "APFD",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Apply different levels of severity to faults."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Noor, Tanzeem Bin; Hemmati, Hadi",
        "author_keys": [
            "noor_tanzeem_bin",
            "hemmati_hadi"
        ],
        "title": "A similarity-based approach for test case prioritization using historical failure data",
        "bibtex": "noor_similarity-based_2016",
        "abstract": "Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures. Â© 2015 IEEE.",
        "published_in": "2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)",
        "publisher": "IEEE",
        "doi": "10.1109/ISSRE.2015.7381799",
        "date": "2016-01-14",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Defects4J (up to 7927 TCs)\n\nResearch dataset, large scale (albeit using test method as granularity)",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Similarity-based (BC, HD, ED)",
        "info_approach": "Test code",
        "alg_approach": "Similarity / distance-based",
        "metrics": "Tests till first fault (%), method coverage, size of testcase",
        "effe_metrics": "Coverage Effectiveness (CE), Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "\"In the future, we will try to improve the similarity function by abstracting the method sequence calls into a state model. Moreover, we will also assess the impact of run-time parametrization on the test case inputs. We are also interested to build an automatic test generation tool that can generate high-quality tests, using this new quality metric.\""
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Schwartz, Amanda; Do, Hyunsook",
        "author_keys": [
            "schwartz_amanda",
            "do_hyunsook"
        ],
        "title": "Cost-effective regression testing through adaptive test prioritization strategies",
        "bibtex": "schwartz_cost-effective_2016",
        "abstract": "We propose two new ATP (Adaptive Test Prioritization) strategies.We \nconduct an empirical study investigating existing and new ATP \nstrategies.We provide a statistical analysis examining all ATP \nstrategies proposed.Our findings show that FESART is the most consistent\n cost-effective ATP strategy. Regression testing is an important part of\n the software development life cycle. It is also very expensive. Many \ndifferent techniques have been proposed for reducing the cost of \nregression testing. However, research has shown that the effectiveness \nof different techniques varies under different testing environments and \nsoftware change characteristics. In prior work, we developed strategies \nto investigate ways of choosing the most cost-effective regression \ntesting technique for a particular regression testing session. In this \nwork, we empirically study the existing strategies presented in prior \nwork as well as develop two additional Adaptive Test Prioritization \n(ATP) strategies using fuzzy analytical hierarchy process (AHP) and the \nweighted sum model (WSM). We also provide a comparative study examining \neach of the ATP strategies presented to date. This research will provide\n researchers and practitioners with strategies to utilize in regression \ntesting plans as well as provide data to use when deciding which of the \nstrategies would best fit their testing needs. The empirical studies \nprovided in this research show that utilizing these strategies can \nimprove the cost-effectiveness of regression testing.",
        "published_in": "Journal of Systems and Software",
        "publisher": "Elsevier",
        "doi": "10.1016/j.jss.2016.01.018",
        "date": "2016-01-27",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "SIR (up to 912 TCs)\n\nResearch dataset, medium scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Approach is adaptively chosen for each software and even version using some decision support system",
        "info_approach": "",
        "alg_approach": "Bloom filter or window-based",
        "metrics": "Eelative cost-benefit (in dollars) among compared TCP techniques",
        "effe_metrics": "Fault Detection Capability, Cost-benefit model",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Expand to other techniques, and to larger programs."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Hirzel, Matthias; Klaeren, Herbert",
        "author_keys": [
            "hirzel_matthias",
            "klaeren_herbert"
        ],
        "title": "Graph-walk-based selective regression testing of web applications created with Google web toolkit",
        "bibtex": "hirzel_graph-walk-based_2016",
        "abstract": "Modern web applications are usually based on JavaScript. Due to its loosely typed, dynamic nature, test execution is time expensive and costly. Techniques for regression testing and fault-localization as well as frameworks like the Google Web Toolkit (GWT) ease the develop- ment and testing process, but still require approaches to reduce the testing effort. In this paper, we investigate the efficiency of a spe- cialized, graph-walk based selective regression testing technique that aims to detect code changes on the client side in order to determine a reduced set of web tests. To do this, we analyze web applications created with GWT on different precision levels and with varying looka- heads. We examine how these parameters affect the localization of client-side code changes, run time, memory consumption and the num- ber of web tests selected for re-execution. In addition, we propose a dynamic heuristics which targets an analysis that is as exact as possible while reducing memory consumption. The results are partially appli- cable on non-GWT applications. In the context of web applications,we see that the efficiency relies to a great degree on both the structure of the application and the code modifications, which is why we propose further measures tailored to the results of our approach.",
        "published_in": "Software Engineering Workshops 2016",
        "publisher": "CEUR",
        "doi": "http://ceur-ws.org/Vol-1559/paper05.pdf",
        "date": "2016-02-17",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "FALSE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Hupa (32 TCs) and Meisterplan (104 TCs)\n\nOpen-source, small scale\nIndustry proprietary, small scale",
        "prog_language": "JavaScript",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package and Eclipse Plug-in, well documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/MH42/srt-for-web-apps",
        "approach": "Graph-based",
        "info_approach": "",
        "alg_approach": "Graph-based",
        "metrics": "Time/memory consumption, selection percentage",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "Execution time, Memory usage",
        "other_metrics": "",
        "open_challenges": "Try to further reduce selection percentage."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Lu, Yafeng; Lou, Yiling; Cheng, Shiang; Zhang, Lingming; Hao, Dan; Zhou, Yangfan; Zhang, Lu",
        "author_keys": [
            "lu_yafeng",
            "lou_yiling",
            "cheng_shiang",
            "zhang_lingming",
            "hao_dan",
            "zhou_yangfan",
            "zhang_lu"
        ],
        "title": "How does regression test prioritization perform in real-world software evolution?",
        "bibtex": "lu_how_2016",
        "abstract": "In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.",
        "published_in": "ICSE '16: Proceedings of the 38th International Conference on Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/2884781.2884874",
        "date": "2016-05-14",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java programs (up to 5269 TCs)\n\nOpen-source, small to large scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, poorly documented",
        "put_practice": "FALSE",
        "suppl_url": "https://personal.utdallas.edu/~lxz144130/icse16support.html",
        "approach": "greedy, search-based, adaptive, time-aware",
        "info_approach": "Cost-aware",
        "alg_approach": "Search-based, Greedy",
        "metrics": "APFD",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Consider other metrics"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "VÃ¶st, Sebastian; Wagner, Stefan",
        "author_keys": [
            "vost_sebastian",
            "wagner_stefan"
        ],
        "title": "Trace-based test selection to support continuous integration in the automotive industry",
        "bibtex": "vost_trace-based_2016",
        "abstract": "System testing in the automotive industry is a very expensive and time-consuming task of growing importance, because embedded systems in the domain are distributed over numerous controllers (ECUs). Modern software development techniques such as continuous integration require regular, repeated and fast testing. To achieve this in the automotive domain, test suites for a specific software change must be tailored. We propose a novel test selection technique for system-level functions in the automotive industry based on component and communication models. The idea is to follow input and output signals that are used in the testing steps through the ECUs implementing a function. We select only those tests for a planned integration in which at least one of the signals sent in its steps is processed by the ECU that was changed and thus triggered the integration. The technique is well-suited for black-box testing since it requires only the full test suite specification and the system architecture. We applied the technique to a test suite of the Active Cruise Control function at BMW Group in the context of hardware-in-the-loop system testing and found the possible reduction rates to be 82.3% on average in comparison to the full test suite. Possible future work includes the evaluation with a wider set of functions, the evaluation of the fault detection rate, further automation and combination with other test selection techniques.",
        "published_in": "CSED '16: Proceedings of the International Workshop on Continuous Software Evolution and Delivery",
        "publisher": "ACM",
        "doi": "10.1145/2896941.2896951",
        "date": "2016-05-14",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "BMW adaptive cruise control (186 TCs)\n\nindustrial proprietary, small scale",
        "prog_language": "\"a semi-formal language based on Keywords\"",
        "ind_partner": "BMW (Germany)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Trace-based",
        "info_approach": "Trace-based",
        "alg_approach": "",
        "metrics": "Selection rate",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Expand to include other ECU libraries; automate more steps"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Wang, Shuai; Ali, Shaukat; Yue, Tao; Bakkeli, Oyvind; Liaaen, Marius",
        "author_keys": [
            "wang_shuai",
            "ali_shaukat",
            "yue_tao",
            "bakkeli_oyvind",
            "liaaen_marius"
        ],
        "title": "Enhancing test case prioritization in an industrial setting with resource awareness and multi-objective search",
        "bibtex": "wang_enhancing_2016",
        "abstract": "Test case prioritization is an essential part of test execution systems for large organizations developing software systems in the context that their software versions are released very frequently. They must be tested on a variety of compatible hardware with different configurations to ensure correct functioning of a software version on a compatible hardware. In practice, test case execution must not only execute cost-effective test cases in an optimal order, but also optimally allocate required test resources, in order to deliver high quality software releases. To optimize the current test execution system for testing software releases developed for Videoconferencing Systems (VCSs) at Cisco, Norway, in this paper, we propose a resource-aware multi-objective optimization solution with a fitness function defined based on four cost-effectiveness measures. In this context, a set of software releases must be tested on a set of compatible VCS hardware (test resources) by executing a set of cost-effective test cases in an optimal order within a given test cycle constrained by maximum allowed time budget and maximum available test resources. We empirically evaluated seven search algorithms regarding their performance and scalability by comparing with the current practice (random ordering (RO)). The results show that the proposed solution with the best search algorithm (i.e., Random-Weighted Genetic Algorithm) improved the current practice by reducing on average 40.6% of time for test resource allocation and test case execution, improved test resource usage on average by 37.9% and fault detection on average by 60%.",
        "published_in": "ICSE '16: Proceedings of the 38th International Conference on Software Engineering Companion",
        "publisher": "ACM",
        "doi": "10.1145/2889160.2889240",
        "date": "2016-05-14",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Cisco videoconferencing system (305 TCs)\n\nIndustrial proprietary, small scale",
        "prog_language": "Unclear",
        "ind_partner": "Cisco (Norway)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "MAYBE",
        "suppl_url": "",
        "approach": "resource-aware, multi-objective search",
        "info_approach": "Cost-aware",
        "alg_approach": "Search-based",
        "metrics": "Number of test cases, resource usage, fault detection capability, total time, prioritization density",
        "effe_metrics": "Fault Detection Capability, Fault detection within a budget",
        "effi_metrics": "Execution time, Total/End-to-end time, Memory usage",
        "other_metrics": "",
        "open_challenges": "Further industrial case studies; survey with Cisco engineers for validation and application of approach."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Srikanth, Hema; Cashman, Mikaela; Cohen, Myra B.",
        "author_keys": [
            "srikanth_hema",
            "cashman_mikaela",
            "cohen_myra_b"
        ],
        "title": "Test Case Prioritization of Build Acceptance Tests for an Enterprise Cloud Application",
        "bibtex": "srikanth_test_2016",
        "abstract": "The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize BAT tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't.",
        "published_in": "Journal of Systems and Software",
        "publisher": "Elsevier",
        "doi": "10.1016/j.jss.2016.06.017",
        "date": "2016-06-16",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "IBM cloud application (1000+ TCs)\n\nIndustrial proprietary, medium scale",
        "prog_language": "Unclear",
        "ind_partner": "IBM (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "3 history-based TCP heuristics.",
        "info_approach": "History-based",
        "alg_approach": "",
        "metrics": "APFD and APFDc",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": " - investigate the use of in-house defects when ï¬eld failures data is not available.\n - evaluate systems where running times of individual test cases are different."
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Blondeau, Vincent; Etien, Anne; Anquetil, Nicolas; Cresson, Sylvain; Croisy, Pascal; Ducasse, StÃ©phane",
        "author_keys": [
            "blondeau_vincent",
            "etien_anne",
            "anquetil_nicolas",
            "cresson_sylvain",
            "croisy_pascal",
            "ducasse_stephane"
        ],
        "title": "Test case selection in industry: an analysis of issues related to static approaches",
        "bibtex": "blondeau_test_2017",
        "abstract": "Automatic testing constitutes an important part of everyday development practice. Worldline, a major IT company, is creating more and more tests to ensure the good behavior of its applications and gains in efficiency and quality. But running all these tests may take hours. This is especially true for large systems involving, for example, the deployment of a web server or communication with a database. For this reason, tests are not launched as often as they should be and are mostly run at night. The company wishes to improve its development and testing process by giving to developers rapid feedback after a change. An interesting solution is to reduce the number of tests to run by identifying only those exercising the piece of code changed. Two main approaches are proposed in the literature: static and dynamic. The static approach creates a model of the source code and explores it to find links between changed methods and tests. The dynamic approach records invocations of methods during the execution of test scenarios. Before deploying a test case selection solution, Worldline created a partnership with us to investigate the situation in its projects and to evaluate these approaches on three industrial, closed source, cases to understand the strengths and weaknesses of each solution. We propose a classification of problems that may arise when trying to identify the tests that cover a method. We give concrete examples of these problems and list some possible solutions. We also evaluate other issues such as the impact on the results of the frequency of modification of methods or considering groups of methods instead of single ones. We found that solutions must be combined to obtain better results, and problems have different impacts on projects. Considering commits instead of individual methods tends to worsen the results, perhaps due to their large size.",
        "published_in": "Software Quality Journal",
        "publisher": "Springer",
        "doi": "10.1007/s11219-016-9328-4",
        "date": "2016-07-08",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Three undisclosed Worldline projects (5000+ TCs, 5+ hours execution)\n\nIndustrial proprietary, large scale",
        "prog_language": "Java",
        "ind_partner": "Worldline (France)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Dynamic vs. static selection",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "Selection ratio, precision, recall, F-measure",
        "effe_metrics": "Selection/reduction count/percentage, Accuracy/precision/recall, Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Experiment with hybrid static-dynamic approach; perform further experiments implementing the approach at Worldline."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Chen, Junjie; Bai, Yanwei; Hao, Dan; Xiong, Yingfei; Zhang, Hongyu; Zhang, Lu; Xie, Bing",
        "author_keys": [
            "chen_junjie",
            "bai_yanwei",
            "hao_dan",
            "xiong_yingfei",
            "zhang_hongyu",
            "zhang_lu",
            "xie_bing"
        ],
        "title": "Test case prioritization for compilers: A text-vector based approach",
        "bibtex": "chen_test_2016",
        "abstract": "Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efficiency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach first transforms each test case into a text-vector by extracting its tokens which reflect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efficiency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and find that our approach is much more efficient than the existing approaches and is effective in prioritizing test cases.",
        "published_in": "2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)",
        "publisher": "IEEE",
        "doi": "10.1109/ICST.2016.19",
        "date": "2016-07-21",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "C compilers GCC (3.3M LOC) and LLVM (4.7M LOC)\n\nOpen-source, unclear scale (in tests)",
        "prog_language": "C",
        "ind_partner": "",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "text-vector based (similarity)",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "-APFD\n-time spent in prioritization",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Use more types and representations of test programs."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Pradhan, Dipesh; Wang, Shuai; Ali, Shaukat; Yue, Tao",
        "author_keys": [
            "pradhan_dipesh",
            "wang_shuai",
            "ali_shaukat",
            "yue_tao"
        ],
        "title": "Search-Based Cost-Effective Test Case Selection within a Time Budget: An Empirical Study",
        "bibtex": "pradhan_search-based_2016",
        "abstract": "Due to limited time and resources available for execution, test case selection always remains crucial for cost-effective testing. It is even more prominent when test cases require manual steps, e.g., operating physical equipment. Thus, test case selection must consider complicated trade-offs between cost (e.g., execution time) and effectiveness (e.g., fault detection capability). Based on our industrial collaboration within the Maritime domain, we identified a real-world and multi-objective test case selection problem in the context of robustness testing, where test case execution requires human involvement in certain steps, such as turning on the power supply to a device. The high-level goal is to select test cases for execution within a given time budget, where test engineers provide weights for a set of objectives, depending on testing requirements, standards, and regulations. To address the identified test case selection problem, we defined a fitness function including one cost measure, i.e., Time Difference (TD) and three effectiveness measures, i.e., Mean Priority (MPR), Mean Probability (MPO) and Mean Consequence (MC) that were identified together with test engineers. We further empirically evaluated eight multi-objective search algorithms, which include three weight-based search algorithms (e.g., Alternating Variable Method) and five Pareto-based search algorithms (e.g., Strength Pareto Evolutionary Algorithm 2 (SPEA2)) using two weight assignment strategies (WASs). Notice that Random Search (RS) was used as a comparison baseline. We conducted two sets of empirical evaluations: 1) Using a real world case study that was developed based on our industrial collaboration; 2) simulating the real world case study to a larger scale to assess the scalability of the search algorithms. Results show that SPEA2 with either of the WASs performed the best for both the studies. Overall, SPEA2 managed to improve on average 32.7%, 39% and 33% in terms of MPR, MPO and MC respectively as compared to RS.",
        "published_in": "GECCO '16: Proceedings of the Genetic and Evolutionary Computation Conference 2016",
        "publisher": "ACM",
        "doi": "10.1145/2908812.2908850",
        "date": "2016-07-20",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Realistic case study based on standards, public requirements and a handbook (165 TCs)\n\nIndustrial proprietary, small scale",
        "prog_language": "N/A",
        "ind_partner": "Undisclosed oil & gas company (Norway)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Search-based",
        "info_approach": "",
        "alg_approach": "Search-based",
        "metrics": "fitness value, hypervolume",
        "effe_metrics": "Fault detection within a budget, Algorithm performance measures",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Hybrid search and evolutionary algorithms for better performance."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Buchgeher, Georg; Klammer, Claus; Heider, Wolfgang; Schuetz, Martin; Huber, Heinz",
        "author_keys": [
            "buchgeher_georg",
            "klammer_claus",
            "heider_wolfgang",
            "schuetz_martin",
            "huber_heinz"
        ],
        "title": "Improving testing in an enterprise SOA with an architecture-based approach",
        "bibtex": "buchgeher_improving_2016",
        "abstract": "High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis. Â© 2016 IEEE.",
        "published_in": "2016 13th Working IEEE/IFIP Conference on Software Architecture (WICSA)",
        "publisher": "IEEE",
        "doi": "10.1109/WICSA.2016.24",
        "date": "2016-07-21",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Undisclosed projects from RSG\n\nIndustrial proprietary, unclear scale.",
        "prog_language": "Unclear",
        "ind_partner": "Raiffeisen Software GmbH (Austria)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "model-based, load factor",
        "info_approach": "Model-based, Load factor",
        "alg_approach": "",
        "metrics": "Comparison with expert",
        "effe_metrics": "Comparison to expert",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Combine different information sources to improve TCP+TCS; incorporate system evolution information; use derived information towards test case generation"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Tahvili, Sahar; Saadatmand, Mehrdad; Larsson, Stig; Afzal, Wasif; Bohlin, Markus; Sundmark, Daniel",
        "author_keys": [
            "tahvili_sahar",
            "saadatmand_mehrdad",
            "larsson_stig",
            "afzal_wasif",
            "bohlin_markus",
            "sundmark_daniel"
        ],
        "title": "Dynamic integration test selection based on test case dependencies",
        "bibtex": "tahvili_dynamic_2016",
        "abstract": "Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.",
        "published_in": "2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSTW.2016.14",
        "date": "2016-08-04",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Brake and air supply systems from Bombardier (probably related to tahvili_cost-benefit_2016)\n\nIndustrial proprietary, unclear scale.",
        "prog_language": "Unclear",
        "ind_partner": "Bombardier Transportation (Sweden)",
        "ind_author": "FALSE",
        "prac_feedback": "TRUE",
        "avai_tool": "N/A",
        "put_practice": "MAYBE",
        "suppl_url": "",
        "approach": "The work combines offline prioritization with online selection from the prioritized sets. \nThis work is related to row 5 and here test case dependencies are also captured manually.",
        "info_approach": "Cost-aware, Manual classification",
        "alg_approach": "",
        "metrics": "Requirement coverage; time efficiency; cost efficiency; fault detection probability. Introduces a \"dependency degree\" metric for prioritizing test cases; these are not an evaluation metric, but rather metrics attributed to each test case used by the TCP algorithm.\n\nFor evaluation: test execution time.",
        "effe_metrics": "Testing time",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Provide further evidence by executing the online phase; investigate opposite form of result dependency."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Ãqvist, Jesper; Hedin, GÃ¶rel; Magnusson, Boris",
        "author_keys": [
            "oqvist_jesper",
            "hedin_gorel",
            "magnusson_boris"
        ],
        "title": "Extraction-based regression test selection",
        "bibtex": "oqvist_extraction-based_2016",
        "abstract": "Frequent regression testing is a core activity in agile software development, but large test suites can lead to long test running times, hampering agility. By safe RTS (Regression Test Selection) techniques, a subset of the tests can be identified that cover all tests that can change result since the last run. To pay off in practice, the RTS overhead must be low. Most existing RTS techniques are based on dynamic coverage analysis, making the overhead related to the tests run. We present Extraction-Based RTS, a new safe RTS technique which uses a fast static analysis with very low overhead, related to the size of the modification rather than to the tests run. The method is suitable for program-driven testing, commonly used in agile development, where each test is a piece of code that uses parts of the system under test. We have implemented the method for Java, and bench-marked it on a number of open source projects, showing that it pays off substantially in practice.",
        "published_in": "PPPJ '16: Proceedings of the 13th International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools",
        "publisher": "ACM",
        "doi": "10.1145/2972206.2972224",
        "date": "2016-08-29",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "five Open Source Java projects (from 20-254K lines of code, up to 271 TCs): Apache Commons Lang, Closure\nCompiler, Functor, Jaxen, and JUnit.\n\nOpen-source, small scale.",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well-documented",
        "put_practice": "FALSE",
        "suppl_url": "https://bitbucket.org/joqvist/autorts\n\nhttps://bitbucket.org/joqvist/tsbench/src/master/",
        "approach": "\"safe regression test selection method for program-\ndriven testing\": a variation on change-based focusing on the size of the changes",
        "info_approach": "Change-based",
        "alg_approach": "",
        "metrics": "total running time",
        "effe_metrics": "",
        "effi_metrics": "Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": " - develop methods for generating program-driven tests from input-\ndriven tests\n - combine the\nmethod with unsafe RTS methods to be able to skip tests"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "MagalhÃ£es, ClÃ¡udio; Mota, Alexandre; Barros, FlÃ¡via; Maia, Eliot",
        "author_keys": [
            "magalhaes_claudio",
            "mota_alexandre",
            "barros_flavia",
            "maia_eliot"
        ],
        "title": "Automatic selection of test cases for regression testing",
        "bibtex": "magalhaes_automatic_2016",
        "abstract": "Regression testing is a safety measure to attest that changes made on a system preserve prior accepted behavior. Identifying which test cases must compose a regression test suite in a certain development stage is tricky, particularly when one only has test cases and change requests described in natural language, and the execution of the test suite will be performed manually. That is the case of our industrial partner. We propose a selection of regression test cases based on information retrieval and implement as a web-service. In performed experiments, we show that we can improve the creation of regression test suites of our industrial partner by providing more effective test cases based on keywords analysis in an automatic way.",
        "published_in": "SAST: Proceedings of the 1st Brazilian Symposium on Systematic and Automated Software Testing",
        "publisher": "ACM",
        "doi": "10.1145/2993288.2993299",
        "date": "2016-09-19",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Test repository from Motorola (up to 427 TCs)\n\nIndustrial proprietary, small scale",
        "prog_language": "Natural language",
        "ind_partner": "Motorola Mobility (Brazil)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "An information retrieval-based TCS approach.",
        "info_approach": "",
        "alg_approach": "Search-based",
        "metrics": "Traditional information retrieval-based metrics such as recall, number (and %) of elements returned at the top of the list (top-n).",
        "effe_metrics": "Accuracy/precision/recall, Comparison to expert",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": " - to completely replace the manual TCS approach with the automated one\n - try other variants of the merge strategy\n - Include source code as an input artefact\n - evaluate the effect of the derived TS on coverage"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Aman, Hirohisa; Tanaka, Yuta; Nakano, Takashi; Ogasawara, Hideto; Kawahara, Minoru",
        "author_keys": [
            "aman_hirohisa",
            "tanaka_yuta",
            "nakano_takashi",
            "ogasawara_hideto",
            "kawahara_minoru"
        ],
        "title": "Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing",
        "bibtex": "aman_application_2016",
        "abstract": "To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods. Â© 2016 IEEE.",
        "published_in": "2016 42th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
        "publisher": "IEEE",
        "doi": "10.1109/SEAA.2016.29",
        "date": "2016-10-18",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "FALSE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Undisclosed system from Toshiba (300 TCs)\n\nIndustrial proprietary, small scale",
        "prog_language": "Unclear",
        "ind_partner": "Toshiba (Japan)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "MAYBE",
        "suppl_url": "",
        "approach": "Similarity-based",
        "info_approach": "History-based",
        "alg_approach": "Similarity / distance-based",
        "metrics": "GLC (number of versions in which a certain test T was not run), failure rate",
        "effe_metrics": "Fault Detection Rate (FDR)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Apply in other software domains and compare with other TCP techniques."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Busjaeger, Benjamin; Xie, Tao",
        "author_keys": [
            "busjaeger_benjamin",
            "xie_tao"
        ],
        "title": "Learning for test prioritization: An industrial case study",
        "bibtex": "busjaeger_learning_2016",
        "abstract": "Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration envi-ronments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimiza-tion mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in indus-trial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques.",
        "published_in": "FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/2950290.2983954",
        "date": "2016-11-01",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Automation system at Salesforce (~45000 TCs)\n\nIndustrial proprietary, very large scale",
        "prog_language": "Java",
        "ind_partner": "Salesforce (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Machine learning-based (algo)\n\nCode coverage-based, similarity-based, history-based, test age (info)",
        "info_approach": "History-based, Coverage-based",
        "alg_approach": "Similarity / distance-based, Machine learning-based",
        "metrics": "APFD, recall",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Accuracy/precision/recall",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Deepen learning, adding more features."
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Yoshida, Hiroaki; Tokumoto, Susumu; Prasad, Mukul R.; Ghosh, Idradeep; Uehara, Tadahiro",
        "author_keys": [
            "yoshida_hiroaki",
            "tokumoto_susumu",
            "prasad_mukul_r",
            "ghosh_idradeep",
            "uehara_tadahiro"
        ],
        "title": "FSX: A tool for fine-grained incremental unit test generation for C/C++ Programs",
        "bibtex": "yoshida_fsx_2016",
        "abstract": "Automated unit test generation bears the promise of significantly reducing test cost and hence improving software quality. However, the maintenance cost of the automatically generated tests presents a significant barrier to adoption of this technology. To address this challenge, in previous work, we proposed a novel technique for automated and fine-grained incremental generation of unit tests through minimal augmentation of an existing test suite. In this paper we describe a tool FSX, implementing this technique. We describe the architecture, user-interface, and salient features of FSX, and specific practical use-cases of its technology. We also report on a real, large-scale deployment of FSX as a practical validation of the underlying research contribution and of automated test generation research in general. Â© 2016 ACM.",
        "published_in": "FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/2950290.2983937",
        "date": "2016-11-01",
        "tcp": "",
        "tcs": "",
        "tsr": "",
        "tsa": "X",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source tool iPerf (114 TCs)\n\nOpen-source, small scale",
        "prog_language": "C",
        "ind_partner": "Fujitsu (Japan)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "Coverage-based",
        "info_approach": "Coverage-based",
        "alg_approach": "",
        "metrics": "Statement coverage, branch coverage, #tests, runtime",
        "effe_metrics": "Coverage Effectiveness (CE), Number of tests added",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "n/a"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Tahvili, Sahar; Bohlin, Markus; Saadatmand, Mehrdad; Larsson, Stig; Afzal, Wasif; Sundmark, Daniel",
        "author_keys": [
            "tahvili_sahar",
            "bohlin_markus",
            "saadatmand_mehrdad",
            "larsson_stig",
            "afzal_wasif",
            "sundmark_daniel"
        ],
        "title": "Cost-benefit analysis of using dependency knowledge at integration testing",
        "bibtex": "tahvili_cost-benefit_2016",
        "abstract": "In software system development, testing can take considerable time and resources, and there are numerous examples in the literature of how to improve the testing process. In particular, methods for selection and prioritization of test cases can play a critical role in efficient use of testing resources. This paper focuses on the problem of selection and ordering of integration-level test cases. Integration testing is performed to evaluate the correctness of several units in composition. Further, for reasons of both effectiveness and safety, many embedded systems are still tested manually. To this end, we propose a process, supported by an online decision support system, for ordering and selection of test cases based on the test result of previously executed test cases. To analyze the economic efficiency of such a system, a customized return on investment (ROI) metric tailored for system integration testing is introduced. Using data collected from the development process of a large-scale safety-critical embedded system, we perform Monte Carlo simulations to evaluate the expected ROI of three variants of the proposed new process. The results show that our proposed decision support system is beneficial in terms of ROI at system integration testing and thus qualifies as an important element in improving the integration testing process. Â© Springer International Publishing AG 2016.",
        "published_in": "PROFES 2016: Product-Focused Software Process Improvement",
        "publisher": "Springer",
        "doi": "10.1007/978-3-319-49094-6_17 ",
        "date": "2016-11-06",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Train control managment subsystem (4578 TCs)\n\nIndustrial proprietary, large scale",
        "prog_language": "Natural language",
        "ind_partner": "Bombardier Transportation (Sweden)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "N/A",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "the paper evaluates the ROI for the TCP approach proposed in row 42; here they build a cost model and compare the costs of using TCP or not. Note that the TCP uses dependencies among tests that are identifies manually",
        "info_approach": "Cost-aware, Manual classification",
        "alg_approach": "",
        "metrics": "Return on investment",
        "effe_metrics": "Cost-benefit model",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "- do study on real data and not on simulation\n-consider other criteria for TCP"
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Ramler, Rudolf; Salomon, Christian; Buchgeher, Georg; Lusser, Michael",
        "author_keys": [
            "ramler_rudolf",
            "salomon_christian",
            "buchgeher_georg",
            "lusser_michael"
        ],
        "title": "Tool support for change-based regression testing: An industry experience report",
        "bibtex": "ramler_tool_2017",
        "abstract": "Changes may cause unexpected side effects and inconsistencies. Regression testing is the process of re-testing a software system after changes have been made to ensure that the new version of the system has retained the capabilities of the old version and that no new defects have been introduced. Regression testing is an essential activity, but it is also time-consuming and costly. Thus, regression testing should concentrate on those parts of the system that have been modified or which are affected by changes. Regression test selection has been proposed over three decades ago and, since then, it has been frequently in the focus of empirical studies. However, regression test selection is still not widely adopted in practice. Together with the test team of an industrial software company we have developed a tool-based approach that assists software testers in selecting regression test cases based on change information and test coverage data. This paper describes the main usage scenario of the approach, illustrates the implemented solution, and reports on its evaluation in a large industry project. The evaluation showed that the tool support reduces the time required for compiling regression test suites and fosters an accurate selection of regression test cases. The paper concludes with our lessons learned from implementing the tool support in a real-world setting. Â© Springer International Publishing AG 2017.",
        "published_in": "SWQD 2017: Software Quality. Complexity and Challenges of Software Engineering in Emerging Technologies",
        "publisher": "Springer",
        "doi": "10.1007/978-3-319-49421-0_10",
        "date": "2016-11-12",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Undisclosed software system at Omicron (over 5000 TCs)\n\nIndustrial proprietary, large scale",
        "prog_language": "Multi-language",
        "ind_partner": "OMICRON Electronics GmbH (Austria)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "change-based, coverage-based, graph walk",
        "info_approach": "Change-based, Coverage-based",
        "alg_approach": "Graph-based",
        "metrics": "testing time",
        "effe_metrics": "Testing time",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "lack of details provided by version control; redundant test cases; polluted coverage footprints; creating and maintaining coverage"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Strandberg, Per Erik; Sundmark, Daniel; Afzal, Wasif; Ostrand, Thomas J.; Weyuker, Elaine J.",
        "author_keys": [
            "strandberg_per_erik",
            "sundmark_daniel",
            "afzal_wasif",
            "ostrand_thomas_j",
            "weyuker_elaine_j"
        ],
        "title": "Experience Report: Automated System Level Regression Test Prioritization Using Multiple Factors",
        "bibtex": "strandberg_experience_2016",
        "abstract": "We propose a new method of determining an effective ordering of regression test cases, and describe its implementation as an automated tool called SuiteBuilder developed by Westermo Research and Development AB. The tool generates an efficient order to run the cases in an existing test suite by using expected or observed test duration and combining priorities of multiple factors associated with test cases, including previous fault detection success, interval since last executed, and modifications to the code tested. The method and tool were developed to address problems in the traditional process of regression testing, such as lack of time to run a complete regression suite, failure to detect bugs in time, and tests that are repeatedly omitted. The tool has been integrated into the existing nightly test framework for Westermo software that runs on large-scale data communication systems. In experimental evaluation of the tool, we found significant improvement in regression testing results. The re-ordered test suites finish within the available time, the majority of fault-detecting test cases are located in the first third of the suite, no important test case is omitted, and the necessity for manual work on the suites is greatly reduced. Â© 2016 IEEE.",
        "published_in": "2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)",
        "publisher": "IEEE",
        "doi": "10.1109/ISSRE.2016.23",
        "date": "2016-12-08",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Embedded systems running Westermo's operating system WeOS  (unknown scale)\n\nIndustrial proprietary, unknown scale",
        "prog_language": "Python",
        "ind_partner": "Westermo Research and Development (Sweden)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "History-based, change-based",
        "info_approach": "History-based, Change-based",
        "alg_approach": "",
        "metrics": "Testing time; fault detection rate",
        "effe_metrics": "Testing time, Fault Detection Rate (FDR)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Different kinds of suites and prioritizers; apply approach to minimization"
    },
    {
        "type": "primary",
        "year": "2016",
        "authors": "Marijan, Dusica; Liaaen, Marius",
        "author_keys": [
            "marijan_dusica",
            "liaaen_marius"
        ],
        "title": "Effect of time window on the performance of continuous regression testing",
        "bibtex": "marijan_effect_2016",
        "abstract": "Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",
        "published_in": "2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSME.2016.77",
        "date": "2017-01-16",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Cisco videoconferencing system (460 TCs)\n\nIndustrial proprietary, small-scale",
        "prog_language": "Unclear",
        "ind_partner": "Cisco (Norway)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "History-based, fault detection-based",
        "info_approach": "History-based, Fault-based",
        "alg_approach": "",
        "metrics": "fault detection capability, APFD",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Fault Detection Capability",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Utilize comprehensive cost-benefit measures, experiments in other industry domains"
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Gotlieb, Arnaud; Marijan, Dusica",
        "author_keys": [
            "gotlieb_arnaud",
            "marijan_dusica"
        ],
        "title": "Using global constraints to automate regression testing",
        "bibtex": "gotlieb_using_2017",
        "abstract": "Communicating or autonomous systems rely on high-quality software-based components. that must be thoroughly verified before they are released and deployed in operational settings. Regression testing is a crucial verification process that compares any new release of a software-based component against its previous versions, by executing available test cases. However, limited testing time makes selection of test cases in regression testing challenging, and some selection criteria must be respected. Validation engineers usually address this problem, coined as test suite reduction (TSR), through manual analysis or by using approximation techniques. In this paper, we address the TSR problem with sound artificial intelligence techniques such as constraint programming (CP) and global constraints. By using distinct cost-value-aggregating criteria, we propose several constraint-optimization models to find a subset of test cases that cover all the test requirements and optimize the overall cost of selected test cases. Our contribution includes reuse of existing preprocessing rules to simplify the problem before solving it and the design of structure-aware heuristics that take into account the notion of the costs associated with test cases. The work presented in this paper has been motivated by an industrial application in the communication domain. Our overall goal is to develop a constraint-based approach of test suite reduction that can be deployed to test a complete product line of conferencing systems in continuous delivery mode. By implementing this approach in a software prototype tool and experimentally evaluating it on both randomly generated and industrial instances, we hope to foster a quick adoption of the technology. Copyright Â© 2017, Association for the Advancement of Artificial Intelligence. All rights reserved.",
        "published_in": "\n\nAI Magazine",
        "publisher": "AAAI",
        "doi": "10.1609/aimag.v38i1.2714",
        "date": "2017-03-31",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Cisco videoconferencing system (377 TCs)\n\nIndustrial proprietary, small scale",
        "prog_language": "Unclear",
        "ind_partner": "Cisco (Norway)",
        "ind_author": "FALSE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "MAYBE",
        "suppl_url": "",
        "approach": "Optimized approach to Constraint-based test suite reduction and its evalutation in comparison with SOTA",
        "info_approach": "",
        "alg_approach": "Constraints-based",
        "metrics": "CPU time of solving\nTest suite size reduction",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "- improve preprocessing  heuristics for improving efficiency\n- increase usability to facilitate industrial adoption"
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Chi, Zongzheng; Xuan, Jifeng; Ren, Zhilei; Xie, Xiaoyuan; Guo, He",
        "author_keys": [
            "chi_zongzheng",
            "xuan_jifeng",
            "ren_zhilei",
            "xie_xiaoyuan",
            "guo_he"
        ],
        "title": "Multi-Level Random Walk for Software Test Suite Reduction",
        "bibtex": "chi_multi-level_2017",
        "abstract": "Which test cases should be selected to save the time of software testing? Due to the large time cost of running all test cases, it is necessary to run representative test cases to shorten the software development cycle. Test suite reduction, an NP-hard problem in software engineering, aims to select a subset of test cases to reduce the time cost of test execution in satisfying test requirements. Recently, search based software engineering provides a new direction to test suite reduction by connecting software engineering problems with computational intelligence methods. In this paper, we propose a multi-level optimization algorithm to simplify the original problem instance of test suite reduction. In each level, we search for local optimal solutions with random walk in potential subsets of the test suite. The problem scale is reduced by locking the intersection of local optima and by discarding shielded test cases with no contribution to test requirements. We compare our algorithm with state-of-the-art methods on test suites of ten large-scale open source projects. Experiments show that our algorithm can more efficiently find optima on five out of six projects, in which Integer Linear Programming (ILP) can find optima; for the other four projects that ILP fails to solve, our algorithm provides the best solutions among heuristics in comparison. Â© 2017 IEEE.",
        "published_in": "IEEE Computational Intelligence Magazine",
        "publisher": "IEEE",
        "doi": "10.1109/MCI.2017.2670460",
        "date": "2017-04-12",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 6196 TCs)\n\nOpen-source, large scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "MAYBE\n\nOnly dataset, well-documented",
        "put_practice": "FALSE",
        "suppl_url": "http://cstar.whu.edu.cn/p/multi-walk/",
        "approach": "search-based",
        "info_approach": "",
        "alg_approach": "Search-based",
        "metrics": "reduction rate, runtime",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Improve scalability for large and small instances."
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Bach, Thomas; Andrzejak, Artur; Pannemans, Ralf",
        "author_keys": [
            "bach_thomas",
            "andrzejak_artur",
            "pannemans_ralf"
        ],
        "title": "Coverage-Based Reduction of Test Execution Time: Lessons from a Very Large Industrial Project",
        "bibtex": "bach_coverage-based_2017",
        "abstract": "There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects. Â© 2017 IEEE.",
        "published_in": "2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSTW.2017.6",
        "date": "2017-04-17",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Database management system (19472 TCs) \n\nIndustrial proprietary, very large scale (\"atomic fragments of test code\")",
        "prog_language": "Python",
        "ind_partner": "SAP SE (Germany)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "Greedy (algo)\n\nCoverage-based and overlap-aware (info)",
        "info_approach": "Coverage-based",
        "alg_approach": "Greedy",
        "metrics": "Coverage",
        "effe_metrics": "Selection/reduction count/percentage, Fault detection within a budget",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Flaky tests, experiment with other over-lap aware heuristics."
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Spieker, Helge; Gotlieb, Arnaud; Marijan, Dusica; Mossige, Morten",
        "author_keys": [
            "spieker_helge",
            "gotlieb_arnaud",
            "marijan_dusica",
            "mossige_morten"
        ],
        "title": "Reinforcement learning for automatic test case prioritization and selection in continuous integration",
        "bibtex": "spieker_reinforcement_2017",
        "abstract": "Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.",
        "published_in": "ISSTA 2017: Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "publisher": "ACM",
        "doi": "10.1145/3092703.3092709",
        "date": "2017-07-10",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "FALSE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial open-source, medium to large scale\nOpen-source, small to large scale\n\nABB Robotics dataset (2k TCs)\n\nGSDTSR (Google) dataset (5.5k TCs)",
        "prog_language": "",
        "ind_partner": "ABB Robotics (Norway)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well-documented\n\nAlso dataset separately",
        "put_practice": "FALSE",
        "suppl_url": "https://bitbucket.org/HelgeS/retecs/src/master/\n\nhttps://bitbucket.org/HelgeS/atcs-data/src/master/",
        "approach": "machine-learning based (reinforcement learning)",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "NAPFD",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "With increased data available, larger networks and deep learning are promising."
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Vasic, Marko; Parvez, Zuhair; Milicevic, Aleksandar; Gligoric, Milos",
        "author_keys": [
            "vasic_marko",
            "parvez_zuhair",
            "milicevic_aleksandar",
            "gligoric_milos"
        ],
        "title": "File-Level vs. Module-Level Regression Test Selection for .NET",
        "bibtex": "vasic_file-level_2017",
        "abstract": "Regression testing is used to check the correctness of evolving software. With the adoption of Agile development methodology, the number of tests and software revisions has dramatically increased, and hence has the cost of regression testing. Researchers proposed regression test selection (RTS) techniques that optimize regression testing by skipping tests that are not impacted by recent program changes. Ekstazi is one such state-of-the art technique; Ekstazi is implemented for the Java programming language and has been adopted by several companies and open-source projects.\nWe report on our experience implementing and evaluating Ekstazi#, an Ekstazi-like tool for .NET. We describe the key challenges of bringing the Ekstazi idea to the .NET platform. We evaluate Ekstazi# on 11 open-source projects, as well as an internal Microsoft project substantially larger than each of the open-source projects. Finally, we compare Ekstazi# to an incremental build system (also developed at Microsoft), which, out of the box, provides module-level dependency tracking and skipping tasks (including test execution) whenever dependencies of a task do not change between the current and the last successful build. Ekstazi# on average reduced regression testing time by 43.70% for the open-source projects and by 65.26% for the Microsoft project (the latter is in addition to the savings provided by incremental builds).",
        "published_in": "ESEC/FSE 2017: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3106237.3117763",
        "date": "2017-08-21",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Open-source C# projects (up to 183 TCs) plus one Microsoft project (37 \"test modules\")\n\nOpen-source, small scale\nIndustrial proprietary, unclear scale",
        "prog_language": "C#",
        "ind_partner": "Microsoft (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, poorly documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/marko-vasic/ekstaziSharp",
        "approach": "Change-based",
        "info_approach": "Change-based",
        "alg_approach": "",
        "metrics": "selection count, execution time, cumulative time",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "Execution time, Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "n/a"
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Celik, Ahmet; Vasic, Marko; Milicevic, Aleksandar; Gligoric, Milos",
        "author_keys": [
            "celik_ahmet",
            "vasic_marko",
            "milicevic_aleksandar",
            "gligoric_milos"
        ],
        "title": "Regression test selection across JVM boundaries",
        "bibtex": "celik_regression_2017",
        "abstract": "Modern software development processes recommend that changes be integrated into the main development line of a project multiple times a day. Before a new revision may be integrated, developers practice regression testing to ensure that the latest changes do not break any previously established functionality. The cost of regression testing is high, due to an increase in the number of revisions that are introduced per day, as well as the number of tests developers write per revision. Regression test selection (RTS) optimizes regression testing by skipping tests that are not affected by recent project changes. Existing dynamic RTS techniques support only projects written in a single programming language, which is unfortunate knowing that an open-source project is on average written in several programming languages. We present the first dynamic RTS technique that does not stop at predefined language boundaries. Our technique dynamically detects, at the operating system level, all file artifacts a test depends on. Our technique is, hence, oblivious to the specific means the test uses to actually access the files: be it through spawning a new process, invoking a system call, invoking a library written in a different language, invoking a library that spawns a process which makes a system call, etc. We also provide a set of extension points which allow for a smooth integration with testing frameworks and build systems. We implemented our technique in a tool called RTSLinux as a loadable Linux kernel module and evaluated it on 21 Java projects that escape the JVM by spawning new processes or invoking native code, totaling 2, 050, 791 lines of code. Our results show that RTSLinux, on average, skips 74.17% of tests and saves 52.83% of test execution time compared to executing all tests. Â© 2017 Association for Computing Machinery.",
        "published_in": "ESEC/FSE 2017: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3106237.3106297",
        "date": "2017-08-21",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 431 TCs)\n\nOpen-source, small scale",
        "prog_language": "Java",
        "ind_partner": "Microsoft (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Change-based",
        "info_approach": "Change-based, Execution context",
        "alg_approach": "",
        "metrics": "testing time, reduction rate, dependency discovery, overhead",
        "effe_metrics": "Selection/reduction count/percentage, Testing time",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Incorporate approach with other programming languages."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Ouriques, JoÃ£o Felipe S.; Cartaxo, Emanuela G.; Machado, PatrÃ­cia D.L.",
        "author_keys": [
            "ouriques_joao_felipe_s",
            "cartaxo_emanuela_g",
            "machado_patricia_dl"
        ],
        "title": "Test case prioritization techniques for model-based testing: a replicated study",
        "bibtex": "ouriques_test_2018",
        "abstract": "Recently, several test case prioritization (TCP) techniques have been proposed to order test cases for achieving a goal during test execution, particularly, revealing faults sooner. In the model-based testing (MBT) context, such techniques are usually based on heuristics related to structural elements of the model and derived test cases. In this sense, techniques' performance may vary due to a number of factors. While empirical studies comparing the performance of TCP techniques have already been presented in literature, there is still little knowledge, particularly in the MBT context, about which factors may influence the outcomes suggested by a TCP technique. In a previous family of empirical studies focusing on labeled transition systems, we identified that the model layout, i.e., amount of branches, joins, and loops in the model, alone may have little influence on the effectiveness of TCP techniques investigated, whereas characteristics of test cases that actually fail definitely influences this aspect. However, we considered only synthetic artifacts in the study, which reduced the ability of representing properly the reality. In this paper, we present a replication of one of these studies, now with a larger and more representative selection of techniques and considering test suites from industrial systems as experimental objects. Our objective is to find out whether the results remain while increasing the validity in comparison to the original study. Results reinforce that there is no best performer among the investigated techniques and characteristics of test cases that fail represent an important factor, although adaptive random-based techniques are less affected by it.",
        "published_in": "Software Quality Journal",
        "publisher": "IEEE",
        "doi": "10.1007/s11219-017-9398-y",
        "date": "2018-01-23",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Undisclosed systems at Ingenico (up to 48 TCs)\n\nIndustrial proprietary, small scale.",
        "prog_language": "C, Java, Groovy",
        "ind_partner": "Ingenico (Brazil)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "coverage-based, similarity-based, path complexity-based, model-based",
        "info_approach": "Coverage-based, Model-based",
        "alg_approach": "Similarity / distance-based, Graph-based",
        "metrics": "APFD",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "discover influence of other factors on effectiveness; collect supplementary data to improve techniques."
    },
    {
        "type": "primary",
        "year": "2017",
        "authors": "Kwon, Jung-Hyun; Ko, In-Young",
        "author_keys": [
            "kwon_jung-hyun",
            "ko_in-young"
        ],
        "title": "Cost-effective regression testing using bloom filters in continuous integration development environments",
        "bibtex": "kwon_cost-effective_2017",
        "abstract": "Regression testing in continuous integration development environments \nmust be cost-effective and should provide fast feedback on test suite \nfailures to the developers. In order to provide faster feedback on \nfailures to developers while using computing resources efficiently, two \ntypes of regression testing techniques have been developed: Regression \nTesting Selection (RTS) and Test Case Prioritization (TCP). One of the \nfactors that reduces the effectiveness of the RTS and TCP techniques is \nthe inclusion of test suites that fail only once over a period. We \npropose an approach based on Bloom filtering to exclude such test suites\n during the RTS process, and to assign such test suites with a lower \npriority during the TCP process. We experimentally evaluate our approach\n using a Google dataset, and demonstrate that cost-effectiveness of the \nproposed RTS and TCP techniques outperforms the state-of-the-art \ntechniques.",
        "published_in": "2017 24th Asia-Pacific Software Engineering Conference (APSEC)",
        "publisher": "IEEE",
        "doi": "10.1109/APSEC.2017.22",
        "date": "2018-03-05",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "GSDTSR (5555 TCs)\n\nIndustrial open-source, large scale.",
        "prog_language": "C, Java, Python, etc.",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "A bloom ï¬lter is used together with the window-based method proposed by Elbaum and Penix with the purpose of removing test suites that failed only once over a period. The selected test suites receive a weight that is later used for prioritizing the TS execution (so some notion of orchestration could possibly be considered).",
        "info_approach": "History-based",
        "alg_approach": "Bloom filter or window-based",
        "metrics": "Fault detection capability, Efftime (efficiency), and Precision.",
        "effe_metrics": "Accuracy/precision/recall, Fault Detection Capability, Faults per tests/time",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Improve the approach to reduce the false-positive cases."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Garousi, Vahid; Ãzkan, Ramazan; Betin-Can, Aysu",
        "author_keys": [
            "garousi_vahid",
            "ozkan_ramazan",
            "betin-can_aysu"
        ],
        "title": "Multi-objective regression test selection in practice: An empirical study in the defense software industry",
        "bibtex": "garousi_multi-objective_2018",
        "abstract": "Context: Executing an entire regression test-suite after every code change is often costly in large software projects. To cope with this challenge, researchers have proposed various regression test-selection techniques. Objective: This paper was motivated by a real industrial need to improve regression-testing practices in the context of a safety-critical industrial software in the defence domain in Turkey. To address our objective, we set up and conducted an âaction-researchâ collaborative project between industry and academia. Method: After a careful literature review, we selected a conceptual multi-objective regression-test selection framework (called MORTO) and adopted it to our industrial context by developing a custom-built genetic algorithm (GA) based on that conceptual framework. GA is able to provide full coverage of the affected (changed) requirements while considering multiple cost and benefit factors of regression testing. e.g., minimizing the number of test cases, and maximizing cumulative number of detected faults by each test suite. Results: The empirical results of applying the approach on the Software Under Test (SUT) demonstrate that this approach yields a more efficient test suite (in terms of costs and benefits) compared to the old (manual) test-selection approach, used in the company, and another applicable approach chosen from the literature. With this new approach, regression selection process in the project under study is not ad-hoc anymore. Furthermore, we have been able to eliminate the subjectivity of regression testing and its dependency on expert opinions. Conclusion: Since the proposed approach has been beneficial in saving the costs of regression testing, it is currently in active use in the company. We believe that other practitioners can apply our approach in their regression-testing contexts too, when applicable. Furthermore, this paper contributes to the body of evidence in regression testing by offering a success story of successful implementation and application of multi-objective regression testing in practice. Â© 2018",
        "published_in": "Information and Software Technology",
        "publisher": "Elsevier",
        "doi": "10.1016/j.infsof.2018.06.007",
        "date": "2018-06-30",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Defense software system (up to 3588 TCs)\n\nGovernment proprietary, large scale",
        "prog_language": "Unclear",
        "ind_partner": "Government organization (Turkey)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, poorly documented",
        "put_practice": "TRUE",
        "suppl_url": "https://zenodo.org/record/1149058#.Xto4gfKxVTY",
        "approach": "multi-objective genetic algorithm",
        "info_approach": "",
        "alg_approach": "Search-based",
        "metrics": "Test execution time, third-party costs, system setup costs, technical resources cost, verification cost",
        "effe_metrics": "Testing time, Cost-benefit model",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Cover other cost/benefit parameters; apply in other contexts and RTS problems; further assess fault detection effectiveness with mutation testing."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Shi, August; Gyori, Alex; Mahmood, Suleman; Zhao, Peiyuan; Marinov, Darko",
        "author_keys": [
            "shi_august",
            "gyori_alex",
            "mahmood_suleman",
            "zhao_peiyuan",
            "marinov_darko"
        ],
        "title": "Evaluating test-suite reduction in real software evolution",
        "bibtex": "shi_evaluating_2018",
        "abstract": "Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.\n\nWe perform the first extensive study of TSR using real test failures in (failed) builds that occurred for real code changes. We analyze 1478 failed builds from 32 GitHub projects that run their tests on Travis. Each failed build can have multiple faults, so we propose a family of mappings from test failures to faults. We use these mappings to compute Failed-Build Detection Loss (FBDL), the percentage of failed builds where the reduced test suite misses to detect all the faults detected by the original test suite. We find that FBDL can be up to 52.2%, which is higher than suggested by traditional TSR metrics. Moreover, traditional TSR metrics are not good predictors of FBDL, making it difficult for developers to decide whether to use reduced test suites.",
        "published_in": "ISSTA 2018: Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "publisher": "ACM",
        "doi": "10.1145/3213846.3213875",
        "date": "2018-07-12",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source programs (up to 770 TCs)\n\nOpen-source, medium scale",
        "prog_language": "Unclear",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "coverage-based",
        "info_approach": "Coverage-based",
        "alg_approach": "",
        "metrics": "FBDL (failed build detection loss)",
        "effe_metrics": "Fault Detection Loss",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "n/a"
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Haghighatkhah, Alireza; MÃ¤ntylÃ¤,  Mika; Oivo, Markku; Kuvaja, Pasi",
        "author_keys": [
            "haghighatkhah_alireza",
            "mantyla__mika",
            "oivo_markku",
            "kuvaja_pasi"
        ],
        "title": "Test prioritization in continuous integration environments",
        "bibtex": "haghighatkhah_test_2018",
        "abstract": "Two heuristics namely diversity-based (DBTP) and history-based test prioritization (HBTP) have been separately proposed in the literature. Yet, their combination has not been widely studied in continuous integration (CI) environments. The objective of this study is to catch regression faults earlier, allowing developers to integrate and verify their changes more frequently and continuously. To achieve this, we investigated six open-source projects, each of which included several builds over a large time period. Findings indicate that previous failure knowledge seems to have strong predictive power in CI environments and can be used to effectively prioritize tests. HBTP does not necessarily need to have large data, and its effectiveness improves to a certain degree with larger history interval. DBTP can be used effectively during the early stages, when no historical data is available, and also combined with HBTP to improve its effectiveness. Among the investigated techniques, we found that history-based diversity using NCD Multiset is superior in terms of effectiveness but comes with relatively higher overhead in terms of method execution time. Test prioritization in CI environments can be effectively performed with negligible investment using previous failure knowledge, and its effectiveness can be further improved by considering dissimilarities among the tests.",
        "published_in": "Journal of Systems and Software",
        "publisher": "Elsevier",
        "doi": "10.1016/j.jss.2018.08.061",
        "date": "2018-08-31",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 411 TCs)\n\nOpen-source, small scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "combination of diversity-based and history-based",
        "info_approach": "History-based",
        "alg_approach": "Similarity / distance-based",
        "metrics": "APFD, execution time",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Replicate study in industry and larger systems."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Zhang, Lingming",
        "author_keys": [
            "zhang_lingming"
        ],
        "title": "Hybrid regression test selection",
        "bibtex": "zhang_hybrid_2018",
        "abstract": "Regression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time.",
        "published_in": "ICSE '18: Proceedings of the 40th International Conference on Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3180155.3180198",
        "date": "2018-09-03",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 16069 TCs)\n\nOpen-source, very large scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nUsable tool, Maven plug-in",
        "put_practice": "FALSE",
        "suppl_url": "http://hyrts.org/",
        "approach": "A dynamic approach to TCS that combines file-level (like Ekstazi) with method level selection, under different variations, to improve precision while keeping efficiency",
        "info_approach": "Change-based",
        "alg_approach": "",
        "metrics": "- Selected test ratio\n\n- E2E testing time ",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "Apply the idea of hybris also to static RTS techniques"
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Miranda, Breno; Cruciani, Emilio; Verdecchia, Roberto; Bertolino, Antonia",
        "author_keys": [
            "miranda_breno",
            "cruciani_emilio",
            "verdecchia_roberto",
            "bertolino_antonia"
        ],
        "title": "FAST Approaches to Scalable Similarity-Based Test Case Prioritization",
        "bibtex": "miranda_fast_2018",
        "abstract": "Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in e ciency with no significant impact on e ectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.",
        "published_in": "2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)",
        "publisher": "ACM",
        "doi": "10.1145/3180155.3180210",
        "date": "2018-09-03",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source C and Java projects (up to 670 TCs) + synthetic data for scalability measure (up to 1M TCs)\n\nOpen-source, medium scale\nSynthetic data, very large scale",
        "prog_language": "C, Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well-documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/icse18-fast/FAST",
        "approach": "similarity-based",
        "info_approach": "Test code",
        "alg_approach": "Similarity / distance-based",
        "metrics": "Prioritization time, total testing time, scalability",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "Execution time, Total/End-to-end time, Scalability",
        "other_metrics": "",
        "open_challenges": "Adapt algorithm ti include other objective functions in addition to dissimilarity."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Yilmaz, UÄur; Tarhan, AyÃ§a",
        "author_keys": [
            "yilmaz_ugur",
            "tarhan_ayca"
        ],
        "title": "A case study to compare regression test selection techniques on open-source software projects",
        "bibtex": "yilmaz_case_2018",
        "abstract": "Regression testing is the type of testing performed on a modified software to validate integrated parts are functioning properly. Especially with agile development practices being increasingly used, regression testing needs to be fast and practical enough to coexist with the nature of agile development. To satisfy this need, Regression Test Selection (RTS) techniques are proposed to reduce number of tests. Although there are many studies analyzing these techniques and their effectiveness in terms of the number of reduced tests, time and cost; the applicability and practicality aspects have been mostly neglected. To this end, in this paper a case study is carried out to compare highly cited and mostly used RTS techniques. Selected techniques are applied on extensively used and tested open-source software projects, and the result of their comparison in regards of practicality, applicability, performance and cost-effectiveness are discussed.",
        "published_in": "Proceedings of the 12th Turkish National Software Engineering Symposium ",
        "publisher": "CEUR",
        "doi": "http://ceur-ws.org/Vol-2201/UYMS_YTM_2018_paper_87.pdf",
        "date": "2018-09-10",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 361 TCs)\n\nOpen-source, small scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "N/A",
        "put_practice": "N/A",
        "suppl_url": "",
        "approach": "Comparison of change-based and graph-based techniques",
        "info_approach": "Change-based",
        "alg_approach": "Graph-based",
        "metrics": "testing time, selection rate",
        "effe_metrics": "Selection/reduction count/percentage, Testing time",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": ""
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Chen, Junjie; Lou, Yiling; Zhang, Lingming; Zhou, Jianyi; Wang, Xiaoleng; Hao, Dan; Zhang, Lu",
        "author_keys": [
            "chen_junjie",
            "lou_yiling",
            "zhang_lingming",
            "zhou_jianyi",
            "wang_xiaoleng",
            "hao_dan",
            "zhang_lu"
        ],
        "title": "Optimizing Test Prioritization via Test Distribution Analysis",
        "bibtex": "chen_optimizing_2018",
        "abstract": "Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice. To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming stateof- the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks fromthe testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.",
        "published_in": "ESEC/FSE 2018: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3236024.3236053",
        "date": "2018-10-26",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Open-source Java projects (up to 9691 TCs) plus undisclosed projects from Baidu (up to 4139 TCs)\n\nOpen-source, large scale\nIndustrial proprietary, large scale",
        "prog_language": "C, Java",
        "ind_partner": "Baidu (China)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, documented",
        "put_practice": "TRUE",
        "suppl_url": "https://github.com/JunjieChen/PTP",
        "approach": "Analyzes test coverage to predict the optimal TCP technique for a given project.",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "APFDc, FT, LT, AT",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Apply ML on distribution images; consider additional factors that might affect PTP; consider time-based metrics to guide the solution."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Celik, Ahmet; Lee, Young Chul; Gligoric, Milos",
        "author_keys": [
            "celik_ahmet",
            "lee_young_chul",
            "gligoric_milos"
        ],
        "title": "Regression Test Selection for TizenRT",
        "bibtex": "celik_regression_2018",
        "abstract": "Regression testing - running tests after code modifications - is widely practiced in industry, including at Samsung. Regression Test Selection (RTS) optimizes regression testing by skipping tests that are not affected by recent code changes. Recent work has developed robust RTS tools, which mostly target managed languages, e.g., Java and C#, and thus are not applicable to large C projects, e.g., TizenRT, a lightweight RTOS-based platform. We present Selfection, an RTS tool for projects written in C; we discuss the key challenges to develop Selfection and our design decisions. Selfection uses the objdump and readelf tools to statically build a dependency graph of functions from binaries and detect modified code elements. We integrated Selfection in TizenRT and evaluated its benefits if tests are run in an emulator and on a supported hardware platform (ARTIK 053). We used the latest 150 revisions of TizenRT available on GitHub. We measured the benefits of Selfection as the reduction in the number of tests and reduction in test execution time over running all tests at each revision (i.e., RetestAll). Our results show that Selfection can reduce, on average, the number of tests to 4.95% and end-to-end execution time to 7.04% when tests are executed in the emulator, and to 5.74% and 26.82% when tests are executed on the actual hardware. Our results also show that the time taken to maintain the dependency graph and detect modified functions is negligible.",
        "published_in": "ESEC/FSE 2018: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3236024.3275527",
        "date": "2018-10-26",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "TizenRT (877 TCs)\n\nIndustrial proprietary, medium scale",
        "prog_language": "C",
        "ind_partner": "Samsung (South Korea)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, documented",
        "put_practice": "MAYBE",
        "suppl_url": "https://github.com/ahmet-celik/Selfection",
        "approach": "Change-based, graph-based",
        "info_approach": "Change-based",
        "alg_approach": "Graph-based",
        "metrics": "Selection rate, testing time, build time",
        "effe_metrics": "Selection/reduction count/percentage, Testing time",
        "effi_metrics": "Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "improve safety; evaluate on other projects; \"optimize the transfer time by incrementally patching previously transferred binaries\""
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Zhu, Yuecai; Shihab, Emad; Rigby, Peter C.",
        "author_keys": [
            "zhu_yuecai",
            "shihab_emad",
            "rigby_peter_c"
        ],
        "title": "Test re-prioritization in continuous testing environments",
        "bibtex": "zhu_test_2018",
        "abstract": "New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests based on their co-failure probability distributions. We named this approach CODYNAQ and in particular, we propose three prioritization variants called CODYNAQSINGLE, CODYNAQDOUBLE and CODYNAQFLEXI. We evaluate our approach on two data sets, CHROME and Google testing data. We find that our co-failure dynamic re-prioritization approach, CODYNAQ, outperforms the default order, FIFOBASELINE, finding the first failure and all failures for a change request by 31% and 62% faster, respectively. CODYNAQ also outperforms GOOGLETCP by finding the first failure 27% faster and all failures 62% faster. Â© 2018 IEEE.",
        "published_in": "2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSME.2018.00016",
        "date": "2018-11-12",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "GSDTSR (up to 2.4k requests per minute), data scraped from the Chromium project (up to 149 requests per minute)\n\nIndustrial open-source, large scale",
        "prog_language": "Unclear",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Co-failure distributions",
        "info_approach": "Fault-based",
        "alg_approach": "",
        "metrics": "FirstFail, AllFail, delayed failures (goal: speed up detection of first/all failure)",
        "effe_metrics": "Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Evaluate on other projects; continue to examine failure distributions; explore use of other prioritization techniques."
    },
    {
        "type": "primary",
        "year": "2018",
        "authors": "Azizi, Maral; Do, Hyunsook",
        "author_keys": [
            "azizi_maral",
            "do_hyunsook"
        ],
        "title": "Retest: A cost effective test case selection technique for modern software development",
        "bibtex": "azizi_retest_2018",
        "abstract": "Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modified version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel language-independent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efficient by selecting a far smaller subset of tests compared to the existing techniques.",
        "published_in": "2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)",
        "publisher": "IEEE",
        "doi": "10.1109/ISSRE.2018.00025",
        "date": "2018-11-19",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Two open-source .net and C# programs (up to 628 TCs) and two Java subjects from Defects4J (up to 393 TCs)\n\nOpen-source, medium scale\nResearch dataset, small scale",
        "prog_language": "C#, Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Graph-based",
        "info_approach": "",
        "alg_approach": "Graph-based",
        "metrics": "selection count, fault detection rate",
        "effe_metrics": "Selection/reduction count/percentage, Fault Detection Rate (FDR)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "automate parameter selection, apply approach on TCP and TSR."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Guo, Bo; Kwon, Young-Woo; Song, Myoungkyu",
        "author_keys": [
            "guo_bo",
            "kwon_young-woo",
            "song_myoungkyu"
        ],
        "title": "Decomposing Composite Changes for Code Review and Regression Test Selection in Evolving Software",
        "bibtex": "guo_decomposing_2019",
        "abstract": "Inspecting and testing code changes typically require a significant amount of developer effort. As a system evolves, developers often create composite changes by mixing multiple development issues, as opposed to addressing one independent issue â an atomic change. Inspecting composite changes often becomes time-consuming and error-prone. To test unrelated edits on composite changes, rerunning all regression tests may require excessive time. To address the problem, we present an interactive technique for change decomposition to support code reviews and regression test selection, called ChgCutter. When a developer specifies code change within a diff patch, ChgCutter partitions composite changes into a set of related atomic changes, which is more cohesive and self-contained regarding the issue being addressed. For composite change inspection, it generates an intermediate program version that only includes a related change subset using program dependence relationships. For cost reduction during regression testing, it safely selects only affected tests responsible for changes to an intermediate version. In the evaluation, we apply ChgCutter to 28 composite changes in four open source projects. ChgCutter partitions these changes with 95.7% accuracy, while selecting affected tests with 89.0% accuracy. We conduct a user study with professional software engineers at PayPal and find that ChgCutter is helpful in understanding and validating composite changes, scaling to industry projects. Â© 2019, Springer Science+Business Media, LLC & Science Press, China.",
        "published_in": "Journal of Computer Science and Technology ",
        "publisher": "Springer",
        "doi": "10.1007/s11390-019-1917-9",
        "date": "2019-03-22",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Generated test suite based on four open-source Java projects (totaling 3456 TCs)\n\nSynthetic data, large scale",
        "prog_language": "Java",
        "ind_partner": "PayPal (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "TRUE\n\nReplication package, with a video demo",
        "put_practice": "TRUE",
        "suppl_url": "https://sites.google.com/unomaha.edu/interactively-partitioning",
        "approach": "Change-based",
        "info_approach": "Change-based",
        "alg_approach": "",
        "metrics": "Precision/Recall",
        "effe_metrics": "Accuracy/precision/recall",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Evaluate on human-written test suites; explore further heuristics; avoid false positives."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Zhong, Hua; Zhang, Lingming; Khurshid, Sarfraz",
        "author_keys": [
            "zhong_hua",
            "zhang_lingming",
            "khurshid_sarfraz"
        ],
        "title": "TestSage: Regression test selection for large-scale Web service testing",
        "bibtex": "zhong_testsage:_2019",
        "abstract": "Regression testing is an important but expensive activity in software development. Among various types of tests, web service tests are usually one of the most expensive (due to network communications) but widely adopted types of tests in commercial software development. Regression test selection (RTS) aims to reduce the number of tests which need to be retested by only running tests that are affected by code changes. Although a large number of RTS techniques have been proposed in the past few decades, these techniques have not been adopted on large-scale web service testing. This is because most existing RTS techniques either require direct code dependency between tests and code under test or cannot be applied on large scale systems with enough efficiency. In this paper, we present a novel RTS technique, TestSage, that performs RTS for web service tests on large scale commercial software. With a small overhead, TestSage is able to collect fine grained (function level) dependency between test and service under test that do not directly depend on each other. TestSage has also been successfully applied to large complex systems with over a million functions. We conducted experiments of TestSage on a large scale backend service at Google. Experimental results show that TestSage reduces 34% of testing time when running all AEC (Analysis, Execution and Collection) phases, 50% of testing time while running without collection phase. TestSage has been integrated with internal testing framework at Google and runs day-to-day at the company.",
        "published_in": "2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",
        "publisher": "IEEE",
        "doi": "10.1109/ICST.2019.00052",
        "date": "2019-06-06",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Google Assistant (tens of thousands of tests)\n\nIndustrial proprietary, very large scale",
        "prog_language": "C, C++",
        "ind_partner": "Google (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "Trace-based",
        "info_approach": "Coverage-based, Trace-based",
        "alg_approach": "",
        "metrics": "Selection ratio, testing time reduction",
        "effe_metrics": "Testing time",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Explore more granularity levels; explore source-test dependency."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Fu, Ben; Misailovic, Sasa; Gligoric, Milos",
        "author_keys": [
            "fu_ben",
            "misailovic_sasa",
            "gligoric_milos"
        ],
        "title": "Resurgence of Regression Test Selection for C++",
        "bibtex": "fu_resurgence_2019",
        "abstract": "Regression testing - running available tests after each project change - is widely practiced in industry. Despite its widespread use and importance, regression testing is a costly activity. Regression test selection (RTS) optimizes regression testing by selecting only tests affected by project changes. RTS has been extensively studied and several tools have been deployed in large projects. However, work on RTS over the last decade has mostly focused on languages with abstract computing machines (e.g., JVM). Meanwhile development practices (e.g., frequency of commits, testing frameworks, compilers) in C++ projects have dramatically changed and the way we should design and implement RTS tools and the benefits of those tools is unknown. We present a design and implementation of an RTS technique, dubbed RTS++, that targets projects written in C++, which compile to LLVM IR and use the Google Test testing framework. RTS++ uses static analysis of a function call graph to select tests. RTS++ integrates with many existing build systems, including AutoMake, CMake, and Make. We evaluated RTS++ on 11 large open-source projects, totaling 3,811,916 lines of code. To the best of our knowledge, this is the largest evaluation of an RTS technique for C++. We measured the benefits of RTS++ compared to running all available tests (i.e., retest-all). Our results show that RTS++ reduces the number of executed tests and end-to-end testing time by 88% and 61% on average.",
        "published_in": "2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",
        "publisher": "IEEE",
        "doi": "10.1109/ICST.2019.00039",
        "date": "2019-06-06",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source C++ projects (up to 673 TCs)\n\nOpen-source, medium scale",
        "prog_language": "C++",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Change-based",
        "info_approach": "Change-based",
        "alg_approach": "",
        "metrics": "selection count, end-to-end time, cumulative time",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "Improve precision by removing certian class dependencies and dynamically collecting dependencies; support the Boost testing framework; develop adaptive and hybrid RTS for C++"
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Eda, Ravi; Do, Hyunsook ",
        "author_keys": [
            "eda_ravi",
            "do_hyunsook"
        ],
        "title": "An efficient regression testing approach for PHP Web applications using test selection and reusable constraints",
        "bibtex": "eda_efficient_2019",
        "abstract": "Web applications undergo frequent changes. These changes can be due to the addition of new features or the modification of existing features to support customer requests or to patch faults in the system. Given that Web applications have a large surface area subject to attack, changes often include security fixes either in response to malicious attacks or to forestall such attacks. Effective regression testing should ensure that any change does not disable existing features or compromise security. Executing the entire regression test suite takes time and consumes many resources. One approach is to focus regression test efforts only on code paths that were modified in the new version. Such code paths can be identified using tools such as PHP Analysis and Regression Testing Engine (PARTE). In this paper, we extend this approach to test selection where a subset of existing tests that cover the modified code paths can be detected. To further reduce the amount of regression testing needed, we used PARTEâs reusable constraint value information to identify tests that can be reused against the new version without having to modify the input test values. We performed an empirical study to determine whether test selection data combined with reusable constraint values would further improve the turnaround time for regression tests. Results from the experiment conducted on four Hypertext Preprocessor (PHP) web applications demonstrate that this approach is effective in reducing the cost of regression testing of frequently patched Web applications.",
        "published_in": "Software Quality Journal",
        "publisher": "Springer",
        "doi": "10.1007/s11219-019-09449-2",
        "date": "2019-06-11",
        "tcp": "",
        "tcs": "X",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source PHP projects (up to 124 TCs)\n\nOpen-source, small scale",
        "prog_language": "PHP",
        "ind_partner": "",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Change-based",
        "info_approach": "Change-based",
        "alg_approach": "Constraints-based",
        "metrics": "Selected tests, obsolete tests, reusable inputs",
        "effe_metrics": "Selection/reduction count/percentage",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Investigate if the approach can be applied in industrial setting; analyse long-term benefits of the technique."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Goyal, Amit; Shyamasundar, R. K.; Jetley, Raoul; Mohan, Devina; Ramaswamy, Srini",
        "author_keys": [
            "goyal_amit",
            "shyamasundar_r_k",
            "jetley_raoul",
            "mohan_devina",
            "ramaswamy_srini"
        ],
        "title": "Test suite minimization of evolving software systems: A case study",
        "bibtex": "goyal_test_2019",
        "abstract": "Test suite minimization ensures that an optimum set of test cases are selected to provide maximum coverage of requirements. In this paper, we discuss and evaluate techniques for test suite minimization of evolving software systems. As a case study, we have used an industrial tool, Static Code Analysis (SCAN) tool for Electronic Device Description Language (EDDL) as the System Under Test (SUT). We have used standard approaches including Greedy, Greedy Essential (GE) and Greedy Redundant Essential (GRE) for minimization of the test suite for a given set of requirements of the SUT. Further, we have proposed and implemented k-coverage variants of these approaches. The minimized test suite which is obtained as a result reduces testing effort and time during regression testing. The paper also addresses the need for choosing an appropriate level of granularity of requirements to efficiently cover all requirements. The paper demonstrates how fine grained requirements help in finding an optimal test suite to completely address the requirements and also help in detecting bugs in each version of the software. Finally, the results from different analyses have been presented and compared and it has been observed that GE heuristics performs the best (run time) under certain conditions. Copyright Â© 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",
        "published_in": "Proceedings of the 14th International Conference on Software Technologies - ICSOFT",
        "publisher": "SciTePress",
        "doi": "10.5220/0007842502260237",
        "date": "2019-07-28",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "SCAN tool from ABB\n\nIndustrial proprietary, unclear scale",
        "prog_language": "Unclear",
        "ind_partner": "ABB (India/USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "requirements-based",
        "info_approach": "Requirements-based",
        "alg_approach": "",
        "metrics": "reduction rate, runtime",
        "effe_metrics": "Selection/reduction count/percentage, Testing time",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "generation with similar approach; consider time of each test case."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Yu, Zhe; Fahid, Fahmid; Menzies, Tim; Rothermel, Gregg; Patrick, Kyle; Cherian, Snehit",
        "author_keys": [
            "yu_zhe",
            "fahid_fahmid",
            "menzies_tim",
            "rothermel_gregg",
            "patrick_kyle",
            "cherian_snehit"
        ],
        "title": "TERMINATOR: better automated UI test case prioritization",
        "bibtex": "yu_terminator_2019",
        "abstract": "Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process.\nTo mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is \"black box\" in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 \"black box\" test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead.",
        "published_in": "ESEC/FSE 2019: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3338906.3340448",
        "date": "2019-08-12",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Dataset from LexisNexis (2661 TCs)\n\nIndustrial proprietary, large scale",
        "prog_language": "Gherkin, Java",
        "ind_partner": "LexisNexis (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "MAYBE\n\nOnly results",
        "put_practice": "TRUE",
        "suppl_url": "https://github.com/ai-se/Data-for-automated-UI-testing-from-LexisNexis",
        "approach": "Coverage, history, cost, description, feedback",
        "info_approach": "History-based, Coverage-based, Cost-aware, Manual classification",
        "alg_approach": "",
        "metrics": "Coverage, APFD, APFDc, failure detection rate",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Fault Detection Rate (FDR), Coverage Effectiveness (CE)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "identify flaky tests, identify fault location, apply approach to different TCP problems"
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Correia, Daniel; Abreu, Rui; Santos, Pedro; Nadkarni, Joo",
        "author_keys": [
            "correia_daniel",
            "abreu_rui",
            "santos_pedro",
            "nadkarni_joo"
        ],
        "title": "MOTSD: A multi-objective test selection tool using test suite diagnosability",
        "bibtex": "correia_motsd_2019",
        "abstract": "Performing regression testing on large software systems becomes unfeasible as it takes too long to run all the test cases every time a change is made. The main motivation of this work was to provide a faster and earlier feedback loop to the developers at OutSystems when a change is made. The developed tool, MOTSD, implements a multi-objective test selection approach in a C# code base using a test suite diagnosability metric and historical metrics as objectives and it is powered by a particle swarm optimization algorithm. We present implementation challenges, current experimental results and limitations of the tool when applied in an industrial context. Screencast demo link: \\textlessa\\textgreaterhttps://www.youtube.com/watch?v=CYMfQTUu2BE\\textless/a\\textgreater Â© 2019 ACM.",
        "published_in": "ESEC/FSE 2019: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3338906.3341187",
        "date": "2019-08-12",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "OutSystems codebase in C# (over 8500 TCs)\n\nIndustrial proprietary, large scale",
        "prog_language": "C#",
        "ind_partner": "OutSystems (Portugal)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well documented",
        "put_practice": "TRUE",
        "suppl_url": "http://github.com/danielcorreia96/MOTSD",
        "approach": "multi-objective, particle swarm optimization",
        "info_approach": "",
        "alg_approach": "Search-based",
        "metrics": "Diagnosability",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "Diagnosability",
        "open_challenges": "use data other than coverage (e.g. dependency graph); questions regarding evaluation; common benchmark for this type of tool."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Machalica, Mateusz; Samylkin, Alex; Porth, Meredith; Chandra, Satish",
        "author_keys": [
            "machalica_mateusz",
            "samylkin_alex",
            "porth_meredith",
            "chandra_satish"
        ],
        "title": "Predictive Test Selection",
        "bibtex": "machalica_predictive_2018",
        "abstract": "Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95% of individual test failures and over 99.9% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.",
        "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSE-SEIP.2019.00018",
        "date": "2019-08-19",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Facebook mobile app repository (undisclosed scale) \n\nIndustrial proprietary, supposedly large scale",
        "prog_language": "Multi-language",
        "ind_partner": "Facebook (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "",
        "approach": "machine learning-based, history-based",
        "info_approach": "History-based",
        "alg_approach": "Machine learning-based",
        "metrics": "Recall, selection rate",
        "effe_metrics": "Selection/reduction count/percentage, Accuracy/precision/recall",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Incorporate more features into the ML model; more sophisticated ML algorithms and models; correlation between tests that are impacted by the same code."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Najafi, Armin; Shang, Weiyi; Rigby, Peter C.",
        "author_keys": [
            "najafi_armin",
            "shang_weiyi",
            "rigby_peter_c"
        ],
        "title": "Improving Test Effectiveness Using Test Executions History: An Industrial Experience Report",
        "bibtex": "najafi_improving_2019",
        "abstract": "The cost of software testing has become a burden for software companies in the era of rapid release and continuous integration. Our industrial collaborator Ericsson also faces the challenges of expensive testing processes which are typically part of a complex and specialized testing environment. In order to assist Ericsson with improving the test effectiveness of one of its large subsystems, we adopt test selection and prioritization approaches based on test execution history from prior research. By adopting and simulating those approaches on six months of testing data from our subject system, we confirm the existence of valuable information in the test execution history. In particular, the association between test failures provide the most value to the test selection and prioritization processes. More importantly, during this exercise, we encountered various challenges that are unseen or undiscussed in prior research. We document the challenges, our solutions and the lessons learned as an experience report. Our experiences can be valuable for other software testing practitioners and researchers who would like to adopt existing test effectiveness improvement approaches into their work environment.",
        "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSE-SEIP.2019.00031",
        "date": "2019-08-19",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Undisclosed large-scale system at Ericsson\n\nIndustrial proprietary, supposedly large-scale",
        "prog_language": "Unclear (\"modern typical programming language\")",
        "ind_partner": "Ericsson (Canada)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "history-based",
        "info_approach": "History-based",
        "alg_approach": "",
        "metrics": "Failure frequency, failure association, cost, execution time, missed failures, execution cost",
        "effe_metrics": "Fault Detection Loss",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Evaluate best duration of training data; consider dependencies among tests."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Leong, Claire; Singh, Abhayendra; Papadakis, Mike; Le Traon, Yves; Micco, John",
        "author_keys": [
            "leong_claire",
            "singh_abhayendra",
            "papadakis_mike",
            "le_traon_yves",
            "micco_john"
        ],
        "title": "Assessing Transition-Based Test Selection Algorithms at Google",
        "bibtex": "leong_assessing_2019",
        "abstract": "Continuous Integration traditionally relies on testing every code commit with all impacted tests. This practice requires considerable computational resources, which at Google scale, results in delayed test results and high operational costs. To deal with this issue and provide fast feedback, test selection and prioritization methods aim to execute the tests which are most likely to reveal changes in test results as soon as possible. In this paper we present a simulation framework to support the study and evaluation, with real data, of such techniques. We propose a test selection algorithm evaluation method, and detail several practical requirements which are often ignored by related work, such as the detection of transitions, the collection and analysis of data, and the handling of flaky tests. Based on this framework, we design an experiment evaluating five potential regression test selection algorithms, based on simple heuristics and inspired by previous research, though the evaluation technique is applicable to any number of algorithms for future experiments. Our results show that algorithms based on the recent (transition) execution history do not perform as well as expected (given the previously reported results) and that the test selection problem remains largely open. We found that the best performing algorithms are based on the number of times a test has been triggered and the number of distinct authors committing code that triggers particular tests. More research is needed in order to close the gap between the current approaches and the optimal solution.",
        "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSE-SEIP.2019.00019",
        "date": "2019-08-19",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Dataset from Google Test Automation Platform\n\nIndustrial proprietary, supposedy very large scale",
        "prog_language": "Unclear",
        "ind_partner": "Google (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "transition-based (apparently a mix of history-based and change-based)",
        "info_approach": "Fault-based, Author count",
        "alg_approach": "",
        "metrics": "Transition count, affected count, author count",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Causal relationships between algorithms and results; algorithms included do not fully solve the problem; alternative uses of historical data"
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Cruciani, Emilio; Miranda, Breno; Verdecchia, Roberto; Bertolino, Antonia",
        "author_keys": [
            "cruciani_emilio",
            "miranda_breno",
            "verdecchia_roberto",
            "bertolino_antonia"
        ],
        "title": "Scalable Approaches for Test Suite Reduction",
        "bibtex": "cruciani_scalable_2019",
        "abstract": "Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or specification models, that are not commonly available at large scale. We present a family of novel very efficient approaches for similaritybased test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for finding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input).We evaluate four approaches in a version that selects a fixed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some fixed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency. When applied to a suite of more than 500K real world test cases, the most efficient of the four approaches could select B test cases (for varying B values) in less than 10 seconds.",
        "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSE.2019.00055",
        "date": "2019-08-26",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "SIR and Defects4J\nGenerated test suite (500K+ tests)\n\nResearch dataset, medium to large scale\nSynthetic data, very large scale",
        "prog_language": "C, Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well documented",
        "put_practice": "FALSE",
        "suppl_url": "https://zenodo.org/record/2550079\n\nhttps://github.com/ICSE19-FAST-R/FAST-R/tree/v1.0.0\n",
        "approach": "Similarity-based",
        "info_approach": "Test code",
        "alg_approach": "Similarity / distance-based",
        "metrics": "Fault detection loss, test suite reduction, time",
        "effe_metrics": "Fault Detection Loss",
        "effi_metrics": "Execution time, Scalability",
        "other_metrics": "",
        "open_challenges": "Filter out test cases that do not impact modified files; more efficient heuristics."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Philip, Adithya Abraham; Bhagwan, Ranjita; Kumar, Rahul; Maddila, Chandra Sekhar; Nagppan, Nachiappan",
        "author_keys": [
            "philip_adithya_abraham",
            "bhagwan_ranjita",
            "kumar_rahul",
            "maddila_chandra_sekhar",
            "nagppan_nachiappan"
        ],
        "title": "FastLane: Test Minimization for Rapidly Deployed Large-Scale Online Services",
        "bibtex": "philip_fastlane:_2019",
        "abstract": "Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases. This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%.",
        "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSE.2019.00054",
        "date": "2019-08-26",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Microsoft Office 365 (tens of thousands of TCs)\n\nIndustrial proprietary, very large scale",
        "prog_language": "C#",
        "ind_partner": "Microsoft (India)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "MAYBE",
        "suppl_url": "",
        "approach": "machine learning-based",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "Precision/Recall, time saved",
        "effe_metrics": "Testing time, Accuracy/precision/recall",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "n/a"
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "MagalhÃ£es, Claudio; Andrade, JoÃ£o; Perrusi, Lucas; Mota, Alexandre; Barros, FlÃ¡via; Maia, Eliot",
        "author_keys": [
            "magalhaes_claudio",
            "andrade_joao",
            "perrusi_lucas",
            "mota_alexandre",
            "barros_flavia",
            "maia_eliot"
        ],
        "title": "HSP: A hybrid selection and prioritisation of regression test cases based on information retrieval and code coverage applied on an industrial case study",
        "bibtex": "magalhaes_hsp_2020",
        "abstract": "The usual way to guarantee quality of software products is via testing. This paper presents a novel strategy for selection and prioritisation of Test Cases (TC) for Regression testing. In the lack of code artifacts from where to derive Test Plans, this work uses information conveyed by textual documents maintained by Industry, such as Change Requests. The proposed process is based on Information Retrieval techniques combined with indirect code coverage measures to select and prioritise TCs. The aim is to provide a high coverage Test Plan which would maximise the number of bugs found. This process was implemented as a prototype tool which was used in a case study with our industrial partner (Motorola Mobility). Experiments results revealed that the combined strategy provides better results than the use of information retrieval and code coverage independently. Yet, it is worth mentioning that any of these automated options performed better than the previous manual process deployed by our industrial partner to create test plans.",
        "published_in": "Journal of Systems and Software\nVolume 159, January 2020",
        "publisher": "Elsevier",
        "doi": "10.1016/j.jss.2019.110430",
        "date": "2019-09-24",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial case studies\n\n(88 to 302 test cases)\n\nIndustrial proprietary, small scale",
        "prog_language": "Natural language",
        "ind_partner": "TRUE\nMotorola Mobility LLC",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE",
        "suppl_url": "FALSE",
        "approach": "Hybrid Selection and Prioritisation based on Information Retrieval and Code Coverage",
        "info_approach": "Coverage-based",
        "alg_approach": "Search-based",
        "metrics": " - Code coverage\n - Correlation\n - CR (change request) coverage",
        "effe_metrics": "Coverage Effectiveness (CE)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": " - investigating other prioritisation strategies\n - propose strategies / approaches to create and suggest new test cases or scenarios, in order to increase the coverage"
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Wu, Zhaolin; Yang, Yang; Li, Zheng; Zhao, Ruilian",
        "author_keys": [
            "wu_zhaolin",
            "yang_yang",
            "li_zheng",
            "zhao_ruilian"
        ],
        "title": "A Time Window Based Reinforcement Learning Reward for Test Case Prioritization in Continuous Integration",
        "bibtex": "wu_time_2019",
        "abstract": "Continuous integration refers to the practice of merging the working copies of all developers into the mainline frequently. Regression testing for each mergence is characterized by continually changing test suite, limited execution time, and fast feedback, which demands new test optimization techniques. Reinforcement learning is introduced for test case prioritization to save computing resources in continuous integration environment, where a reasonable reward function is highly important for learning strategy, since the process of reinforcement learning is a reward-guided behavior. In this paper, APHFW, a novel reward function is proposed by using partial historical information of test cases effectively for fast feedback and cost reduction. The experiments are based on three open-source data sets, and the results show that the proposed reward function is more cost-effect than other reinforcement learning rewards in continuous integration environment.",
        "published_in": "Internetware '19: Proceedings of the 11th Asia-Pacific Symposium on Internetware",
        "publisher": "ACM",
        "doi": "10.1145/3361242.3361258",
        "date": "2019-10-28",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "GSDTSR (5555 TCs) plus datasets from ABB Robotics (up to 2086 TCs)\n\nIndustrial open-source, large scale",
        "prog_language": "Unclear",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "TCP by Reinforcement Learning, the paper experiments different windows on which the reward is calculated, in cmparison with Speiker (only recent observation) and He (full history)",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "NAPFD\nTime cost of prioritization function",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "use windows of dynamic size"
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Land, Kathrin; Neumann, Eva-Maria; Ziegltrum, Simon; Li, Huaxia; Vogel-Heuser, Birgit",
        "author_keys": [
            "land_kathrin",
            "neumann_eva-maria",
            "ziegltrum_simon",
            "li_huaxia",
            "vogel-heuser_birgit"
        ],
        "title": "An Industrial Evaluation of Test Prioritisation Criteria and Metrics",
        "bibtex": "land_industrial_2019",
        "abstract": "Automated production systems become more and more complex. This makes it\n increasingly difficult to keep track of performed changes and already \nexecuted test cases. This endangers the systems quality as the risk of \nmissing important test cases while planning the test execution is high, \nespecially for testers with little experience. To face this challenge, \ntesters should be supported by an automatic test prioritisation based on\n metrics in selecting the right test cases for the test execution. In \nindustry, many different test prioritisation criteria and strategies are\n used for this purpose. In an industrial interview, experts discussed \nand ranked prioritisation criteria that are currently used within the \nrespective companies. As a result, this paper presents the cactus \nprioritisation model, which graphically resembles the industrial ranking\n and weighting of the criteria. Based on the prioritisation cactus and \nits criteria, a simple prioritisation metric is introduced to determine \nthe utility of each test case regarding the system under test. The test \ncases are prioritised according to their descending utility. \nFurthermore, approaches and metrics to realise the different individual \nprioritisation criteria are proposed.",
        "published_in": "2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)\n ",
        "publisher": "IEEE",
        "doi": "10.1109/SMC.2019.8914505",
        "date": "2019-11-28",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "N/A",
        "exp_subjects": "N/A",
        "prog_language": "",
        "ind_partner": "Undisclosed industrial partner (Germany)",
        "ind_author": "FALSE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "N/A",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "N/A",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "When prioritizing, should the focus be entirely on the safety-critical tests, or should some other tests be sprinkled in-between?"
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Noemmer, Raphael; Haas, Roman",
        "author_keys": [
            "noemmer_raphael",
            "haas_roman"
        ],
        "title": "An Evaluation of Test Suite Minimization Techniques",
        "bibtex": "noemmer_evaluation_2020",
        "abstract": "As a software project evolves over time, the associated test suite usually grows with it. If test suites are not carefully maintained, this can easily result in massive test execution duration, reducing the benefits of regression testing because faults are found later in development or even after release. Test suite minimization aims to combat long running test suites by removing redundant test cases. Previous work mainly evaluates test suite minimization techniques based on comparably small projects, which are less practically relevant. In this paper, we compare four test suite minimization techniques by applying them to several open source software projects and evaluate the results. We find that the size and execution time of all the test suites can be reduced by over 70% on average. However, there is a substantial loss in fault detection capability of, on average, around 12.5%, restricting the applicability of this form of test suite minimization. Â© Springer Nature Switzerland AG 2020.",
        "published_in": "SWQD 2020: International Conference on Software Quality",
        "publisher": "Springer",
        "doi": "10.1007/978-3-030-35510-4_4 ",
        "date": "2019-12-09",
        "tcp": "",
        "tcs": "",
        "tsr": "X",
        "tsa": "",
        "ind_motivation": "FALSE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 14770 TCs)\n\nOpen-source, very large scale",
        "prog_language": "Java",
        "ind_partner": "CQSE (Germany)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "basic greedy and the HGS\nalgorithm (guided by statement coverage only)",
        "info_approach": "Coverage-based",
        "alg_approach": "Greedy",
        "metrics": "-test suite reduction achieved\n-Impact on fault detection capability\n-execution time of the reduced test suite (important finding: \"The reduction in number of tests appears to be a bad indicator for the reduction in execution time\")\n",
        "effe_metrics": "Selection/reduction count/percentage, Testing time, Fault Detection Loss",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Investigate multiple objective-based algorithms; increase variety of study subjects; evaluate TSR with historical data rather than mutation testing."
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "LÃ¼bke, Daniel",
        "author_keys": [
            "lubke_daniel"
        ],
        "title": "Selecting and Prioritizing Regression Test Suites by Production Usage Risk in Time-Constrained Environments",
        "bibtex": "lubke_selecting_2020",
        "abstract": "Regression Testing is an important quality assurance activity for combating unwanted side-effects, which might have been introduced in a new software release. Selecting and prioritizing regression test cases is a challenge in practice â especially in a world of ever increasing complex- ity, distribution, and size of the software solutions. Current approaches try to minimize the number of regression test cases by analyzing the change and the coverage of the tests with regards to this change. Our approach utilizes usage frequencies from the previous, productive soft- ware version in order to select or prioritize test cases by calculating the Regression Risk of a change. This takes into account that not all features of a software are used the same. We successfully validate our approach in a case study of an industry project which develops a complex process integration platform.",
        "published_in": "SWQD 2020: Software Quality: Quality Intelligence in Software and Systems Engineering",
        "publisher": "Springer",
        "doi": "10.1007/978-3-030-35510-4_3 ",
        "date": "2019-12-09",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Dataset from Terravis (375 TCs)\n\nIndustrial proprietary, small scale (in number of tests, long execution time)",
        "prog_language": "WS-BPEL 2.0",
        "ind_partner": "Terravis (Switzerland)",
        "ind_author": "FALSE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "\"risk-coverage\"-based",
        "info_approach": "Coverage-based",
        "alg_approach": "",
        "metrics": "Accumulated covered regression risk",
        "effe_metrics": "Coverage Effectiveness (CE), Accumulated regression risk",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "combine with other TCP/TCS strategies; study differences between minor and major releases w.r.t. regression risk."
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Yackley, Jeffrey J.; Kessentini, Marouane; Bavota, Gabriele; Alizadeh, Vahid; Maxim, Bruce R.",
        "author_keys": [
            "yackley_jeffrey_j",
            "kessentini_marouane",
            "bavota_gabriele",
            "alizadeh_vahid",
            "maxim_bruce_r"
        ],
        "title": "Simultaneous refactoring and regression testing",
        "bibtex": "yackley_simultaneous_2019",
        "abstract": "Currently, refactoring and regression testing are treated independently by existing studies. However, software developers frequently switch between these two activities, using regression testing to identify unwanted behavior changes introduced while refactoring and applying refactoring on identified buggy code fragments. Our hypothesis is that the tools to support developers in these two tasks could transfer part of the knowledge extracted from the process of finding refactoring opportunities to identify relevant test cases, and vice-versa. We propose a simultasking, search-based algorithm that unifies the tasks of refactoring and regression testing, hence solving them simultaneously and enabling knowledge transfer between them. The salient feature of the proposed algorithm is a unified and generic solution representation scheme for both problems, which serves as a common platform for knowledge transfer between them. We implemented and evaluated the proposed simultasking approach on six opensource systems and one industrial project. Our study features quantitative and qualitative analysis performed with developers, and the results achieved show that the proposed approach provides advantages over mono-task techniques treating refactoring and regression testing separately. Â© 2019 IEEE.",
        "published_in": "2019 19th International Working Conference on Source Code Analysis and Manipulation (SCAM)",
        "publisher": "IEEE",
        "doi": "10.1109/SCAM.2019.00032",
        "date": "2019-12-12",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "FALSE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 128 TCs)\n\nOpen-source, small scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Search-based",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "quality metric based on reusability, flexibility, understandability and effectiveness (but this is a metric for the refactoring)",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Validate technique with additional refactoring types, languages, code smells, etc."
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Dirim, Sahin; Sozer, Hasan",
        "author_keys": [
            "dirim_sahin",
            "sozer_hasan"
        ],
        "title": "Prioritization of Test Cases with Varying Test Costs and Fault Severities for Certification Testing",
        "bibtex": "dirim_prioritization_2020",
        "abstract": "We present an industrial case study on the application of test case prioritization techniques in the context of certification testing in consumer electronics domain. Test execution times and fault severities are subject to high variations in this domain. As a result, most of the existing techniques and metrics turn out to be inappropriate for this application context. We discuss such deficiencies and the room for improvement based on our case study with the certification test suites of 3 Smart TV applications as real experimental objects. We also propose a new metric, LAPFD, which is based on the calculation of the average of the percentage of faults detected. This calculation is weighted according to the cost of test cases and calculated separately per severity class. Then, a lexicographic ordering is performed based on these classes. We compared the baseline (random) ordering of test cases with respect to an alternative ordering based on cost, measured as the test execution time. These alternative orderings are evaluated by using the LAPFD metric. We observed that cost-based ordering of test cases consistently outperformed random ordering. Another observation is that there is a large room for improvement regarding the effectiveness of test case prioritization in this application domain.",
        "published_in": "2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSTW50294.2020.00069",
        "date": "2020-08-04",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Smart TV applications for Netflix, YouTube and Prime Video (up to 213 TCs â but up to 40 hours)\n\nIndustrial proprietary",
        "prog_language": "Unclear",
        "ind_partner": "Vestel Electronics (Turkey)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "MAYBE",
        "suppl_url": "",
        "approach": "very simple, prioritize test cases based on their execution time (shortest first)",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "LAPFD (variant of APFD the considers cost for each of 4 severity classes)",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "consider other TCP techniques as lexographic or history based"
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Prado Lima, Jackson A.; Vergilio, Silvia R.",
        "author_keys": [
            "prado_lima_jackson_a",
            "vergilio_silvia_r"
        ],
        "title": "Multi-Armed Bandit Test Case Prioritization in Continuous Integration Environments: A Trade-off Analysis",
        "bibtex": "lima_multi-armed_2020",
        "abstract": "Continuous Integration (CI) practices lead the software to be integrated and tested many times a day, usually subject to a test budget. To deal with this scenario, cost-effective test case prioritization techniques are required. COLEMAN is a Multi-Armed Bandit approach that learns from the test case failure-history the best prioritization order to maximize early fault detection. Reported results show that COLEMAN has reached promising results with different test budgets and spends, in the worst case, less than one second to execute. However, COLEMAN has not been evaluated against a search-based approach. Such an approach can generate near-optimal solutions but is not suitable to the CI budget because it takes too long to execute. Considering this fact, this paper analyses the trade-offs of the COLEMAN solutions in comparison with the near-optimal solutions generated by a Genetic Algorithm (GA). We use measures, which better fit with time constraints: Normalized Average Percentage of Faults Detected (NAPFD), Root-Mean-Square-Error (RMSE), and Prioritization Time. We use seven large-scale real-world software systems, and three different test budgets, 10%, 50%, and 80% of the total time required to execute the test set available for a CI cycle. COLEMAN obtains solutions near to the GA solutions in 90% of the cases, but scenarios with high volatility of test cases and a small number of cycles hamper the prioritization.",
        "published_in": "SAST 20: Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing",
        "publisher": "ACM",
        "doi": "10.1145/3425174.3425210",
        "date": "2020-10-20",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source programs (up to 2391 TCs)\n\nOpen-source, large scale",
        "prog_language": "Unclear, seemingly Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "reinforcement learning based",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "NAPFD, RMSE (root-mean-square-error), prioritization time",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": ""
    },
    {
        "type": "primary",
        "year": "2019",
        "authors": "Shi, August; Zhao, Peiyuan; Marinov, Darko",
        "author_keys": [
            "shi_august",
            "zhao_peiyuan",
            "marinov_darko"
        ],
        "title": "Understanding and improving regression test selection in continuous integration",
        "bibtex": "shi_understanding_2019",
        "abstract": "Developers rely on regression testing in their continuous integration (CI) environment to find changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS. In this paper, we compare module-and class-level RTS techniques in a cloud-based CI environment, Travis. We also develop and evaluate a hybrid RTS technique that combines aspects of the module-and class-level RTS techniques. We evaluate all the techniques on real Travis builds. We find that the RTS techniques do save testing time compared to running all tests (RetestAll), but the percentage of time for a full build using RTS (76.0%) is not as low as found in previous work, due to the extra overhead in a cloud-based CI environment. Moreover, we inspect test failures from RetestAll builds, and although we find that RTS techniques can miss to select failed tests, these test failures are almost all flaky test failures. As such, RTS techniques provide additional value in helping developers avoid wasting time debugging failures not related to the recent code changes. Overall, our results show that RTS can be beneficial for the developers in the CI environment, and RTS not only saves time but also avoids misleading developers by flaky test failures.",
        "published_in": "2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)",
        "publisher": "IEEE",
        "doi": "10.1109/ISSRE.2019.00031",
        "date": "2020-02-10",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 335 TCs)\n\nOpen-source, small scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "change-based",
        "info_approach": "Change-based",
        "alg_approach": "",
        "metrics": "Selection rate, time savings",
        "effe_metrics": "Selection/reduction count/percentage, Testing time",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "n/a"
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "Lima, Jackson A. Prado; Vergilio, Silvia Regina",
        "author_keys": [
            "lima_jackson_a_prado",
            "vergilio_silvia_regina"
        ],
        "title": "A Multi-Armed Bandit Approach for Test Case Prioritization in Continuous Integration Environments",
        "bibtex": "lima_multi-armed_2022",
        "abstract": "Continuous Integration (CI) environments have been increasingly adopted in the industry to allow frequent integration of software changes, making software evolution faster and cost-effective. In such environments, Test Case Prioritization (TCP) techniques play an important role to reduce regression testing costs, establishing a test case execution order that usually maximizes early fault detection. Existing works on TCP in CI environments (TCPCI) present some limitations. Few pieces of work consider CI particularities, such as the test case volatility, that is, they do not consider the dynamic environment of the software life-cycle in which new test cases can be added or removed (discontinued), characteristic related to the Exploration versus Exploitation (EvE) dilemma. To solve such a dilemma an approach needs to balance: i) the diversity of test suite; and ii) the quantity of new test cases and test cases that are error-prone or that comprise high fault-detection capabilities. To deal with this, most approaches use, besides the failure-history, other measures that rely on code instrumentation or require additional information, such as testing coverage. However, to maintain the information updated can be difficult and time-consuming, not scalable due to the test budget of CI environments. In this context, and to properly deal with the TCPCI problem, this work presents an approach based on Multi-Armed Bandit (MAB) called COLEMAN (Combinatorial VOlatiLE Multi-Armed BANdit). The TCPCI problem falls into the category of volatile and combinatorial MAB, because multiple arms (test cases) need to be selected, and they are added or removed over the cycles. We conducted an evaluation considering three time budgets and eleven systems. The results show the applicability of our approach and that COLEMAN outperforms the most similar approach from literature in terms of early fault detection and performance.",
        "published_in": "IEEE Transactions on Software Engineering",
        "publisher": "IEEE",
        "doi": "10.1109/TSE.2020.2992428",
        "date": "2020-05-04",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial open-source, medium to large scale\nOpen-source, small to large scale\n\nABB Robotics dataset (1.9k TCs)\n\nGSDTSR (Google) dataset (5.5k TCs)\n\nGitHub projects (117 - 2.4k TCs)",
        "prog_language": "Unclear",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "MAYBE\n\nDataset and results",
        "put_practice": "FALSE",
        "suppl_url": "https://osf.io/wmcbt/",
        "approach": "reinforcement learning based",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "NAPFD, APFDc, RMSE (root-mean-square-error), prioritization time, NTR (normalized time reduction), RFTC (Rank of Failing Test Cases)",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Time/tests To First Failure, Fault detection within a budget",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Apply approach in industrial scenario; consider including individual test case time; evaluate scalability as nÂº of TCs vs prioritization time; provide COLEMAN as an open-source API or plug-in."
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Zhou, Zhi Quan; Liu, Chen; Chen, Tsong Yueh; Tse, T. H.; Susilo, Willy",
        "author_keys": [
            "zhou_zhi_quan",
            "liu_chen",
            "chen_tsong_yueh",
            "tse_t_h",
            "susilo_willy"
        ],
        "title": "Beating Random Test Case Prioritization",
        "bibtex": "zhou_beating_2020",
        "abstract": "Existing test case prioritization (TCP) techniques have limitations when\n applied to real-world projects, because these techniques require \ncertain information to be made available before they can be applied. For\n example, the family of input-based TCP techniques are based on test \ncase values or test script strings; other techniques use test coverage, \ntest history, program structure, or requirements information. Existing \ntechniques also cannot guarantee to always be more effective than random\n prioritization (RP) that does not have any precondition. As a result, \nRP remains the most applicable and most fundamental TCP technique. This \narticle proposes an extremely simple, effective, and efficient way to \nprioritize test cases through the introduction of a dispersity metric. \nOur technique is as applicable as RP. We conduct empirical studies using\n 43 different versions of 15 real-world projects. Empirical results show\n that our technique is more effective than RP. Our algorithm has a \nlinear computational complexity and, therefore, provides a practical \nsolution to the problem of prioritizing very large test suites (such as \nthose containing hundreds of thousands, or millions, of test cases), \nwhere the execution time of conventional nonlinear prioritization \nalgorithms can be prohibitive. Our technique also provides a practical \nsolution to TCP when neither input-based nor execution-based techniques \nare applicable due to lack of information.",
        "published_in": "IEEE Transactions on Reliability",
        "publisher": "IEEE",
        "doi": "10.1109/TR.2020.2979815",
        "date": "2020-06-16",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Major open-source multi language projects, including Firefox (480575 TCs) and SQLite (787530 TCs), plus SIR\n\nOpen-source, very large scale\nResearch dataset, very large scale",
        "prog_language": "C, C++, Ada, Java, sh, Perl, Lisp",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Similarity-based",
        "info_approach": "",
        "alg_approach": "Similarity / distance-based",
        "metrics": "Applicability, APFD, F-measure, time till first failure",
        "effe_metrics": "Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "Applicability/Generality",
        "open_challenges": "More subject programs; leverage structural information of test suites; perform empirical investigation of obs. I (neighboring tests tend to be related/similar)"
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Peng, Qianyang; Shi, August; Zhang, Lingming",
        "author_keys": [
            "peng_qianyang",
            "shi_august",
            "zhang_lingming"
        ],
        "title": "Empirically revisiting and enhancing IR-based test-case prioritization",
        "bibtex": "peng_empirically_2020",
        "abstract": "Test-case prioritization (TCP) aims to detect regression bugs faster via reordering the tests run. While TCP has been studied for over 20 years, it was almost always evaluated using seeded faults/mutants as opposed to using real test failures. In this work, we study the recent change-aware information retrieval (IR) technique for TCP. Prior work has shown it performing better than traditional coverage-based TCP techniques, but it was only evaluated on a small-scale dataset with a cost-unaware metric based on seeded faults/mutants. We extend the prior work by conducting a much larger and more realistic evaluation as well as proposing enhancements that substantially improve the performance. In particular, we evaluate the original technique on a large-scale, real-world software-evolution dataset with real failures using both cost-aware and cost-unaware metrics under various configurations. Also, we design and evaluate hybrid techniques combining the IR features, historical test execution time, and test failure frequencies. Our results show that the change-aware IR technique outperforms stateof-the-art coverage-based techniques in this real-world setting, and our hybrid techniques improve even further upon the original IR technique. Moreover, we show that flaky tests have a substantial impact on evaluating the change-aware TCP techniques based on real test failures.\n",
        "published_in": "ISSTA 2020: Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "publisher": "ACM",
        "doi": "10.1145/3395363.3397383",
        "date": "2020-07-18",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source Java projects (up to 144 TCs)\n\nOpen-source, small scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "MAYBE\n\nDataset and results",
        "put_practice": "FALSE",
        "suppl_url": "https://sites.google.com/view/ir-based-tcp",
        "approach": "Information retrieval; the idea is that code changes can be used to construct queries which in turn lead to related tests by finding textual similarities (e.g. variable names, function calls).",
        "info_approach": "Test code",
        "alg_approach": "Search-based",
        "metrics": "APFD, APFDc",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": ""
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Bertolino, Antonia; Guerriero, Antonio; Miranda, Breno; Pietrantuono, Roberto; Russo, Stefano",
        "author_keys": [
            "bertolino_antonia",
            "guerriero_antonio",
            "miranda_breno",
            "pietrantuono_roberto",
            "russo_stefano"
        ],
        "title": "Learning-to-rank vs ranking-to-learn: Strategies for regression testing in continuous integration",
        "bibtex": "bertolino_learning--rank_2020",
        "abstract": "In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-to-rank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.",
        "published_in": "ICSE '20: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering",
        "publisher": "ACM/IEEE",
        "doi": "10.1145/3377811.3380369",
        "date": "2020-10-01",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "six Java subjects from the open-source\nApache Commons project (up to 4864 TCs)\n\nOpen-source, small to large scale",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/icse20/RT-CI",
        "approach": "evaluation of ten machine learning algorithms for test prioritization after selection in CI.",
        "info_approach": "",
        "alg_approach": "Machine learning-based, Graph-based",
        "metrics": "- Rank Percentile Average (RPA)\n- end-to-end time: time for test selection + time for prioritization + time for the execution of the selected tests",
        "effe_metrics": "Rank Percentile Average (RPA)",
        "effi_metrics": "Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "Explore agressive reductions in large-scale systems"
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Chen, Yizhen; Chen, Mei-Hwa",
        "author_keys": [
            "chen_yizhen",
            "chen_mei-hwa"
        ],
        "title": "Multi-objective regression test selection",
        "bibtex": "chen_multi-objective_2021",
        "abstract": "Regression testing is challenging, yet essential, for maintaining evolving complex software. Efficient regression testing that minimizes the regression testing time and maximizes the detection of the regression faults is in great demand for fast-paced software development. Many research studies have been proposed for selecting regression tests under a time constraint. This paper presents a new approach that first evaluates the fault detectability of each regression test based on the extent to which the test is impacted by the changes. Then, two optimization algorithms are proposed to optimize a multi-objective function that takes fault detectability and execution time of the test as inputs to select an optimal subset of the regression tests that can detect maximal regression faults under a given time constraint. The validity and efficacy of the approach were evaluated using two empirical studies on industrial systems. The promising results suggest that the proposed approach has great potential to ensure the quality of the fast-paced evolving systems. Â© 2021, EasyChair. All rights reserved.",
        "published_in": "SEDE 2020. 29th International Conference on Software Engineering and Data Engineering",
        "publisher": "EasyChair",
        "doi": "10.29007/7z5n",
        "date": "2021-03-01",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, small scale\n\nIdea Thread Mapper (142k LOC, 397-412 TCs)\n\nMicroarray (21k LOC, 84 TCs)\n\nCEDCD (32k LOC, 190 TCs)",
        "prog_language": "Unclear",
        "ind_partner": "Idea Thread Mapper (USA)\n\nNational Cancer Institute (USA)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Multi-objective (fault detection + cost constrained)",
        "info_approach": "Cost-aware, Fault-based",
        "alg_approach": "",
        "metrics": "faults detected per given budget",
        "effe_metrics": "Fault Detection Loss",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Define program states for other languages."
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Rosenbauer, Lukas; Stein, Anthony; HÃ¤hner, JÃ¶rg",
        "author_keys": [
            "rosenbauer_lukas",
            "stein_anthony",
            "hahner_jorg"
        ],
        "title": "An Artificial Immune System for Black Box Test Case Selection",
        "bibtex": "zarges_artificial_2021",
        "abstract": "Testing is a crucial part of the development of a new product. For software validation a transformation from manual to automated tests can be observed which enables companies to implement large numbers of test cases. However, during testing situations may occur where it is not feasible to run all tests due to time constraints. Hence a set of critical test cases must be compiled which usually fulfills several criteria. Within this work we focus on criteria that are feasible for black box testing such as system tests. We adapt an existing artificial immune system for our use case and evaluate our method in a series of experiments using industrial datasets. We compare our approach with several other test selection methods where our algorithm shows superior performance.",
        "published_in": "Evolutionary Computation in Combinatorial Optimization: 21st European Conference, EvoCOP 2021, Held as Part of EvoStar 2021, Virtual Event, April 7--9, 2021, Proceedings",
        "publisher": "Springer",
        "doi": "10.1007/978-3-030-72904-2_11",
        "date": "2021-03-27",
        "tcp": "",
        "tcs": "x",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, small to medium scale.\n\nThree datasets from the industrial partner (up to 1499 TCs)",
        "prog_language": "Unclear",
        "ind_partner": "BSH HausgerÃ¤te GmbH (Germany)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, poorly documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/LagLukas/moa_testing",
        "approach": "nature-inspired GCAIS is applied for selection, and optimization is multiobjective, including maximinzing req coverage, max probab of failure and minim time (they assume a time budget)",
        "info_approach": "",
        "alg_approach": "Search-based",
        "metrics": "a) time budget needed to reveal the first error\nb) Number of broken features found (would be the req causing the failure)\n",
        "effe_metrics": "Fault Detection Capability, Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Not much developed, they mention applying GCAIS to white box testing (which is a bit contradictory)"
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Bagherzadeh, Mojtaba; Kahani, Nafiseh; Briand, Lionel",
        "author_keys": [
            "bagherzadeh_mojtaba",
            "kahani_nafiseh",
            "briand_lionel"
        ],
        "title": "Reinforcement learning for test case prioritization",
        "bibtex": "bagherzadeh_reinforcement_2022",
        "abstract": "Continuous Integration (CI) significantly reduces integration problems, speeds up development time, and shortens release time. However, it also introduces new challenges for quality assurance activities, including regression testing, which is the focus of this work. Though various approaches for test case prioritization have shown to be very promising in the context of regression testing, specific techniques must be designed to deal with the dynamic nature and timing constraints of CI. Recently, Reinforcement Learning (RL) has shown great potential in various challenging scenarios that require continuous adaptation, such as game playing, real-time ads bidding, and recommender systems. Inspired by this line of work and building on initial efforts in supporting test case prioritization with RL techniques, we perform here a comprehensive investigation of RL-based test case prioritization in a CI context. To this end, taking test case prioritization as a ranking problem, we model the sequential interactions between the CI environment and a test case prioritization agent as an RL problem, using three alternative ranking models. We then rely on carefully selected and tailored state-of-the-art RL techniques to automatically and continuously learn a test case prioritization strategy, whose objective is to be as close as possible to the optimal one. Our extensive experimental analysis shows that the best RL solutions provide a significant accuracy improvement over previous RL-based work, with prioritization strategies getting close to being optimal, thus paving the way for using RL to prioritize test cases in a CI context.",
        "published_in": "IEEE Transactions on Software Engineering ( Volume: 48, Issue: 8, 01 August 2022) ",
        "publisher": "IEEE",
        "doi": "10.1109/TSE.2021.3070549",
        "date": "2021-04-02",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE\n(given the CI context)",
        "ind_evaluation": "FALSE",
        "exp_subjects": "8 Java projects (6 Apache Commons projects)",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "FALSE",
        "approach": "Evaluation of multiple SOA reinforcement learning approaches for test prioritization in CI. ",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": " - Normalized Rank Percentile Average (NRPA)\n - Average Percentage of Faults Detected (APFD)",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Rank Percentile Average (RPA)",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": " - Automatically and systematically using search-based\nfor tuning and optimization the approach\n\n- Preparing of a rich dataset and a benchmark."
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Elsner, Daniel; Hauer, Florian; Pretschner, Alexander; Reimer, Silke",
        "author_keys": [
            "elsner_daniel",
            "hauer_florian",
            "pretschner_alexander",
            "reimer_silke"
        ],
        "title": "Empirically evaluating readily available information for regression test optimization in continuous integration",
        "bibtex": "elsner_empirically_2021",
        "abstract": "Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.\n",
        "published_in": "ISSTA 2021: Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis",
        "publisher": "ACM",
        "doi": "10.1145/3460319.3464834",
        "date": "2021-07-11",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "3 Industrial projects \n\nplus\n\n20 open-source projects from RTPtorrent",
        "prog_language": "C, C++, Java, DSL, JavaScript",
        "ind_partner": "IVU Traffic Technologies",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "TRUE\n\nReplication package\n\nIt is a methodology, it has been released to IVU as a web service, unclear in which format it is released in the replication package",
        "put_practice": "TRUE",
        "suppl_url": "https://doi.org/10.6084/m9.figshare.13656443",
        "approach": "Compare predictive capabilities of ML features for RTP and RTS exclusively relying on CI and VCS\nmetadata \nThey observe that test history is important and simple models can outperform more complex ones",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "APFDð + sensitivity analysis\nTraining time",
        "effe_metrics": "",
        "effi_metrics": "Measuring time/cost",
        "other_metrics": "",
        "open_challenges": "Extend  application of their methodology in IVU"
    },
    {
        "type": "primary",
        "year": "2020",
        "authors": "Pan, Chaoyue; Yang, Yang; Li, Zheng; Guo, Junxia",
        "author_keys": [
            "pan_chaoyue",
            "yang_yang",
            "li_zheng",
            "guo_junxia"
        ],
        "title": "Dynamic Time Window based Reward for Reinforcement Learning in Continuous Integration Testing",
        "bibtex": "pan_dynamic_2020",
        "abstract": "Continuous Integration (CI) testing is an expensive, time-consuming, and resource-intensive process. Test case prioritization (TCP) can effectively reduce the workload of regression testing in the CI environment, where Reinforcement Learning (RL) is adopted to prioritize test cases, since the TCP in CI testing can be formulated as a sequential decision-making problem, which can be solved by RL effectively. A useful reward function is a crucial component in the construction of the CI system and a critical factor in determining RLâs learning performance in CI testing. This paper focused on the validity of the execution history information of the test cases on the TCP performance in the existing CI testing optimization methods based on RL, and a Dynamic Time Window based reward function are proposed by using partial information dynamically for fast feedback and cost reduction. Experimental studies are carried out on six industrial datasets. The experimental results showed that using dynamic time window based reward function can significantly improve the learning efficiency of RL and the fault detection ability when comparing with the reward function based on fixed time window.",
        "published_in": "Internetware'20: 12th Asia-Pacific Symposium on Internetware",
        "publisher": "ACM",
        "doi": "10.1145/3457913.3457930",
        "date": "2021-07-21",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "six industrial datasets (from 89 to 5,555 test cases)\n\nPaint Control and IOF/ROL are from ABB Robotics Norway, \nGoogle Shared Dataset of Test Suite Results (GSDTSR),\nRails, Mybatis and Apache Drill are extracted from Travis Torrent.\n\nIndustrial open-source, large scale",
        "prog_language": "Unclear",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "FALSE",
        "approach": "reward function based on dynamic time window",
        "info_approach": "",
        "alg_approach": "Bloom filter or window-based",
        "metrics": " - NAPFD\n - Recall\n - Test To Failure (TTF)",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Accuracy/precision/recall, Time/tests To First Failure",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "(1) Combining test cases to execute other information features, optimize calculation methods for dynamic time windows, and improve performance on datasets; (2) Combining deep learning algorithms to optimize the agent algorithm of RL to improve the performance of RL in CI testing."
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Mehta, Sonu; Farmahinifarahani, Farima; Bhagwan, Ranjita; Guptha, Suraj; Jafari, Sina; Kumar, Rahul; Saini, Vaibhav; Santhiar, Anirudh",
        "author_keys": [
            "mehta_sonu",
            "farmahinifarahani_farima",
            "bhagwan_ranjita",
            "guptha_suraj",
            "jafari_sina",
            "kumar_rahul",
            "saini_vaibhav",
            "santhiar_anirudh"
        ],
        "title": "Data-driven test selection at scale",
        "bibtex": "mehta_data-driven_2021",
        "abstract": "Large-scale services depend on Continuous Integration/Continuous Deployment (CI/CD) processes to maintain their agility and code-quality. Change-based testing plays an important role in finding bugs, but testing after every change is prohibitively expensive at a scale where thousands of changes are committed every hour. Test selection models deal with this issue by running a subset of tests for every change.\n\nIn this paper, we present a generic, language-agnostic and lightweight statistical model for test selection. Unlike existing techniques, the proposed model does not require complex feature extraction techniques. Consequently, it scales to hundreds of repositories of varying characteristics while capturing more than 99% of buggy pull requests. Additionally, to better evaluate test selection models, we propose application-specific metrics that capture both a reduction in resource cost and a reduction in pull-request turn-around time. By evaluating our model on 22 large repositories at Microsoft, we find that we can save 15%â30% of compute time while reporting back more than â99% of buggy pull requests.",
        "published_in": "ESEC/FSE 2021: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "publisher": "ACM",
        "doi": "10.1145/3468264.3473916",
        "date": "2021-08-18",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, very large scale\n\n22 large scale repositories at Microsoft (up to 60 million test suites!?)",
        "prog_language": "Language-agnostic",
        "ind_partner": "Microsoft (USA)",
        "ind_author": "TRUE",
        "prac_feedback": "TRUE",
        "avai_tool": "FALSE",
        "put_practice": "TRUE (it's an improvement to an already-used test platform at Microsoft, so it is quite likely to be implemented)",
        "suppl_url": "",
        "approach": "failure probability (history-based?)",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "COGS (cost of goods sold), turn-around time reduction, failure detection rate",
        "effe_metrics": "Testing time, Cost-benefit model",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Developer experience, complexity of code changes, weighted test selection"
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Xu, Jincheng; Du, Qingfeng; Li, Xiaojun",
        "author_keys": [
            "xu_jincheng",
            "du_qingfeng",
            "li_xiaojun"
        ],
        "title": "A Requirement-based Regression Test Selection Technique in Behavior-Driven Development",
        "bibtex": "xu_requirement-based_2021",
        "abstract": "Regression testing is an essential software maintenance activity before the release of a new version implementing a bug fix or a new feature. A regression test selection (RTS) technique chooses a subset of existing test cases to ensure that the system will not be adversely affected by the latest modifications. With the rise of DevOps, behavior-driven development (BDD) is growing in popularity as it is in close alignment with agile practices, for example, continuous integration. Hence, it is necessary to propose a novel and effective RTS technique for BDD specifically to accelerate the development process while ensuring software quality. Since most existing techniques for RTS are code-based and thus subject to some limitations, we present a requirement-based technique which uses the requirements in BDD to select test cases in both high-level (acceptance testing) and low-level (unit testing). Our technique firstly illustrates the new requirement with a scenario, and subsequently computes the semantic similarity of the new scenario and all existing scenarios with the vector space model. According to the results, the modification-traversing regression test cases can be selected in a semi-automated way. We also conduct an experimental study to evaluate our technique in terms of inclusiveness, precision, efficiency and generality. The study shows that our technique is applicable for BDD and effective in practice.",
        "published_in": "2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",
        "publisher": "IEEE",
        "doi": "10.1109/COMPSAC51774.2021.00182",
        "date": "2021-09-09",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, small scale\n\nConfidential subsystem \"X-crawler\" (233 TCs \"executable specifications\")",
        "prog_language": "Natural language",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Requirement-based",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "Inclusiveness, precision, efficiency and generality",
        "effe_metrics": "Accuracy/precision/recall",
        "effi_metrics": "Execution time",
        "other_metrics": "Applicability/Generality",
        "open_challenges": "Combine technique to traditional code-based approaches."
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Zhou, Jianyi; Chen, Junjie; Hao, Dan",
        "author_keys": [
            "zhou_jianyi",
            "chen_junjie",
            "hao_dan"
        ],
        "title": "Parallel Test Prioritization",
        "bibtex": "zhou_parallel_2022",
        "abstract": "Although regression testing is important to guarantee the software quality in software evolution, it suffers from the widely known cost problem. To address this problem, existing researchers made dedicated efforts on test prioritization, which optimizes the execution order of tests to detect faults earlier; while practitioners in industry leveraged more computing resources to save the time cost of regression testing. By combining these two orthogonal solutions, in this article, we define the problem of parallel test prioritization, which is to conduct test prioritization in the scenario of parallel test execution to reduce the cost of regression testing.Different from traditional sequential test prioritization, parallel test prioritization aims at generating a set of test sequences, each of which is allocated in an individual computing resource and executed in parallel. In particular, we propose eight parallel test prioritization techniques by adapting the existing four sequential test prioritization techniques, by including and excluding testing time in prioritization.To investigate the performance of the eight parallel test prioritization techniques, we conducted an extensive study on 54 open-source projects and a case study on 16 commercial projects from Baidu, a famous search service provider with 600M monthly active users. According to the two studies, parallel test prioritization does improve the efficiency of regression testing, and cost-aware additional parallel test prioritization technique significantly outperforms the other techniques, indicating that this technique is a good choice for practical parallel testing. Besides, we also investigated the influence of two external factors, the number of computing resources and time allowed for parallel testing, and find that more computing resources indeed improve the performance of parallel test prioritization. In addition, we investigated the influence of two more factors, test granularity and coverage criterion, and find that parallel test prioritization can still accelerate regression testing in parallel scenario. Moreover, we investigated the benefit of parallel test prioritization on the regression testing process of continuous integration, considering both the cumulative acceleration performance and the overhead of prioritization techniques, and the results demonstrate the superiority of parallel test prioritization.",
        "published_in": "ACM Trans. Softw. Eng. Methodol.",
        "publisher": "Association for Computing Machinery",
        "doi": "10.1145/3471906",
        "date": "2021-09-28",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, large scale\nOpen-source, large scale\n\n54 open-source Java projects (up to 120k LOC, 5623 TCs)\n\n5 projects from Baidu (> 500k LOC, up to 4246 TCs)",
        "prog_language": "Java",
        "ind_partner": "Baidu (China)",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/PTCP/PTCP",
        "approach": "cost-unaware: greedy, search-based, adaptive\n\ncost-aware: greedy, search-based, adaptive",
        "info_approach": "Cost-aware",
        "alg_approach": "Search-based, Greedy",
        "metrics": "APFD, modified APFDc, FT (first fault), AT (all faults), overhead",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Time/tests To First Failure",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "adopt more efficient sequential TCP techniques in parallel scenario"
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Sharif, Aizaz; Marijan, Dusica; Liaaen, Marius",
        "author_keys": [
            "sharif_aizaz",
            "marijan_dusica",
            "liaaen_marius"
        ],
        "title": "DeepOrder: Deep Learning for Test Case Prioritization in Continuous Integration Testing",
        "bibtex": "sharif_deeporder_2021",
        "abstract": "Continuous integration testing is an important step in the modern software engineering life cycle. Test prioritization is a method that can improve the efficiency of continuous integration testing by selecting test cases that can detect faults in the early stage of each cycle. As continuous integration testing produces voluminous test execution data, test history is a commonly used artifact in test prioritization. However, existing test prioritization techniques for continuous integration either cannot handle large test history or are optimized for using a limited number of historical test cycles. We show that such a limitation can decrease fault detection effectiveness of prioritized test suites. This work introduces DeepOrder, a deep learning-based model that works on the basis of regression machine learning. DeepOrder ranks test cases based on the historical record of test executions from any number of previous test cycles. DeepOrder learns failed test cases based on multiple factors including the duration and execution status of test cases. We experimentally show that deep neural networks, as a simple regression model, can be efficiently used for test case prioritization in continuous integration testing. DeepOrder is evaluated with respect to time-effectiveness and fault detection effectiveness in comparison with an industry practice and the state of the art approaches. The results show that DeepOrder outperforms the industry practice and state-of-the-art test prioritization approaches in terms of these two metrics.",
        "published_in": "2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSME52107.2021.00053",
        "date": "2021-11-24",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, small scale\nIndustrial open-source, small to large scale\n\nCisco dataset (320 TCs)\n\nABB Robotics dataset (1.4k TCs)\n\nGSDTSR (Google) dataset (5.5k TCs)",
        "prog_language": "Unclear",
        "ind_partner": "Cisco (Norway)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/AizazSharif/DeepOrder-ICSME21\n\nhttps://github.com/T3AS/DeepOrder-ICSME21",
        "approach": "Machine learning based",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "APFD, NAPFD, MSE (mean squared error), FT (time to first fault), LT (time to last fault), AT (avg. time to all faults), PT (time to process data, train, validate), RT (time to prioritize), TT (total algorithm time)",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Accounting for changes in test code; flaky tests; consider non-executed tests; evaluate on more datasets; deploy on real CI environment."
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Li, Feng; Zhou, Jianyi; Li, Yinzhu; Hao, Dan; Zhang, Lu",
        "author_keys": [
            "li_feng",
            "zhou_jianyi",
            "li_yinzhu",
            "hao_dan",
            "zhang_lu"
        ],
        "title": "AGA: An Accelerated Greedy Additional Algorithm for Test Case Prioritization",
        "bibtex": "li_aga_2021",
        "abstract": "In recent years, many test case prioritization (TCP) techniques have been proposed to speed up the process of fault detection. However, little work has taken the efficiency problem of these techniques into account. In this paper, we target the Greedy Additional (GA) algorithm, which has been widely recognized to be effective but less efficient, and try to improve its efficiency while preserving effectiveness. In our Accelerated GA (AGA) algorithm, we use some extra data structures to reduce redundant data accesses in the GA algorithm and thus the time complexity is reduced from O(m2n) to O(kmn) when n > m, where m is the number of test cases, n is the number of program elements, and k is the iteration number. Moreover, we observe the impact of iteration numbers on prioritization efficiency on our dataset and propose to use a specific iteration number in the AGA algorithm to further improve the efficiency. We conducted experiments on 55 open-source subjects. In particular, we implemented each TCP algorithm with two kinds of widely-used input formats, adjacency matrix and adjacency list. Since a TCP algorithm with adjacency matrix is less efficient than the algorithm with adjacency list, the result analysis is mainly conducted based on TCP algorithms with adjacency list. The results show that AGA achieves 5.95X speedup ratio over GA on average, while it achieves the same average effectiveness as GA in terms of Average Percentage of Fault Detected (APFD). Moreover, we conducted an industrial case study on 22 subjects, collected from Baidu, and find that the average speedup ratio of AGA over GA is 44.27X, which indicates the practical usage of AGA in real-world scenarios.",
        "published_in": "IEEE Transactions on Software Engineering",
        "publisher": "IEEE",
        "doi": "10.1109/TSE.2021.3137929",
        "date": "2021-12-23",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE \n(Baidu)",
        "exp_subjects": "55 open-source subjects (from 1,621 to 177,546 lines of code)\n+\nindustrial case study on 22 subjects (Baidu)",
        "prog_language": "Java",
        "ind_partner": "TRUE \n(Baidu)",
        "ind_author": "TRUE \n(Baidu)",
        "prac_feedback": "TRUE\n(\"we receive positive feedback from Baidu\")",
        "avai_tool": "TRUE\n\nReplication package, documented",
        "put_practice": "MAYBE\n(at least for conducting the industrial case study: \"we receive positive feedback from Baidu\")",
        "suppl_url": "data, analysis scripts, and data tables\n\nwebsite: https://github.com/Spiridempt/AGA\n\nfigshare: https://figshare.com/s/cf8cc6ba9259c0e0754d.",
        "approach": "An improved version of the traditianl GA algortihm called \"Accelerated Greedy Additional\"",
        "info_approach": "",
        "alg_approach": "Greedy",
        "metrics": " - APFD\n - Speedup ratio\n - Analysis of Covariance (ANCOVA)",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "Execution time",
        "other_metrics": "",
        "open_challenges": "Not discussed"
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "Chen, Yizhen; Chaudhari, Ninad; Chen, Mei-Hwa",
        "author_keys": [
            "chen_yizhen",
            "chaudhari_ninad",
            "chen_mei-hwa"
        ],
        "title": "Context-Aware Regression Test Selection",
        "bibtex": "chen_context-aware_2021",
        "abstract": "Most modern software systems are continuously evolving, with changes frequently taking place in the core components or the execution context. These changes can adversely introduce regression faults, causing previously working functions to fail. Regression testing is essential for maintaining the quality of evolving complex software, but it can be overly time-consuming when the size of the test suite is large, or the execution of the test cases takes a long time. There are extensive research studies on selective regression testing aiming at minimizing the size of the regression test suite while maximizing the detection of the regression faults. However, most of the existing techniques focus on the regression faults caused by the code changes, the impact of the context changes on the non-modified software has barely been explored. This paper presents a context-aware regression test selection (CARTS) approach that not only accounts for the modification of code but also changes in the execution context, including libraries, external APIs, and databases. After a change, CARTS uses the program invariants denoted in the pre- and postconditions of a function to determine if the function is affected by the change and selects all the test cases that executed the modified code as well as the non-modified functions whose preconditions are affected by the change. To evaluate the effectiveness of our approach, we conducted empirical studies on multi-release open-source software and case studies on real-world systems that have ongoing changes in code as well as in the execution context. The results of our controlled experiments show that with an average of 32.5% of the regression test cases, CARTS selected all the fault-revealing test cases. In the case studies, all the fault-revealing test cases were selected by using an average of 25.3% of the regression test suite. These results suggest that CARTS can be effective for selecting fault-revealing test cases for both code and execution context changes.",
        "published_in": "2021 28th Asia-Pacific Software Engineering Conference (APSEC)",
        "publisher": "IEEE",
        "doi": "10.1109/APSEC53868.2021.00050",
        "date": "2022-02-17",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE\n(\"...The results\nof these studies demonstrate the feasibility of applying the\nproposed approach to real-life industrial systems...\")",
        "ind_evaluation": "TRUE\n(\"...the subject programs and the regression faults used\nin the studies are actual industrial programs...\")",
        "exp_subjects": "five open-source subjects (mostly Apache, up to 2505 TCs)\n+\nthree case studies on industrial systems (LoC varies from 21,409 to 142,908, up to 412 TCs)\n\nOpen-source, large scale\nIndustrial proprietary, small scale",
        "prog_language": "Java, JavaScript",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE\n(authors mention they developed a prototype, but apparently it was not made available)",
        "put_practice": "FALSE",
        "suppl_url": "FALSE",
        "approach": "A context-aware regression TCS (CARTS) approach that accounts for the modification of code and changes in the execution context, including libraries, external APIs, and databases.",
        "info_approach": "Change-based, Execution context",
        "alg_approach": "",
        "metrics": " - the number of the regression test cases selected (test suite size)\n - the number of the regression faults detected by each technique (fault detection ability)",
        "effe_metrics": "Selection/reduction count/percentage, Fault Detection Capability",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": " - inferring invariants for other languages that are not currently supported"
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "Zhang, Jiyang; Liu, Yu; Gligoric, Milos; Legunsen, Owolabi; Shi, August",
        "author_keys": [
            "zhang_jiyang",
            "liu_yu",
            "gligoric_milos",
            "legunsen_owolabi",
            "shi_august"
        ],
        "title": "Comparing and Combining Analysis-Based and Learning-Based Regression Test Selection",
        "bibtex": "zhang_comparing_2022",
        "abstract": "Regression testing--rerunning tests on each code version to detect newly--broken functionality-is important and widely practiced. But, regression testing is costly due to the large number of tests and the high frequency of code changes. Regression test selection (RTS) optimizes regression testing by only rerunning a subset of tests that can be affected by changes. Researchers showed that RTS based on program analysis can save substantial testing time for (medium-sized) open-source projects. Practitioners also showed that RTS based on machine learning (ML) works well on very large code repositories, e.g., in Facebook's monorepository. We combine analysis-based RTS and ML-based RTS by using the latter to choose a subset of tests selected by the former. We first train several novel ML models to learn the impact of code changes on test outcomes using a training dataset that we obtain via mutation analysis. Then, we evaluate the benefits of combining ML models with analysis-based RTS on 10 projects, compared with using each technique alone. Combining ML-based RTS with two analysis-based RTS techniques-Ekstazi and STARTS-selects 25.34% and 21.44% fewer tests, respectively. CCS CONCEPTS * Software and its engineering $\\rightarrow$Software testing and debugging.",
        "published_in": "2022 IEEE/ACM International Conference on Automation of Software Test (AST)",
        "publisher": "IEEE/ACM",
        "doi": "",
        "date": "2022-04-01",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source, unclear scale\n\n10 open-source Java programs",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, poorly documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/EngineeringSoftware/predictiverts",
        "approach": "hybrid selection approach that combines code analysis with ML",
        "info_approach": "Change-based",
        "alg_approach": "Machine learning-based",
        "metrics": "- percentage of tests that is selected\n'- time to select tests and time to run them",
        "effe_metrics": "Selection/reduction count/percentage, Testing time",
        "effi_metrics": "Execution time, Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "- difficulty in creating the training set (they use mutation)\n'- combining other criteria for selection"
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "Abdelkarim, Mohamed; ElAdawi, Reem",
        "author_keys": [
            "abdelkarim_mohamed",
            "eladawi_reem"
        ],
        "title": "TCP-Net: Test Case Prioritization using End-to-End Deep Neural Networks",
        "bibtex": "abdelkarim_tcp-net_2022",
        "abstract": "Regression testing is facing a bottleneck due to the growing number of test cases and the wide adoption of continuous integration (CI) in software projects, which increases the frequency of running software builds, making it challenging to run all the regression test cases. Machine learning (ML) techniques can be used to save time and hardware resources without compromising quality. In this work, we introduce a novel end-to-end, self-configurable, and incremental learning deep neural network (DNN) tool for test case prioritization (TCP-Net). TCP-Net is fed with source code-related features, test case metadata, test case coverage information, and test case failure history, to learn a high dimensional correlation between source files and test cases. We experimentally show that TCP-Net can be efficiently used for test case prioritization by evaluating it on three different real-life industrial software packages.",
        "published_in": "2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
        "publisher": "IEEE",
        "doi": "10.1109/ICSTW55395.2022.00034",
        "date": "2022-06-08",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE\n(\"Experimentation with the TCP-Net on three industrial\ndatasets, showing the potential of our approach for integration\ninto CI regression testing in practical environments.)",
        "exp_subjects": "Industrial proprietary, large to very large scale\n\n - Calibre Auto-Waivers (2k TCs)\n - Calibre PERC (11k TCs)\n - Calibre nmDRC (3k TCs)",
        "prog_language": "Unclear",
        "ind_partner": "Siemens EDA (Egypt)",
        "ind_author": "TRUE\n(the two authors are from Siemens EDA)",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "Deep Neural Networks for TCP",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": " - APFD\n - Accuracy, precision, recall, and F1-score",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Accuracy/precision/recall",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "\"For future work, more incremental learning\napproaches will be explored and tested.\""
    },
    {
        "type": "primary",
        "year": "2021",
        "authors": "MagalhÃ£es, Claudio; Mota, Alexandre; Momente, Luis",
        "author_keys": [
            "magalhaes_claudio",
            "mota_alexandre",
            "momente_luis"
        ],
        "title": "UI Test case prioritization on an industrial setting: A search for the best criteria",
        "bibtex": "magalhaes_ui_2021",
        "abstract": "This work was developed in an industrial setting towards UI regression testing, where we do not have access to source code and the majority of test cases are manually executed (and only part of the regression-based test cases can be executed due to limited resources). Test case prioritization (TCP) is indicated for such a scenario. But characteristic of many TCP techniques is that they rely on source code coverage information, whereas we just have access to test cases, change requests, and their features. Thus, our goal is to investigate which criteria is the most relevant for prioritization. Thus, according to the literature we create an optimization model based on historical data. This model is embedded in a constraint solver designed for optimization. Our optimization function is based on the APFD (Average of the Percentage of Faults Detected) metric, but other metrics can be used as well. We have found that our partner already uses an appropriate criterion to identify failures which is statistically equivalent to other criteria used in experiments using our optimization model.",
        "published_in": "Software Quality Journal volume 29, pages 381â403 (2021)",
        "publisher": "Springer",
        "doi": "10.1007/s11219-021-09549-y",
        "date": "2021-04-15",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary, small to medium scale\n\nMotorola \"test plans\" (30-826 TCs)",
        "prog_language": "Unclear",
        "ind_partner": "Motorola (Brazil/USA)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "MAYBE\n\nOnly results",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/cjasm/Data-for-Test-Case-Prioritization-on-an-industrial-setting",
        "approach": "An optimization model based on historical data",
        "info_approach": "",
        "alg_approach": "",
        "metrics": "APFD",
        "effe_metrics": "",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": "Improve execution time; integrate with HSP model; compare to AI techniques"
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "ÃÄ±ngÄ±l, Tutku; SÃ¶zer, Hasan",
        "author_keys": [
            "cingil_tutku",
            "sozer_hasan"
        ],
        "title": "Black-box Test Case Selection by Relating Code Changes with Previously Fixed Defects",
        "bibtex": "cingil_black-box_2022",
        "abstract": "Software continuously changes to address new requirements and to fix defects. Regression testing is performed to ensure that the applied changes do not adversely affect existing functionality. The increasing number of test cases makes it infeasible to execute the whole regression test suite. Test case selection is adopted to select a subset of the test suite, which is associated with the changed parts of the software. These parts are assumed to be error-prone. We present and evaluate a test case selection approach in the context of black-box regression testing of embedded systems. In this context, it is challenging to relate test cases with a set of distinct source code elements to be able to select those test cases associated with the modified parts of the source code. We analyze previously fixed defects for this purpose. We relate test cases that detect these defects with the source files that are previously modified for fixing them. Then, we select test cases related with source code files that are modified in the subsequent revision. The strength of this relation is determined as the number of changes associated with fixed defects previously detected by the same test cases. We conduct a case study on 3 real projects from the consumer electronics domain. Results show that it is possible to detect from 65% up to 85% of the defects detected by the whole test suite by selecting from 30% up to 70% of the test cases.",
        "published_in": "EASE '22: Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022",
        "publisher": "ACM",
        "doi": "10.1145/3530019.3530023",
        "date": "2022-06-13",
        "tcp": "",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Industrial proprietary: small number of tests (38) but long duration (7+ hours)",
        "prog_language": "Unclear",
        "ind_partner": "Vestel (Turkey)",
        "ind_author": "TRUE",
        "prac_feedback": "FALSE",
        "avai_tool": "FALSE",
        "put_practice": "FALSE",
        "suppl_url": "",
        "approach": "change-based and fault-based; relating new code changes to previously fixed defects.",
        "info_approach": "Change-based, Fault-based",
        "alg_approach": "",
        "metrics": "\"percentage of bugs detected\", a sort of fault detection capability; selection percentage, ratio of field errors, selected test case duration",
        "effe_metrics": "Selection/reduction count/percentage, Testing time, Fault Detection Capability",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": ""
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "Yaraghi, Ahmadreza Saboor; Bagherzadeh, Mojtaba; Kahani, Nafiseh; Briand, Lionel",
        "author_keys": [
            "yaraghi_ahmadreza_saboor",
            "bagherzadeh_mojtaba",
            "kahani_nafiseh",
            "briand_lionel"
        ],
        "title": "Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts",
        "bibtex": "yaraghi_scalable_2022",
        "abstract": "Continuous Integration (CI) requires efficient regression testing to ensure software quality without significantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we first define, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we define a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the defined features for 25 open-source software systems with enough failed builds and whose regression testing takes at least five minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques. IEEE",
        "published_in": "IEEE Transactions on Software Engineering",
        "publisher": "IEEE",
        "doi": "10.1109/TSE.2022.3184842",
        "date": "2022-06-20",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Open-source, small to large scale\n\n25 open-source Java projects (up to 1M LOC, 4364 TCs)",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well-documented",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/Ahmadreza-SY/TCP-CI",
        "approach": "machine learning based",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "APFDc, scalability",
        "effe_metrics": "Average Percentage of Faults Detected (APFD)",
        "effi_metrics": "Scalability",
        "other_metrics": "",
        "open_challenges": "Potential correlation between APFDc and subject characteristics; trade-offs between granularity of coverage; scalability; TCP effectiveness"
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "Omri, Safa; Sinz, Carsten",
        "author_keys": [
            "omri_safa",
            "sinz_carsten"
        ],
        "title": "Learning to Rank for Test Case Prioritization",
        "bibtex": "omri_learning_2022",
        "abstract": "In Continuous Integration (CI) environments, the productivity of software engineers depends strongly on the ability to reduce the round-trip time between code commits and feedback on failed test cases. Test case prioritization is popularly used as an optimization mechanism for ranking tests by their likelihood in revealing failures. However, existing techniques are usually time and resource intensive making them not suitable to be applied within CI cycles. This paper formulates the test case prioritization problem as an online learn-to-rank model using reinforcement learning techniques. Our approach minimizes the testing overhead and continuously adapts to the changing environment as new code and new test cases are added in each CI cycle. We validated our approach on an industrial case study showing that over 95% of the test failures are still reported back to the software engineers while only 40% of the total available test cases are being executed.",
        "published_in": "2022 IEEE/ACM 15th International Workshop on Search-Based Software Testing (SBST)",
        "publisher": "IEEE/ACM",
        "doi": "10.1145/3526072.3527525",
        "date": "2022-07-04",
        "tcp": "X",
        "tcs": "",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "TRUE",
        "exp_subjects": "Academic dataset, small to very large scale\nIndustrial open-source, medium to large scale\n\nABB Paint Control\nABB IOF/ROL\nGoogle GSDTSR",
        "prog_language": "Language-agnostic",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "MAYBE\n\nSupposedly yes, apparently unavailable",
        "put_practice": "FALSE",
        "suppl_url": "https://github.com/so1188/learntec (private)\n\nhttps://github.com/so1188/data",
        "approach": "reinforcement learning",
        "info_approach": "",
        "alg_approach": "Machine learning-based",
        "metrics": "recall, APFD",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Accuracy/precision/recall",
        "effi_metrics": "",
        "other_metrics": "",
        "open_challenges": ""
    },
    {
        "type": "primary",
        "year": "2022",
        "authors": "Greca, Renan; Miranda, Breno; Gligoric, Milos; Bertolino, Antonia",
        "author_keys": [
            "greca_renan",
            "miranda_breno",
            "gligoric_milos",
            "bertolino_antonia"
        ],
        "title": "Comparing and combining file-based selection and similarity-based prioritization towards regression test orchestration",
        "bibtex": "greca_comparing_2022",
        "abstract": "Test case selection (TCS) and test case prioritization (TCP) techniques can reduce time to detect the first test failure. Although these techniques have been extensively studied in combination and isolation, they have not been compared one against the other. In this paper, we perform an empirical study directly comparing TCS and TCP approaches, represented by the tools Ekstazi and FAST, respectively. Furthermore, we develop the first combination, named Fastazi, of file-based TCS and similarity-based TCP and evaluate its benefit and cost against each individual technique. We performed our experiments using 12 Java-based open-source projects. Our results show that, in the median case, the combined approach detects the first failure nearly two times faster than either Ekstazi alone (with random test ordering) or FAST alone (without TCS). Statistical analysis shows that the effectiveness of Fastazi is higher than that of Ekstazi, which in turn is higher than that of FAST. On the other hand, FAST adds the least overhead to testing time, while the difference between the additional time needed by Ekstazi and Fastazi is negligible. Fastazi can also improve failure detection in scenarios where the time available for testing is restricted.",
        "published_in": "AST '22: Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test",
        "publisher": "ACM/IEEE",
        "doi": "10.1145/3524481.3527223",
        "date": "2022-07-19",
        "tcp": "X",
        "tcs": "X",
        "tsr": "",
        "tsa": "",
        "ind_motivation": "TRUE",
        "ind_evaluation": "FALSE",
        "exp_subjects": "Academic dataset, small to medium scale\n\nDefects4J",
        "prog_language": "Java",
        "ind_partner": "",
        "ind_author": "FALSE",
        "prac_feedback": "FALSE",
        "avai_tool": "TRUE\n\nReplication package, well-documented",
        "put_practice": "FALSE",
        "suppl_url": "https://zenodo.org/record/5851288#.Yw3LmS8RqLc\n\nhttps://github.com/Fastazi/Fastazi/tree/1.1",
        "approach": "change-based selection +\nsimilarity-based prioritization",
        "info_approach": "Change-based, Test code",
        "alg_approach": "Similarity / distance-based",
        "metrics": "APFD, TTFF, fault detection per budget, selection + prioritization time",
        "effe_metrics": "Average Percentage of Faults Detected (APFD), Time/tests To First Failure, Fault detection within a budget",
        "effi_metrics": "Execution time, Total/End-to-end time",
        "other_metrics": "",
        "open_challenges": "Experiment with more realistic software/datasets; further combine techniques such as TSR/TSA for orchestration."
    }
]