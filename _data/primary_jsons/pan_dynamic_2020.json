{"type": "primary", "year": "2020", "authors": "Pan, Chaoyue; Yang, Yang; Li, Zheng; Guo, Junxia", "author_keys": ["pan_chaoyue", "yang_yang", "li_zheng", "guo_junxia"], "title": "Dynamic Time Window based Reward for Reinforcement Learning in Continuous Integration Testing", "bibtex": "pan_dynamic_2020", "abstract": "Continuous Integration (CI) testing is an expensive, time-consuming, and resource-intensive process. Test case prioritization (TCP) can effectively reduce the workload of regression testing in the CI environment, where Reinforcement Learning (RL) is adopted to prioritize test cases, since the TCP in CI testing can be formulated as a sequential decision-making problem, which can be solved by RL effectively. A useful reward function is a crucial component in the construction of the CI system and a critical factor in determining RL\u2019s learning performance in CI testing. This paper focused on the validity of the execution history information of the test cases on the TCP performance in the existing CI testing optimization methods based on RL, and a Dynamic Time Window based reward function are proposed by using partial information dynamically for fast feedback and cost reduction. Experimental studies are carried out on six industrial datasets. The experimental results showed that using dynamic time window based reward function can significantly improve the learning efficiency of RL and the fault detection ability when comparing with the reward function based on fixed time window.", "published_in": "Internetware'20: 12th Asia-Pacific Symposium on Internetware", "publisher": "ACM", "doi": "10.1145/3457913.3457930", "date": "2021-07-21", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "ind_motivation": "TRUE", "ind_evaluation": "FALSE", "exp_subjects": "six industrial datasets (from 89 to 5,555 test cases)\n\nPaint Control and IOF/ROL are from ABB Robotics Norway, \nGoogle Shared Dataset of Test Suite Results (GSDTSR),\nRails, Mybatis and Apache Drill are extracted from Travis Torrent.\n\nIndustrial open-source, large scale", "prog_language": "Unclear", "ind_partner": "", "ind_author": "FALSE", "prac_feedback": "FALSE", "avai_tool": "FALSE", "put_practice": "FALSE", "suppl_url": "FALSE", "approach": "reward function based on dynamic time window", "info_approach": "", "alg_approach": "Bloom filter or window-based", "metrics": " - NAPFD\n - Recall\n - Test To Failure (TTF)", "effe_metrics": "Average Percentage of Faults Detected (APFD), Accuracy/precision/recall, Time/tests To First Failure", "effi_metrics": "", "other_metrics": "", "open_challenges": "(1) Combining test cases to execute other information features, optimize calculation methods for dynamic time windows, and improve performance on datasets; (2) Combining deep learning algorithms to optimize the agent algorithm of RL to improve the performance of RL in CI testing."}