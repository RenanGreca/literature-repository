[{"year": "2016", "authors": "Rosero, Ra\u00fal H.; G\u00f3mez, Oomar S.; Rodr\u00edguez, Glen", "title": "15 Years of Software Regression Testing Techniques - A Survey", "bibtex": "rosero_15_2016", "abstract": "Software regression testing techniques verify previous functionalities each time software modifications occur or new characteristics are added. With the aim of gaining a better understanding of this subject, in this work we present a survey of software regression testing techniques applied in the last 15 years; taking into account its application domain, kind of metrics they use, its application strategies and the phase of the software development process where they are applied. From an outcome of 460 papers, a set of 25 papers describing the use of 31 software testing regression techniques were identified. Results of this survey suggest that at the time of applying a regression testing techniques, metrics like cost and fault detection efficiency are the most relevant. Most of the techniques were assessed with instrumented programs (experimental cases) under academic settings. Conversely, we observe a minimum set of software regression techniques applied in industrial settings, mainly, under corrective and maintenance approaches. Finally, we observe a trend using some regression techniques under agile approaches. \u00a9 2016 World Scientific Publishing Company.", "published_in": "International Journal of Software Engineering and Knowledge Engineering", "publisher": "World Scientific", "doi": "10.1142/S0218194016300013", "date": "2015-06-25", "tcp": "X", "tcs": "X", "tsr": "X", "tsa": "", "paper_count": "25", "years_covered": "2000-2014", "systematic": "", "approach": "For TSM:\n- Heuristics-based.\n\nFor RTS:\n- Graph-, dependency- or UML-based;\n- Model-based;\n- Heuristics-based;\n- Based on historical data;\n- Based on fault detection;\n- Modifications-based.\n\nFor TCP:\n- Based on fault detection;\n- Historical and probabilistic;\n- Model-based\n- Dynamic analysis and natural language;\n- Graphs-, dependency- or UML-based;\n- Heuristics-based.\n\nAlso include RTO=Rgre Test Optimization (? to do what?):\n- based on fuzzy logic\n- based on heuristic", "metrics": "- Fault detection ability;\n- Accuracy;\n- Selection time;\n- Implementation time.\n--> see also summary in notes (column H)", "conclusions": "- Selection is predominant;\n- AI-based techniques are becoming more popular;\n- Techniques are usually based on source code, historical test data, test cases, heuristics, or AI approaches;\n- Only 16% of papers experimented in industrial context;\n- Results are not always reliable.", "open_challenges": "- Small number in agile context;\n- Only controlled academic experiments;\n- Lack of RT techniques applied in industry;\n- Industry might be reluctant to invest in RT."}, {"year": "2016", "authors": "Do, Hyunsook", "title": "Recent advances in regression testing techniques", "bibtex": "do_recent_2016", "abstract": "Software systems and their environment change are continuous. They are enhanced, corrected, and ported to new platforms. These changes can affect a system adversely, thus software engineers perform regression testing to ensure the quality of the modified systems. Regression testing is an integral part of most major software projects, but as projects grow larger and the number of tests increases, performing regression testing becomes more costly. To address this problem, many researchers and practitioners have proposed and empirically evaluated various regression testing techniques, such as regression test selection, test case prioritization, and test suite minimization. Recent surveys on these techniques indicate that this research area continues to grow, heuristics and the types of data utilized become diverse, and wider application domains have been considered. This chapter presents the current status and the trends of three regression testing techniques and discusses recent advances of each technique.", "published_in": "Advances in Computers", "publisher": "Elsevier", "doi": "10.1016/bs.adcom.2016.04.004", "date": "2016-05-10", "tcp": "X", "tcs": "X", "tsr": "X", "tsa": "", "paper_count": "12", "years_covered": "2010-2016", "systematic": "", "approach": "For RTS:\n- Firewall;\n- Graph walking;\n- Data-flow analysis;\n- Model-based;\n- Integer programming;\n- Symbolic execution;\n- Slicing.\n\nFor TCP:\n- Code coverage;\n- History;\n- Requirements;\n- Model-based;\n- Human-based;\n- Interatcion;\n- Probabilistic;\n- Distribution;\n- Cost-aware.\n\nFor TSM:\n- Code coverage;\n- Model-based;\n- Graph-based.", "metrics": "For RTS:\n- no. of selected tests;\n- Reduction rate;\n- Testing time;\n- Precision/recall;\n- Total time;\n- Cost-benefit analysis.\n\nFor TCP:\n- APFD;\n- APFDc;\n- NAPFD;\n- Most likely relative position;\n- Coverage effectiveness;\n- Cost models similar to the ones from RTS.\n\nFor TSM:\n- Reduction rate;\n- Loss of fault detection ability;\n- Loss of code coverage;\n- Execution time.", "conclusions": "- Most early studies in the field used the Siemens programs as benchmark; researchers should use SIR instead.\n- Recently, more research is focusing on industrial programs, or open source programs of differenty types.\n- The trend is increase variety in algorithmic techniques and input sources, as they have different pros and cons.", "open_challenges": "- Gap in technology transfer to industry;\n- Techniques must be developed considering industrial testing environments."}, {"year": "2016", "authors": "Hao, Dan; Zhang, Lu; Mei, Hong", "title": "Test-case Prioritization: Achievements and Challenges", "bibtex": "hao_test-case_2016", "abstract": "Test-case prioritization, proposed at the end of last century, aims to schedule the execution order of test cases so as to improve test effectiveness. In the past years, test-case prioritization has gained much attention, and has significant achievements in five aspects: prioritization algorithms, coverage criteria, measurement, practical concerns involved, and application scenarios. In this article, we will first review the achievements of test-case prioritization from these five aspects and then give our perspectives on its challenges.", "published_in": "Frontiers of Computer Science", "publisher": "Springer", "doi": "10.1007/s11704-016-6112-3", "date": "2016-06-29", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "27", "years_covered": "1999-2016", "systematic": "", "approach": "- Greedy strategies: additional coverage per unit, total or additional coverage;\n- Metaheuristic search: hill climbing and genetic algorithms;\n- Machine learning;\n- Integer linear programming.\n\nAlso mentions model-based and fault-based (they do not provide a clear classification actually)", "metrics": "- APFD;\n- APFDc;\n- APXC (structural coverage);\n- APSC (statement coverage);\n- APMC (method coverage);\n- APBC (block coverage);\n- NAPFD.", "conclusions": "- There is a disconnect between the ultimate goal of TCP (fault detection) and intermediate goals (such as code coverage).\n- Most of the literature uses APFD.\n- Obtaining the information needed for a technique to work is an additional cost.\n- Most early work used only Siemens programs, now grep and gzip are also common.\n- Few works evaluate against evolving software. According to [55], changes in source code rarely affect prioritization results, but changes in test cases often do.\n- Techniques are often evaluated against small programs, which do not highlight the benefits of prioritization.", "open_challenges": "- A measurement to address multiple practical constraints is needed;\n- Cost of obtaining required information (i.e. coverage);\n- Evaluations are performed on small programs;\n- Techniques do not consider practical concerns;\n- Consider small test suites with long-running tests."}, {"year": "2017", "authors": "Kazmi, Rafaqut; Jawawi, Dayang N. A.; Mohamad, Radziah; Ghani, Imran", "title": "Effective Regression Test Case Selection: A Systematic Literature Review", "bibtex": "kazmi_effective_2017", "abstract": "Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results. The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible. There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.", "published_in": "Computing Surveys", "publisher": "ACM", "doi": "10.1145/3057269", "date": "2017-05-27", "tcp": "", "tcs": "X", "tsr": "", "tsa": "", "paper_count": "47", "years_covered": "2007-2015", "systematic": "X", "approach": "- Mining and learning;\n- Model-based;\n- Slicing;\n- Control flow graph;\n- Oracle-based;\n- Firewall.", "metrics": "- Measuring cost (i.e. compilation time);\n- Test suite size;\n- Testing time;\n- Defect discovery time;\n- Bounded execution time;\n- Cost of each test case;\n- Selection time.", "conclusions": "- Ideas of test case classification and safe regression testing.\n- New trends (CI, cloud, etc) pose different challenges to RT.\n- Mining and learning is popular, but have several drawbacks.\n- Coverage, cost or fault detection not sufficient alone.\n- Consider coverage granularity.\n- Focus should be on fault detection, not code coverage.\n- Evaluations should be performed with artifact repositories or open-source software.\n- Focus on measurable aspects of RT.\n- Need of more statistical methods.\n- Mutation score might replace coverage adequacy.", "open_challenges": "- Consider trade-offs between current techniques and mining/learning ones.\n- Design a general-purpose evaluation technique for RTS.\n- Empirical study design guidelines are not mature;\n- Investigate multi-criteria cases with statistical methods;\n- Determine relationship between cost, coverage and fault-detection abilities."}, {"year": "2018", "authors": "Mukherjee, Rajendrani; Patnaik, K. Sridhar", "title": "A survey on different approaches for software test case prioritization", "bibtex": "mukherjee_survey_2018", "abstract": "Testing is the process of evaluating a system by manual or automated means. While Regression Test Selection (RTS) discards test cases and Test Suite Minimization (TSM) shows diminution in fault detection rate, Test Case Prioritization (TCP) does not discard test cases. Test Case Prioritization techniques can be coverage or historical information based or model based. It can also be cost-time aware or requirement-risk aware. GUI/Web applications need special prioritization mechanism. In this paper, 90 scholarly articles ranging from 2001 to 2018 have been reviewed. We have explored IEEE, Wiley, ACM Library, Springer, Taylor & Francis and Elsevier database. We have also described each prioritization method with their findings and subject programs. This paper includes a chronological catalogue listing of the reviewed papers. We have framed three research questions which sum up the frequently used prioritization metrics, regularly used subject programs and the distribution of different prioritization techniques. To the best of our knowledge, this is the first review with a detail report of the last 18\u202fyears of TCP techniques. We hope this article will be beneficial for both beginners and seasoned professionals.", "published_in": "Journal of King Saud University - Computer and Information Sciences", "publisher": "Elsevier", "doi": "10.1016/j.jksuci.2018.09.005", "date": "2018-10-03", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "90", "years_covered": "2001-2018", "systematic": "X", "approach": "- Coverage-aware;\n- Historical;\n- Cost cognizant;\n- Time-aware;\n- Requirement and risk-oriented;\n- Model-based.\n\nalso \n-GUI/web application focused\n- other", "metrics": "- APFD;\n- APFDc;\n- NAPFD;\n- Savings factor (SF);\n- Coverage effectiveness (CE);\n- Convergence index (CI);\n- Average severity of faults detected (ASFD);\n- Most likely relative position (RP);\n- Average percentage of damage prevented (APDP).", "conclusions": "- APFD most used; APFDc, NAPFD etc. also valuable.\n- SIR is valuable for evaluation subjects.\n- Coverage-aware is predominant, followed by requirement-based methods; model-based is increasing.", "open_challenges": "- Few techinques evaluated on real-world software systems;\n- Effect of alteration of test suite on TCP efficacy;\n- Coverage-less techniques should be explored;\n- Static-dynamic combined techniques;\n- TCP based on program changes;\n- Software product line related TCP."}, {"year": "2018", "authors": "Bajaj, Anu; Sangwan, Om Prakash", "title": "A survey on regression testing using nature-inspired approaches", "bibtex": "bajaj_survey_2018", "abstract": "Efficient regression testing plays an important role for organizations that have large investment in active, ever-changing software development. Efficiency can be obtained by optimizing the test cases as it provides a balance between the safety and precision. Many optimization techniques from various domains have been applied in regression testing for optimizing the search and the solutions. But nature-inspired algorithms are gaining more popularity now a days as the algorithms are more efficient for complex problems. In this paper, we have explored the research work done by various researchers on regression testing using nature-inspired approaches. It is found that biology inspired computation e.g. genetic algorithm have been widely used in regression testing optimization with the intent of maximizing fault or code coverage in minimum time. It is also concluded that nature-inspired approaches have great potential to optimize regression testing problems.", "published_in": "4th International Conference on Computing Communication and Automation (ICCCA)", "publisher": "IEEE", "doi": "10.1109/CCAA.2018.8777692", "date": "2019-07-29", "tcp": "X", "tcs": "X", "tsr": "X", "tsa": "", "paper_count": "15", "years_covered": "1999-2016", "systematic": "", "approach": "some Genetic and Evolutionary approaches, ant-colony opt, swarm opt., etc\nThey classify the techniques among: Physics/Chemistry Inspired,  Biology Inspired, and Social Phenomenon Inspired", "metrics": "They summarize that most used metrics are: \ntotal coverage, \nAPFD, \nexecution cost, and \nreduced percentage of test suite size\nIn the paper a table is included that give a detailed report of metrics in each study", "conclusions": "Genetic algorithms and biology-inspired are the most frequently used, and provide effective approaches", "open_challenges": "Lack of standard benchmarking data\nHow to choose the approach as there are many involved factors"}, {"year": "2018", "authors": "Rehman Khan, Saif Ur; Lee,  Sai Peck; Javaid, Nadeem; Abdul, Wadood", "title": "A systematic review on test suite reduction: Approaches, experiment\u2019s quality evaluation, and guidelines", "bibtex": "rehman_khan_systematic_2018", "abstract": "Regression testing aims at testing a system under test (SUT) in the presence of changes. As a SUT changes, the number of test cases increases to handle the modifications, and ultimately, it becomes practically impossible to execute all of them within limited testing budget. Test suite reduction (TSR) approaches are widely used to improve the regression testing costs by selecting representative test suite without compromising effectiveness, such as fault-detection capability, within allowed time budget. The aim of this systematic review is to identify state-of-the-art TSR approaches categories, assess the quality of experiments reported on this subject, and provide a set of guidelines for conducting future experiments in this area of research. After applying a two-facet study selection procedure, we finalized 113 most relevant studies from an initial pool of 4230 papers published in the field of TSR between 1993 and 2016. The TSR approaches are broadly classified into four main categories based on the literature including greedy, clustering, search, and hybrid approaches. It is noted that majority of the experiments in TSR do not follow any specific guidelines for planning, conducting, and reporting the experiments, which may pose validity threats related to their results. Thus, we recommend conducting experiments that are better designed for the future. In this direction, an initial set of recommendations is provided that are useful for performing well-designed experiments in the field of TSR. Furthermore, we provide a number of future research directions based on current trends in this field of research.", "published_in": "IEEE Access", "publisher": "IEEE", "doi": "10.1109/ACCESS.2018.2809600", "date": "2018-02-27", "tcp": "", "tcs": "", "tsr": "X", "tsa": "", "paper_count": "113", "years_covered": "1993-2016", "systematic": "X", "approach": "TSR are classified as: Greedy, Clustering (similarity-based) and Search-based, as well as hybrid ", "metrics": "Fault Detection Capability (FDC)\nPercentage of Test Suite Reduction (PTSR)\nFault Detection Loss (FDL)\nFor scalability measures they report SUT size and number of test cases", "conclusions": "One main conclusion is that TSR studies did not follow adequate criteria in the experiments and hence the authors of this survey build and propose some standard guidelines", "open_challenges": "Model-based TSR; evaluating TSR heuristics with cost-effectiveness and efficiency; comparing with consolidated benchmarks; industrial-strength datasets; comparative studies; branchmania situation; scalability with search algorithms"}, {"year": "2019", "authors": "bin Ali, Nauman; Engstr\u00f6m, Emelie; Taromirad, Masoumeh; Mousavi, Mohammad Reza; Minhas, Nasir Mehmood; Helgesson, Daniel; Kunze, Sebastian; Varshosaz, Mahsa", "title": "On the search for industry-relevant regression testing research", "bibtex": "bin_ali_search_2019", "abstract": "Regression testing is a means to assure that a change in the software, or its execution environment, does not introduce new defects. It involves the expensive undertaking of rerunning test cases. Several techniques have been proposed to reduce the number of test cases to execute in regression testing, however, there is no research on how to assess industrial relevance and applicability of such techniques. We conducted a systematic literature review with the following two goals: firstly, to enable researchers to design and present regression testing research with a focus on industrial relevance and applicability and secondly, to facilitate the industrial adoption of such research by addressing the attributes of concern from the practitioners' perspective. Using a reference-based search approach, we identified 1068 papers on regression testing. We then reduced the scope to only include papers with explicit discussions about relevance and applicability (i.e. mainly studies involving industrial stakeholders). Uniquely in this literature review, practitioners were consulted at several steps to increase the likelihood of achieving our aim of identifying factors important for relevance and applicability. We have summarised the results of these consultations and an analysis of the literature in three taxonomies, which capture aspects of industrial-relevance regarding the regression testing techniques. Based on these taxonomies, we mapped 38 papers reporting the evaluation of 26 regression testing techniques in industrial settings. \u00a9 2019, The Author(s).", "published_in": "Empirical Software Engineering", "publisher": "Springer", "doi": "10.1007/s10664-018-9670-1", "date": "2019-02-12", "tcp": "X", "tcs": "X", "tsr": "X", "tsa": "", "paper_count": "38", "years_covered": "2002-2017", "systematic": "?", "approach": "Out of the 38 PS considered they classify 26 distinct techniques that are classified according to their taxonomy (Table 5 in the paper)", "metrics": "Classify the possible effects of improvement into 3 categories: coverage, efficiency&effectiveness, awareness. In turn, coverage mainly involved \"feature\" and \"combinatorial\": most papers considered code coverage, but this was considered not important by the practitiones. Eff&eff involved: test size reduction, test time reduction, improved precision, less time for fault detection, reduced resources, improved fault detection, also wrt severe ones, and reduced cost of failures. Finally awareness onl;y measured increased transparency", "conclusions": "One important result for researchers is their Table 4 in which they introduce a taxonomy of relevant factors (validated by three senior testers from industry) classied in scope/context/effect/information. (I wonder whether in our secondary study we should take this into account when we classify primary studies?)", "open_challenges": "coverage information is only partially relevant to practitioners; context information is neglected; focus should be on severe faults and cost reduction; address industrial context factors; study people-related factors"}, {"year": "2019", "authors": "Bajaj, Anu; Sangwan, Om Prakash", "title": "A systematic literature review of test case prioritization using genetic algorithms", "bibtex": "bajaj_systematic_2019", "abstract": "Regression testing is the essential process of software maintenance and evolution phase of the software development life cycle for assuring the quality and reliability of updated software. Test case prioritization is the technique of regression testing to reduce the time and effort required for regression testing. Search-based algorithms are used to enhance the efficiency and effectiveness of the method. Among these search-based optimization algorithms, genetic algorithms are becoming more popular among researchers since the last decade. In this paper, we are doing a systematic literature review, i.e., a secondary study of test case prioritization using genetic algorithms. The objective of this review is to examine and classify the current state of use of the genetic algorithm in test case prioritization. In other words, to give a base for the advancement of test case prioritization research using genetic algorithms. With the use of the systematic literature review protocol, we selected the most relevant studies (20 out of 384) from the appropriate repositories by using a set of search keywords, inclusion/exclusion criteria and the quality assessment of studies. The data extraction and synthesis process and the taxonomic classification are used to answer the research questions. We also performed a rigorous analysis of the techniques by comparing them on research methodology, the prioritization method, dataset specification, test suite size, types of genetic algorithms used, performance metrics, and the validation criteria. The whole process took four months for comprehensive analysis and classification of primary studies. We observed that the parameter settings, the type of operators, the probabilistic rate of operators, and fitness function design have a significant impact on the quality of the solutions obtained. This systematic literature review yields that genetic algorithms have great potential in solving test case prioritization problems, and the area is open ...", "published_in": "IEEE Access", "publisher": "IEEE", "doi": "10.1109/ACCESS.2019.2938260", "date": "2019-08-29", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "20", "years_covered": "2006-2018", "systematic": "X", "approach": "They consider all different types of approaches all centered on GA. The classification they make is among: simple GA, enhanced, two or more and NSGA, single or multiobjective GA (which is reflected in the fitness funtion), the different type of operators, what they cover among code, reqs, fault.", "metrics": "APFD, Fault detection capability, average percentage of element coverage (?never heard?), coverage.", "conclusions": "More empirical research is needed, also in particular for setting the approach parameters\nThe large majority has used coverage that may be an issue, they would rcommend more req coverage.\nThey mention that works have used industrial application which are not available, and this may be a problem.\nImportant: they notice that scalability should be investigated", "open_challenges": "Lack of empirical studies; more requirements-based TCP; industrial & hardly-reproducible vs open & unrealistic datasets; small number of case studies for validation; scalability; lack of standard tools; genetic algorithms; performance metrics (APEC, FDC, CE); statistical analysis"}, {"year": "2020", "authors": "Prado Lima, Jackson A.; Vergilio, Silvia R.", "title": "Test Case Prioritization in Continuous Integration environments: A systematic mapping study", "bibtex": "prado_lima_test_2020", "abstract": "Context: Continuous Integration (CI) environments allow frequent integration of software changes, making software evolution more rapid and cost-effective. In such environments, the regression test plays an important role, as well as the use of Test Case Prioritization (TCP) techniques. Such techniques attempt to identify the test case order that maximizes certain goals, such as early fault detection. This research subject has been raising interest because some new challenges are faced in the CI context, as TCP techniques need to consider time constraints of the CI environments. Objective: This work presents the results of a systematic mapping study on Test Case Prioritization in Continuous Integration environments (TCPCI) that reports the main characteristics of TCPCI approaches and their evaluation aspects. Method: The mapping was conducted following a plan that includes the definition of research questions, selection criteria and search string, and the selection of search engines. The search returned 35 primary studies classified based on the goal and kind of used TCP technique, addressed CI particularities and testing problems, and adopted evaluation measures. Results: The results show a growing interest in this research subject. Most studies have been published in the last four years. 80% of the approaches are history-based, that is, are based on the failure and test execution history. The great majority of studies report evaluation results by comparing prioritization techniques. The preferred measures are Time and number/percentage of Faults Detected. Few studies address CI testing problems and characteristics, such as parallel execution and test case volatility. Conclusions: We observed a growing number of studies in the field. Future work should explore other information sources such as models and requirements, as well as CI particularities and testing problems, such as test case volatility, time constraint, and flaky tests, to solve existing challenges and offer cost-effective approaches to the software industry. \u00a9 2020 Elsevier B.V.", "published_in": "Information and Software Technology", "publisher": "Elsevier", "doi": "10.1016/j.infsof.2020.106268", "date": "2020-01-25", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "35", "years_covered": "2009-2019", "systematic": "X", "approach": "notably they observe that for CI the most common type of approach is History-based, in particular failure and test execution history; they also make an observation that there is a trend towards more lightweight approaches, in previous years there were more studies on coverage or distribution based ... e.g. 50% of studies in 2017/18 are using history ", "metrics": "Most widely used measure is Time by which they collect different measures related to time (48%), followed by FD i.e., fault detected (33%), APFD (24%), APFDc (18%), Expense i.e. relative number of statement to be examined to locate the fault (15%), and NAPFD i.e. Normalized APFD (9%). ", "conclusions": "They observe that CI poses constraints on TCP, as \"search-based techniques or other ones that require extensive code analysis and coverage may be unfeasible due to the time constraints and the test budget for a build\"", "open_challenges": "Model-based prioritization, most reliable information source?, use of GPU, blockchain+ML, hyper-heuristics, TCP+fault localization, more programming languages, more industrial enviroments, more open science approaches, more NAPFD, more statistical test, more scalability, lack of a benchmark, CI+testing"}, {"year": "2020", "authors": "Hasnain, Muhammad; Ghani, Imran; Pasha, Muhammad Fermi; Lim, Chern Hong; Jeong, Seung Ryul", "title": "A Comprehensive Review on Regression Test Case Prioritization Techniques for Web Services", "bibtex": "hasnain_comprehensive_2020", "abstract": "Test Case Prioritization (TCP) involves the rearrangement of test cases on a prioritized basis for various services. This research work focuses on TCP in web services, as it has been a growing challenge for researchers. Web services continuously evolve and hence require reforming and re-execution of test cases to ensure the accurate working of web services. This study aims to investigate gaps, issues, and existing solutions related to test case prioritization. This study examines research publications within popular selected databases. We perform a meticulous screening of research publications and selected 65 papers through which to answer the proposed research questions. The results show that criteria-based test case prioritization techniques are reported mainly in 41 primary studies. Test case prioritization models, frameworks, and related algorithms are also reported in primary studies. In addition, there are eight issues related to TCP techniques. Among these eight issues, optimization and high effectiveness are most discussed within primary studies. This systematic review has identified that a significant proportion of primary studies are not involved in the use of statistical methods in measuring or comparing the effectiveness of TCP techniques. However, a large number of primary studies use 'Average Percentage of Faults Detected' (APFD) or extended APFD metrics to compute the performance of techniques for web services.\n", "published_in": "KSII Transactions on Internet and Information Systems (TIIS)", "publisher": "Korean Society for Internet Information", "doi": "10.3837/tiis.2020.05.001", "date": "2020-05-31", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "65", "years_covered": "2001-2017", "systematic": "X", "approach": "Authors highlight that coverage-based approaches have been widely used to address TCP.\n\n-Coverage-based\n-Session-based\n-Location-based\n-Similarity-based\n-Requirement-based\n-Flow graph-based\n-Risk-based\n-Technology-based", "metrics": "APFD (and its variations) is the most frequently used metric:\n- APFD\n- APFD-P\n- NAPFD\n- Extended APFDc\n- APFDD\n- FDR ?\n- HMFD ?\n- WPFD ?\n- TPFD ?\n\nOther metrics are available in Table 4:\n- MATLAB tool\n- Wsrbench\n- A C program tool\n- Average RP\n- Test cost\n- Test efficiency\n- Hamonic means\n- Code metrics\n\nMany of these make little sense on their own. We'd have to investigate each individual paper to understand the purpose of the metrics. However only two were used by more than one paper: APFD (22) and HMFD (2).", "conclusions": "-Coverage-based TCP techniques have been greatly covered in primary studies\n\n-Most primary studies have mentioned that APFD is a primarily-applied metric, used to compute or compare the effectiveness of TCP approaches.\n\n-Statistical methods were employed in primary studies, including ANOVA analysis and descriptive statistics.\n\n-The authors consider that cost-overhead, optimization, and high effectiveness are essential issues related to web services TCP techniques", "open_challenges": "Overfitting, scalability, robustness, network faults, service integration, metrics aside from APFD"}, {"year": "2019", "authors": "Lou, Yiling; Chen, Junjie; Zhang, Lingming; Hao, Dan", "title": "A Survey on Regression Test-Case Prioritization", "bibtex": "lou_survey_2018", "abstract": "Regression testing is crucial for ensuring the quality of modern software systems, but can be extremely costly in practice. Test-case prioritization has been proposed to improve the effectiveness of regression testing by scheduling the execution order of test cases to detect regression bugs faster. Since its first proposal, test-case prioritization has been intensively studied in the literature. In this chapter, we perform an extensive survey and analysis on existing test-case prioritization techniques, as well as pointing out future directions for test-case prioritization. More specifically, we collect 191 papers on test-case prioritization from 1997 to 2016 and conduct a detailed survey to systematically investigate these work from six aspects, i.e., algorithms, criteria, measurements, constraints, empirical studies, and scenarios. For each of the six aspects, we discuss the existing work and the trend during the evolution of test-case prioritization. Furthermore, we discuss the current limitations/issues in test-case prioritization research, as well as potential future directions on test-case prioritization. Our analyses provide the evidence that test-case prioritization topic is attracting increasing interests, while the need for practical test-case prioritization tools remains.", "published_in": "Advances in Computers", "publisher": "Elsevier", "doi": "10.1016/bs.adcom.2018.10.001", "date": "2018-12-14", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "191", "years_covered": "1997-2016", "systematic": "X", "approach": "The TCP approaches are classified based on different categories:\n\nCRITERION:\nStructural, Model-Level, Fault-Related, Test Input-Based, Change Impact-Based, Risk, Similarity, Service History, Requirement\n\nPRIORITIZATION ALGORITHM:\ngreedy algorithm, search-based algorithm, information-retrieval-based algorithm, integrate-linear-programming-based algorithm, machine-learning-based algorithm.\n\nAPPLICATION SCENARIO:\nGeneral TCP vs Version-Specific TCP", "metrics": " - Most of the surveyed work uses APFD\n\n - APFDc, APXC, NAPFD and RAPFD are variations of APFD\n\n - Weighted gain of faults detected (WGFD): the idea is to weight and sum fault-detection rates of different test cases so as to define \u201cfastness\u201d in different testing scenarios\n\n - Harmonic mean of the rate of fault detection (HMFD): to measure how quickly a prioritized TS can detect faults, independent from the size of TS.", "conclusions": "While the topic of test case prioritization remains attractive to researchers the need for practical test case prioritization tools remains.\n\nMany studies do not consider important aspects while evaluating TCP: the use of real-world software as subjects, algorithm complexity, cost of collecting data, efficiency (these could also be cosidered as \"open challenges\")", "open_challenges": " - Most of the widely used testing criteria are either less precise or costly\n\n - Too much reliance on APFD (what about APFDc, TTFF, TTLF?)\n\n - Algorithm complexity not considered\n\n - Cost of collecting necessary data\n\n - Lack of evaluation on real software with real faults\n\n- The efficiency measurement is mostly ignored in TCP\n\n - Lack of available, effective and easy-to-use TCP tools.\n\n - Intermediate vs ultimate goal"}, {"year": "2018", "authors": "Khatibsyarbini, Muhammad; Isa, Mohd Adham; Jawawi, Dayang N. A.; Tumeng, Rooster", "title": "Test case prioritization approaches in regression testing: A systematic literature review", "bibtex": "khatibsyarbini_test_2018", "abstract": "Context\nSoftware quality can be assured by going through software testing process. However, software testing phase is an expensive process as it consumes a longer time. By scheduling test cases execution order through a prioritization approach, software testing efficiency can be improved especially during regression testing.\n\nObjective\nIt is a notable step to be taken in constructing important software testing environment so that a system's commercial value can increase. The main idea of this review is to examine and classify the current test case prioritization approaches based on the articulated research questions.\n\nMethod\nSet of search keywords with appropriate repositories were utilized to extract most important studies that fulfill all the criteria defined and classified under journal, conference paper, symposiums and workshops categories. 69 primary studies were nominated from the review strategy.\n\nResults\nThere were 40 journal articles, 21 conference papers, three workshop articles, and five symposium articles collected from the primary studies. As for the result, it can be said that TCP approaches are still broadly open for improvements. Each approach in TCP has specified potential values, advantages, and limitation. Additionally, we found that variations in the starting point of TCP process among the approaches provide a different timeline and benefit to project manager to choose which approaches suite with the project schedule and available resources.\n\nConclusion\nTest case prioritization has already been considerably discussed in the software testing domain. However, it is commonly learned that there are quite a number of existing prioritization techniques that can still be improved especially in data used and execution process for each approach.\n", "published_in": "Information and Software Technology", "publisher": "Elsevier", "doi": "10.1016/j.infsof.2017.08.014", "date": "2017-09-01", "tcp": "X", "tcs": "", "tsr": "", "tsa": "", "paper_count": "80", "years_covered": "1999-2016", "systematic": "X", "approach": "Authors grouped the TCP approaches into seven main dimensions:\n-Coverage-based\n-Requirement-based\n-Risk-based\n-Search-based\n-Fault-based\n-History-based\n-Others-based (e.g., cost-aware, bayesian-network, topic model, multi-criteria, workflow)\n\nA description of each approach is provided in Table 10. Advantages and limitations are discussed in Table A4.", "metrics": " - APFD 51%\n - Coverage Effectiveness (CE) 10%\n - APFDc 9%\n - Time execution 7%\n - Others (Mostly variations of APFD) 23%", "conclusions": " - TCP approaches are still broadly open for improvements\n\n - Each TCP approach has specific advantages and limitations\n\n - Authors found that variations in the starting point of TCP process provide a different timeline and benefit; project managers should choose which approaches to use based on the project schedule and available resources.\n\n - Search-based TCP is the most used method among the surveyed studies, followed by coverage-based TCP\n\n - Implementation of artificial intelligence element has been a trend among researchers", "open_challenges": "-The data or information classification in TCP (requirement, system models, and source code) needs to be investigated further\n\n-The specific time frame for the execution process of TCP needs to be detailed out.\n\n-Early prioritization in the early stage of a system development life cycle needs to be further investigated\n\n-Human involvement in decision-making and estimation (human error), \n\n-How does TCP support fault localization compared to TCS?"}]