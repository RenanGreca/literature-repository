[
  {
    "year": 2016,
    "authors": "Srikanth, Hema; Hettiarachchi, Charitha; Do, Hyunsook",
    "title": "Requirements Based Test Prioritization Using Risk Factors",
    "bibtex": "srikanth_requirements_2016",
    "abstract": "Context Software testing is an expensive and time-consuming process. Software engineering teams are often forced to terminate their testing efforts due to budgetary and time constraints, which inevitably lead to long term issues with quality and customer satisfaction. Test case prioritization (TCP) has shown to improve test effectiveness. Objective The results of our prior work on requirements-based test prioritization showed improved rate of fault detection on industrial projects; the customer priority (CP) and the fault proneness (FP) were the biggest contributing factors to test effectiveness. The objective of this paper is to further investigate these two factors and apply prioritization based on these factors in a different domain: an enterprise level cloud application. We aim to provide an effective prioritization scheme that practitioners can implement with minimum effort. The other objective is to compare the results and the benefits of these two factors with two risk-based prioritization approaches that extract risks from the system requirements categories. Method Our approach involved analyzing and assigning values to each requirement based on two important factors, CP and FP, so that the test cases for high-value requirements are prioritized earlier for execution. We also proposed two requirements-based TCP approaches that use risk information of the system. Results Our results indicate that the use of CP and FP can improve the effectiveness of TCP. The results also show that the risk-based prioritization can be effective in improving the TCP. Conclusion We performed an experiment on an enterprise cloud application to measure the fault detection rate of different test suites that are prioritized based on CP, FP, and risks. The results depict that all approaches outperform the random prioritization approach, which is prevalent in the industry. Furthermore, the proposed approaches can easily be used in the industry to address the schedule and budget constraints at the testing phase.",
    "published_in": "Information and Software Technology",
    "publisher": "Elsevier",
    "source": "Query",
    "doi": "https://doi.org/10.1016/j.infsof.2015.09.002",
    "date": "2015-09-26",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "IBM analytics application (1700+ TCs)",
    "ind_partner": "IBM (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Requirements-based",
    "metrics": "APFD",
    "open_challenges": "Apply different levels of severity to faults."
  },
  {
    "year": 2016,
    "authors": "Noor, Tanzeem Bin; Hemmati, Hadi",
    "title": "A similarity-based approach for test case prioritization using historical failure data",
    "bibtex": "noor_similarity-based_2016",
    "abstract": "Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures. Â© 2015 IEEE.",
    "published_in": "2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ISSRE.2015.7381799",
    "date": "2016-01-14",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Defects4J (up to 7927 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Similarity-based (BC, HD, ED)",
    "metrics": "Tests till first fault (%), method coverage, size of testcase",
    "open_challenges": "n/a"
  },
  {
    "year": 2016,
    "authors": "Schwartz, Amanda; Do, Hyunsook",
    "title": "Cost-effective regression testing through adaptive test prioritization strategies",
    "bibtex": "schwartz_cost-effective_2016",
    "abstract": "We propose two new ATP (Adaptive Test Prioritization) strategies.We \nconduct an empirical study investigating existing and new ATP \nstrategies.We provide a statistical analysis examining all ATP \nstrategies proposed.Our findings show that FESART is the most consistent\n cost-effective ATP strategy. Regression testing is an important part of\n the software development life cycle. It is also very expensive. Many \ndifferent techniques have been proposed for reducing the cost of \nregression testing. However, research has shown that the effectiveness \nof different techniques varies under different testing environments and \nsoftware change characteristics. In prior work, we developed strategies \nto investigate ways of choosing the most cost-effective regression \ntesting technique for a particular regression testing session. In this \nwork, we empirically study the existing strategies presented in prior \nwork as well as develop two additional Adaptive Test Prioritization \n(ATP) strategies using fuzzy analytical hierarchy process (AHP) and the \nweighted sum model (WSM). We also provide a comparative study examining \neach of the ATP strategies presented to date. This research will provide\n researchers and practitioners with strategies to utilize in regression \ntesting plans as well as provide data to use when deciding which of the \nstrategies would best fit their testing needs. The empirical studies \nprovided in this research show that utilizing these strategies can \nimprove the cost-effectiveness of regression testing.",
    "published_in": "Journal of Systems and Software",
    "publisher": "Elsevier",
    "source": "backward",
    "doi": "https://doi.org/10.1016/j.jss.2016.01.018",
    "date": "2016-01-27",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Continuous integration",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "SIR (up to 912 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Approach is adaptively chosen for each software and even version using some decision support system",
    "metrics": "Eelative cost-benefit (in dollars) among compared TCP techniques",
    "open_challenges": "Expand to other techniques, and to larger programs."
  },
  {
    "year": 2016,
    "authors": "Hirzel, Matthias; Klaeren, Herbert",
    "title": "Graph-walk-based selective regression testing of web applications created with Google web toolkit",
    "bibtex": "hirzel_graph-walk-based_2016",
    "abstract": "Modern web applications are usually based on JavaScript. Due to its loosely typed, dynamic nature, test execution is time expensive and costly. Techniques for regression testing and fault-localization as well as frameworks like the Google Web Toolkit (GWT) ease the develop- ment and testing process, but still require approaches to reduce the testing effort. In this paper, we investigate the efficiency of a spe- cialized, graph-walk based selective regression testing technique that aims to detect code changes on the client side in order to determine a reduced set of web tests. To do this, we analyze web applications created with GWT on different precision levels and with varying looka- heads. We examine how these parameters affect the localization of client-side code changes, run time, memory consumption and the num- ber of web tests selected for re-execution. In addition, we propose a dynamic heuristics which targets an analysis that is as exact as possible while reducing memory consumption. The results are partially appli- cable on non-GWT applications. In the context of web applications,we see that the efficiency relies to a great degree on both the structure of the application and the code modifications, which is why we propose further measures tailored to the results of our approach.",
    "published_in": "Software Engineering Workshops 2016",
    "publisher": "CEUR",
    "source": "backward",
    "doi": "http://ceur-ws.org/Vol-1559/paper05.pdf",
    "date": "2016-02-17",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Web applications",
    "ind_motivation": "FALSE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Hupa (32 TCs) and Meisterplan (104 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "FALSE",
    "suppl_url": "https://github.com/MH42/srt-for-web-apps",
    "included": "TRUE",
    "approach": "Graph-based",
    "metrics": "Time/memory consumption, selection percentage",
    "open_challenges": "n/a"
  },
  {
    "year": 2016,
    "authors": "Lu, Yafeng; Lou, Yiling; Cheng, Shiang; Zhang, Lingming; Hao, Dan; Zhou, Yangfan; Zhang, Lu",
    "title": "How does regression test prioritization perform in real-world software evolution?",
    "bibtex": "lu_how_2016",
    "abstract": "In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.",
    "published_in": "ICSE '16: Proceedings of the 38th International Conference on Software Engineering",
    "publisher": "ACM",
    "source": "backward",
    "doi": "https://doi.org/10.1145/2884781.2884874",
    "date": "2016-05-14",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Open-source Java programs (up to 5269 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "FALSE",
    "suppl_url": "https://personal.utdallas.edu/~lxz144130/icse16support.html",
    "included": "TRUE",
    "approach": "greedy, search-based, adaptive, time-aware",
    "metrics": "APFD",
    "open_challenges": "n/a"
  },
  {
    "year": 2016,
    "authors": "VÃ¶st, Sebastian; Wagner, Stefan",
    "title": "Trace-based test selection to support continuous integration in the automotive industry",
    "bibtex": "vost_trace-based_2016",
    "abstract": "System testing in the automotive industry is a very expensive and time-consuming task of growing importance, because embedded systems in the domain are distributed over numerous controllers (ECUs). Modern software development techniques such as continuous integration require regular, repeated and fast testing. To achieve this in the automotive domain, test suites for a specific software change must be tailored. We propose a novel test selection technique for system-level functions in the automotive industry based on component and communication models. The idea is to follow input and output signals that are used in the testing steps through the ECUs implementing a function. We select only those tests for a planned integration in which at least one of the signals sent in its steps is processed by the ECU that was changed and thus triggered the integration. The technique is well-suited for black-box testing since it requires only the full test suite specification and the system architecture. We applied the technique to a test suite of the Active Cruise Control function at BMW Group in the context of hardware-in-the-loop system testing and found the possible reduction rates to be 82.3% on average in comparison to the full test suite. Possible future work includes the evaluation with a wider set of functions, the evaluation of the fault detection rate, further automation and combination with other test selection techniques.",
    "published_in": "CSED '16: Proceedings of the International Workshop on Continuous Software Evolution and Delivery",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/2896941.2896951",
    "date": "2016-05-14",
    "categories": "Orchestration technique/Context-specific",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Continuous integration, automotive",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "BMW adaptive cruise control (186 TCs)",
    "ind_partner": "BMW (Germany)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Trace-based",
    "metrics": "Selection rate",
    "open_challenges": ""
  },
  {
    "year": 2016,
    "authors": "Wang, Shuai; Ali, Shaukat; Yue, Tao; Bakkeli, Oyvind; Liaaen, Marius",
    "title": "Enhancing test case prioritization in an industrial setting with resource awareness and multi-objective search",
    "bibtex": "wang_enhancing_2016",
    "abstract": "Test case prioritization is an essential part of test execution systems for large organizations developing software systems in the context that their software versions are released very frequently. They must be tested on a variety of compatible hardware with different configurations to ensure correct functioning of a software version on a compatible hardware. In practice, test case execution must not only execute cost-effective test cases in an optimal order, but also optimally allocate required test resources, in order to deliver high quality software releases. To optimize the current test execution system for testing software releases developed for Videoconferencing Systems (VCSs) at Cisco, Norway, in this paper, we propose a resource-aware multi-objective optimization solution with a fitness function defined based on four cost-effectiveness measures. In this context, a set of software releases must be tested on a set of compatible VCS hardware (test resources) by executing a set of cost-effective test cases in an optimal order within a given test cycle constrained by maximum allowed time budget and maximum available test resources. We empirically evaluated seven search algorithms regarding their performance and scalability by comparing with the current practice (random ordering (RO)). The results show that the proposed solution with the best search algorithm (i.e., Random-Weighted Genetic Algorithm) improved the current practice by reducing on average 40.6% of time for test resource allocation and test case execution, improved test resource usage on average by 37.9% and fault detection on average by 60%.",
    "published_in": "ICSE '16: Proceedings of the 38th International Conference on Software Engineering Companion",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/2889160.2889240",
    "date": "2016-05-14",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Software product line",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Cisco videoconferencing system (305 TCs)",
    "ind_partner": "Cisco (Norway)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "MAYBE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "resource-aware, multi-objective search",
    "metrics": "Number of test cases, resource usage, fault detection capability, total time, prioritization density",
    "open_challenges": "Further industrial case studies; survey with Cisco engineers for validation and application of approach."
  },
  {
    "year": 2016,
    "authors": "Srikanth, Hema; Cashman, Mikaela; Cohen, Myra B.",
    "title": "Test Case Prioritization of Build Acceptance Tests for an Enterprise Cloud Application",
    "bibtex": "srikanth_test_2016",
    "abstract": "The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize BAT tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't.",
    "published_in": "Journal of Systems and Software",
    "publisher": "Elsevier",
    "source": "Query",
    "doi": "https://doi.org/10.1016/j.jss.2016.06.017",
    "date": "2016-06-16",
    "categories": "Orchestration technique/Context-specific",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Prioritizing Build Acceptance Tests based on historical ï¬eld failures",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "IBM cloud application (1000+ TCs)",
    "ind_partner": "IBM (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "3 history-based TCP heuristics.",
    "metrics": "APFD and APFDc",
    "open_challenges": "- investigate the use of in-house defects when ï¬eld failures data is not available.\n - evaluate systems where running times of individual test cases are different."
  },
  {
    "year": 2017,
    "authors": "Blondeau, Vincent; Etien, Anne; Anquetil, Nicolas; Cresson, Sylvain; Croisy, Pascal; Ducasse, StÃ©phane",
    "title": "Test case selection in industry: an analysis of issues related to static approaches",
    "bibtex": "blondeau_test_2017",
    "abstract": "Automatic testing constitutes an important part of everyday development practice. Worldline, a major IT company, is creating more and more tests to ensure the good behavior of its applications and gains in efficiency and quality. But running all these tests may take hours. This is especially true for large systems involving, for example, the deployment of a web server or communication with a database. For this reason, tests are not launched as often as they should be and are mostly run at night. The company wishes to improve its development and testing process by giving to developers rapid feedback after a change. An interesting solution is to reduce the number of tests to run by identifying only those exercising the piece of code changed. Two main approaches are proposed in the literature: static and dynamic. The static approach creates a model of the source code and explores it to find links between changed methods and tests. The dynamic approach records invocations of methods during the execution of test scenarios. Before deploying a test case selection solution, Worldline created a partnership with us to investigate the situation in its projects and to evaluate these approaches on three industrial, closed source, cases to understand the strengths and weaknesses of each solution. We propose a classification of problems that may arise when trying to identify the tests that cover a method. We give concrete examples of these problems and list some possible solutions. We also evaluate other issues such as the impact on the results of the frequency of modification of methods or considering groups of methods instead of single ones. We found that solutions must be combined to obtain better results, and problems have different impacts on projects. Considering commits instead of individual methods tends to worsen the results, perhaps due to their large size.",
    "published_in": "Software Quality Journal",
    "publisher": "Springer",
    "source": "Query",
    "doi": "https://doi.org/10.1007/s11219-016-9328-4",
    "date": "2016-07-08",
    "categories": "Case study",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Financial software",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Three undisclosed Worldline projects (5000+ TCs)",
    "ind_partner": "Worldline (France)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Dynamic vs. static selection",
    "metrics": "Selection ratio, precision, recall, F-measure",
    "open_challenges": "Experiment with hybrid static-dynamic approach; perform further experiments implementing the approach at Worldline."
  },
  {
    "year": 2016,
    "authors": "Pradhan, Dipesh; Wang, Shuai; Ali, Shaukat; Yue, Tao",
    "title": "Search-Based Cost-Effective Test Case Selection within a Time Budget: An Empirical Study",
    "bibtex": "pradhan_search-based_2016",
    "abstract": "Due to limited time and resources available for execution, test case selection always remains crucial for cost-effective testing. It is even more prominent when test cases require manual steps, e.g., operating physical equipment. Thus, test case selection must consider complicated trade-offs between cost (e.g., execution time) and effectiveness (e.g., fault detection capability). Based on our industrial collaboration within the Maritime domain, we identified a real-world and multi-objective test case selection problem in the context of robustness testing, where test case execution requires human involvement in certain steps, such as turning on the power supply to a device. The high-level goal is to select test cases for execution within a given time budget, where test engineers provide weights for a set of objectives, depending on testing requirements, standards, and regulations. To address the identified test case selection problem, we defined a fitness function including one cost measure, i.e., Time Difference (TD) and three effectiveness measures, i.e., Mean Priority (MPR), Mean Probability (MPO) and Mean Consequence (MC) that were identified together with test engineers. We further empirically evaluated eight multi-objective search algorithms, which include three weight-based search algorithms (e.g., Alternating Variable Method) and five Pareto-based search algorithms (e.g., Strength Pareto Evolutionary Algorithm 2 (SPEA2)) using two weight assignment strategies (WASs). Notice that Random Search (RS) was used as a comparison baseline. We conducted two sets of empirical evaluations: 1) Using a real world case study that was developed based on our industrial collaboration; 2) simulating the real world case study to a larger scale to assess the scalability of the search algorithms. Results show that SPEA2 with either of the WASs performed the best for both the studies. Overall, SPEA2 managed to improve on average 32.7%, 39% and 33% in terms of MPR, MPO and MC respectively as compared to RS.",
    "published_in": "GECCO '16: Proceedings of the Genetic and Evolutionary Computation Conference 2016",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/2908812.2908850",
    "date": "2016-07-20",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Realistic case study based on standards, public requirements and a handbook (165 TCs)",
    "ind_partner": "Undisclosed oil & gas company (Norway)",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Search-based",
    "metrics": "fitness value, hypervolume",
    "open_challenges": "Hybrid search and evolutionary algorithms for better performance."
  },
  {
    "year": 2016,
    "authors": "Chen, Junjie; Bai, Yanwei; Hao, Dan; Xiong, Yingfei; Zhang, Hongyu; Zhang, Lu; Xie, Bing",
    "title": "Test case prioritization for compilers: A text-vector based approach",
    "bibtex": "chen_test_2016",
    "abstract": "Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efficiency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach first transforms each test case into a text-vector by extracting its tokens which reflect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efficiency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and find that our approach is much more efficient than the existing approaches and is effective in prioritizing test cases.",
    "published_in": "2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)",
    "publisher": "IEEE",
    "source": "backward",
    "doi": "https://doi.org/10.1109/ICST.2016.19",
    "date": "2016-07-21",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Test of C compilers",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "C compilers GCC (3.3M LOC) and LLVM (4.7M LOC)",
    "ind_partner": "",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "text-vector based (similarity)",
    "metrics": "-APFD\n-time spent in prioritization",
    "open_challenges": "n/a"
  },
  {
    "year": 2016,
    "authors": "Buchgeher, Georg; Klammer, Claus; Heider, Wolfgang; Schuetz, Martin; Huber, Heinz",
    "title": "Improving testing in an enterprise SOA with an architecture-based approach",
    "bibtex": "buchgeher_improving_2016",
    "abstract": "High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis. Â© 2016 IEEE.",
    "published_in": "2016 13th Working IEEE/IFIP Conference on Software Architecture (WICSA)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/WICSA.2016.24",
    "date": "2016-07-21",
    "categories": "Orchestration technique/Context-specific",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "General(Java)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Undisclosed projects from RSG",
    "ind_partner": "Raiffeisen Software GmbH (Austria)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "model-based",
    "metrics": "Load factor?",
    "open_challenges": "Combine different information sources to improve TCP+TCS; incorporate system evolution information; use derived information towards test case generation"
  },
  {
    "year": 2016,
    "authors": "Tahvili, Sahar; Saadatmand, Mehrdad; Larsson, Stig; Afzal, Wasif; Bohlin, Markus; Sundmark, Daniel",
    "title": "Dynamic integration test selection based on test case dependencies",
    "bibtex": "tahvili_dynamic_2016",
    "abstract": "Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.",
    "published_in": "2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
    "publisher": "IEEE",
    "source": "backward",
    "doi": "https://doi.org/10.1109/ICSTW.2016.14",
    "date": "2016-08-04",
    "categories": "",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Combined static and dynamic selection and prioritization of integration tests",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Brake and air supply systems from Bombardier",
    "ind_partner": "Bombardier Transportation (Sweden)",
    "ind_author": "FALSE",
    "prac_feedback": "TRUE",
    "avai_tool": "N/A",
    "put_practice": "MAYBE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "The work combines offline prioritization with online selection from the prioritized sets. \nThis work is related to row 5 and here test case dependencies are also captured manually.",
    "metrics": "Requirement coverage; time efficiency; cost efficiency; fault detection probability. Introduces a \"dependency degree\" metric for prioritizing test cases; these are not an evaluation metric, but rather metrics attributed to each test case used by the TCP algorithm.",
    "open_challenges": "n/a"
  },
  {
    "year": 2016,
    "authors": "ClÃ¡udio MagalhÃ£es, Alexandre Mota, FlÃ¡via Barros, and Eliot Maia",
    "title": "Automatic selection of test cases for regression testing",
    "bibtex": "magalhaes_automatic_2016",
    "abstract": "Regression testing is a safety measure to attest that changes made on a system preserve prior accepted behavior. Identifying which test cases must compose a regression test suite in a certain development stage is tricky, particularly when one only has test cases and change requests described in natural language, and the execution of the test suite will be performed manually. That is the case of our industrial partner. We propose a selection of regression test cases based on information retrieval and implement as a web-service. In performed experiments, we show that we can improve the creation of regression test suites of our industrial partner by providing more effective test cases based on keywords analysis in an automatic way.",
    "published_in": "SAST: Proceedings of the 1st Brazilian Symposium on Systematic and Automated Software Testing",
    "publisher": "ACM",
    "source": "backward",
    "doi": "https://doi.org/10.1145/2993288.2993299",
    "date": "2016-09-19",
    "categories": "Technique analysis",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "An information retrieval-based TCS approach.",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Test repository from Motorola (up to 427 TCs)",
    "ind_partner": "Motorola Mobility (Brazil)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "An information retrieval-based TCS approach.",
    "metrics": "Traditional information retrieval-based metrics such as recall, number (and %) of elements returned at the top of the list (top-n).",
    "open_challenges": "- to completely replace the manual TCS approach with the automated one\n - try other variants of the merge strategy\n - Include source code as an input artefact\n - evaluate the effect of the derived TS on coverage"
  },
  {
    "year": 2016,
    "authors": "Aman, Hirohisa; Tanaka, Yuta; Nakano, Takashi; Ogasawara, Hideto; Kawahara, Minoru",
    "title": "Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing",
    "bibtex": "aman_application_2016",
    "abstract": "To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods. Â© 2016 IEEE.",
    "published_in": "2016 42th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/SEAA.2016.29",
    "date": "2016-10-18",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "General",
    "ind_motivation": "FALSE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Undisclosed system from Toshiba (300 TCs)",
    "ind_partner": "Toshiba (Japan)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "MAYBE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Similarity-based",
    "metrics": "GLC (number of versions in which a certain test T was not run), failure rate",
    "open_challenges": "Apply in other software domains and compare with other TCP techniques."
  },
  {
    "year": 2016,
    "authors": "Busjaeger, Benjamin; Xie, Tao",
    "title": "Learning for test prioritization: An industrial case study",
    "bibtex": "busjaeger_learning_2016",
    "abstract": "Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration envi-ronments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimiza-tion mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in indus-trial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques.",
    "published_in": "FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/2950290.2983954",
    "date": "2016-11-01",
    "categories": "Case study",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Web application",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Automation system at Salesforce (~45000 TCs)",
    "ind_partner": "Salesforce (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Machine learning-based (algo)\n\nCode coverage-based, similarity-based, history-based, test age (info)",
    "metrics": "APFD, recall",
    "open_challenges": "Deepen learning, adding more features."
  },
  {
    "year": 2016,
    "authors": "Yoshida, Hiroaki; Tokumoto, Susumu; Prasad, Mukul R.; Ghosh, Idradeep; Uehara, Tadahiro",
    "title": "FSX: A tool for fine-grained incremental unit test generation for C/C++ Programs",
    "bibtex": "yoshida_fsx_2016",
    "abstract": "Automated unit test generation bears the promise of significantly reducing test cost and hence improving software quality. However, the maintenance cost of the automatically generated tests presents a significant barrier to adoption of this technology. To address this challenge, in previous work, we proposed a novel technique for automated and fine-grained incremental generation of unit tests through minimal augmentation of an existing test suite. In this paper we describe a tool FSX, implementing this technique. We describe the architecture, user-interface, and salient features of FSX, and specific practical use-cases of its technology. We also report on a real, large-scale deployment of FSX as a practical validation of the underlying research contribution and of automated test generation research in general. Â© 2016 ACM.",
    "published_in": "FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/2950290.2983937",
    "date": "2016-11-01",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "",
    "tsr": "",
    "tsa": "X",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source tool iPerf (114 TCs)",
    "ind_partner": "Fujitsu (Japan)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Coverage-based",
    "metrics": "Statement coverage, branch coverage, #tests, runtime",
    "open_challenges": "n/a"
  },
  {
    "year": 2016,
    "authors": "Tahvili, Sahar; Bohlin, Markus; Saadatmand, Mehrdad; Larsson, Stig; Afzal, Wasif; Sundmark, Daniel",
    "title": "Cost-benefit analysis of using dependency knowledge at integration testing",
    "bibtex": "tahvili_cost-benefit_2016",
    "abstract": "In software system development, testing can take considerable time and resources, and there are numerous examples in the literature of how to improve the testing process. In particular, methods for selection and prioritization of test cases can play a critical role in efficient use of testing resources. This paper focuses on the problem of selection and ordering of integration-level test cases. Integration testing is performed to evaluate the correctness of several units in composition. Further, for reasons of both effectiveness and safety, many embedded systems are still tested manually. To this end, we propose a process, supported by an online decision support system, for ordering and selection of test cases based on the test result of previously executed test cases. To analyze the economic efficiency of such a system, a customized return on investment (ROI) metric tailored for system integration testing is introduced. Using data collected from the development process of a large-scale safety-critical embedded system, we perform Monte Carlo simulations to evaluate the expected ROI of three variants of the proposed new process. The results show that our proposed decision support system is beneficial in terms of ROI at system integration testing and thus qualifies as an important element in improving the integration testing process. Â© Springer International Publishing AG 2016.",
    "published_in": "PROFES 2016: Product-Focused Software Process Improvement",
    "publisher": "Springer",
    "source": "Query",
    "doi": "https://doi.org/10.1007/978-3-319-49094-6_17",
    "date": "2016-11-06",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Embedded systems\nNL test cases",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Train control managment subsystem (4578 TCs)",
    "ind_partner": "Bombardier Transportation (Sweden)",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "N/A",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "the paper evaluates the ROI for the TCP approach proposed in row 42; here they build a cost model and compare the costs of using TCP or not. Note that the TCP uses dependencies among tests that are identifies manually",
    "metrics": "Return on investment",
    "open_challenges": "- do study on real data and not on simulation\n-consider other criteria for TCP"
  },
  {
    "year": 2017,
    "authors": "Ramler, Rudolf; Salomon, Christian; Buchgeher, Georg; Lusser, Michael",
    "title": "Tool support for change-based regression testing: An industry experience report",
    "bibtex": "ramler_tool_2017",
    "abstract": "Changes may cause unexpected side effects and inconsistencies. Regression testing is the process of re-testing a software system after changes have been made to ensure that the new version of the system has retained the capabilities of the old version and that no new defects have been introduced. Regression testing is an essential activity, but it is also time-consuming and costly. Thus, regression testing should concentrate on those parts of the system that have been modified or which are affected by changes. Regression test selection has been proposed over three decades ago and, since then, it has been frequently in the focus of empirical studies. However, regression test selection is still not widely adopted in practice. Together with the test team of an industrial software company we have developed a tool-based approach that assists software testers in selecting regression test cases based on change information and test coverage data. This paper describes the main usage scenario of the approach, illustrates the implemented solution, and reports on its evaluation in a large industry project. The evaluation showed that the tool support reduces the time required for compiling regression test suites and fosters an accurate selection of regression test cases. The paper concludes with our lessons learned from implementing the tool support in a real-world setting. Â© Springer International Publishing AG 2017.",
    "published_in": "SWQD 2017: Software Quality. Complexity and Challenges of Software Engineering in Emerging Technologies",
    "publisher": "Springer",
    "source": "Query",
    "doi": "https://doi.org/10.1007/978-3-319-49421-0_10",
    "date": "2016-11-12",
    "categories": "Case study",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Undisclosed software system at Omicron (over 5000 TCs)",
    "ind_partner": "OMICRON Electronics GmbH (Austria)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "change-based, coverage-based, graph walk",
    "metrics": "testing time",
    "open_challenges": "lack of details provided by version control; redundant test cases; polluted coverage footprints; creating and maintaining coverage"
  },
  {
    "year": 2016,
    "authors": "Strandberg, Per Erik; Sundmark, Daniel; Afzal, Wasif; Ostrand, Thomas J.; Weyuker, Elaine J.",
    "title": "Experience Report: Automated System Level Regression Test Prioritization Using Multiple Factors",
    "bibtex": "strandberg_experience_2016",
    "abstract": "We propose a new method of determining an effective ordering of regression test cases, and describe its implementation as an automated tool called SuiteBuilder developed by Westermo Research and Development AB. The tool generates an efficient order to run the cases in an existing test suite by using expected or observed test duration and combining priorities of multiple factors associated with test cases, including previous fault detection success, interval since last executed, and modifications to the code tested. The method and tool were developed to address problems in the traditional process of regression testing, such as lack of time to run a complete regression suite, failure to detect bugs in time, and tests that are repeatedly omitted. The tool has been integrated into the existing nightly test framework for Westermo software that runs on large-scale data communication systems. In experimental evaluation of the tool, we found significant improvement in regression testing results. The re-ordered test suites finish within the available time, the majority of fault-detecting test cases are located in the first third of the suite, no important test case is omitted, and the necessity for manual work on the suites is greatly reduced. Â© 2016 IEEE.",
    "published_in": "2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ISSRE.2016.23",
    "date": "2016-12-08",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Embedded systems running Westermo's operating system  (unknown scale)",
    "ind_partner": "Westermo Research and Development (Sweden)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "History-based, change-based",
    "metrics": "Testing time; fault detection rate",
    "open_challenges": "Different kinds of suites and prioritizers; apply approach to minimization"
  },
  {
    "year": 2016,
    "authors": "Marijan, Dusica; Liaaen, Marius",
    "title": "Effect of time window on the performance of continuous regression testing",
    "bibtex": "marijan_effect_2016",
    "abstract": "Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",
    "published_in": "2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
    "publisher": "IEEE",
    "source": "backward",
    "doi": "https://doi.org/10.1109/ICSME.2016.77",
    "date": "2017-01-16",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Cisco videoconferencing system (460 TCs)",
    "ind_partner": "Cisco (Norway)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "History-based, fault detection-based",
    "metrics": "fault detection capability, APFD",
    "open_challenges": "Utilize comprehensive cost-benefit measures, experiments in other industry domains"
  },
  {
    "year": 2017,
    "authors": "Gotlieb, Arnaud; Marijan, Dusica",
    "title": "Using global constraints to automate regression testing",
    "bibtex": "gotlieb_using_2017",
    "abstract": "Communicating or autonomous systems rely on high-quality software-based components. that must be thoroughly verified before they are released and deployed in operational settings. Regression testing is a crucial verification process that compares any new release of a software-based component against its previous versions, by executing available test cases. However, limited testing time makes selection of test cases in regression testing challenging, and some selection criteria must be respected. Validation engineers usually address this problem, coined as test suite reduction (TSR), through manual analysis or by using approximation techniques. In this paper, we address the TSR problem with sound artificial intelligence techniques such as constraint programming (CP) and global constraints. By using distinct cost-value-aggregating criteria, we propose several constraint-optimization models to find a subset of test cases that cover all the test requirements and optimize the overall cost of selected test cases. Our contribution includes reuse of existing preprocessing rules to simplify the problem before solving it and the design of structure-aware heuristics that take into account the notion of the costs associated with test cases. The work presented in this paper has been motivated by an industrial application in the communication domain. Our overall goal is to develop a constraint-based approach of test suite reduction that can be deployed to test a complete product line of conferencing systems in continuous delivery mode. By implementing this approach in a software prototype tool and experimentally evaluating it on both randomly generated and industrial instances, we hope to foster a quick adoption of the technology. Copyright Â© 2017, Association for the Advancement of Artificial Intelligence. All rights reserved.",
    "published_in": "AI Magazine",
    "publisher": "AAAI",
    "source": "Query",
    "doi": "https://doi.org/10.1609/aimag.v38i1.2714",
    "date": "2017-03-31",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "",
    "tsr": "X",
    "tsa": "",
    "context": "Communication industry, high-configurable systems (but not specifically problems for PL)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Cisco videoconferencing system (377 TCs)",
    "ind_partner": "Cisco (Norway)",
    "ind_author": "FALSE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "MAYBE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Optimized approach to Constraint-based test suite reduction and its evalutation in comparison with SOTA",
    "metrics": "CPU time of solving\nTest suite size reduction",
    "open_challenges": "- improve preprocessing  heuristics for improving efficiency\n- increase usability to facilitate industrial adoption"
  },
  {
    "year": 2017,
    "authors": "Bach, Thomas; Andrzejak, Artur; Pannemans, Ralf",
    "title": "Coverage-Based Reduction of Test Execution Time: Lessons from a Very Large Industrial Project",
    "bibtex": "bach_coverage-based_2017",
    "abstract": "There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects. Â© 2017 IEEE.",
    "published_in": "2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSTW.2017.6",
    "date": "2017-04-17",
    "categories": "Case study, orchestration technique",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Database management system",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Database management system (19472 TCs)",
    "ind_partner": "SAP SE (Germany)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Greedy (algo)\n\nCoverage-based and overlap-aware (info)",
    "metrics": "Coverage",
    "open_challenges": "Flaky tests, experiment with other over-lap aware heuristics."
  },
  {
    "year": 2017,
    "authors": "Vasic, Marko; Parvez, Zuhair; Milicevic, Aleksandar; Gligoric, Milos",
    "title": "File-Level vs. Module-Level Regression Test Selection for .NET",
    "bibtex": "vasic_file-level_2017",
    "abstract": "Regression testing is used to check the correctness of evolving software. With the adoption of Agile development methodology, the number of tests and software revisions has dramatically increased, and hence has the cost of regression testing. Researchers proposed regression test selection (RTS) techniques that optimize regression testing by skipping tests that are not impacted by recent program changes. Ekstazi is one such state-of-the art technique; Ekstazi is implemented for the Java programming language and has been adopted by several companies and open-source projects.\nWe report on our experience implementing and evaluating Ekstazi#, an Ekstazi-like tool for .NET. We describe the key challenges of bringing the Ekstazi idea to the .NET platform. We evaluate Ekstazi# on 11 open-source projects, as well as an internal Microsoft project substantially larger than each of the open-source projects. Finally, we compare Ekstazi# to an incremental build system (also developed at Microsoft), which, out of the box, provides module-level dependency tracking and skipping tasks (including test execution) whenever dependencies of a task do not change between the current and the last successful build. Ekstazi# on average reduced regression testing time by 43.70% for the open-source projects and by 65.26% for the Microsoft project (the latter is in addition to the savings provided by incremental builds).",
    "published_in": "ESEC/FSE 2017: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "backward",
    "doi": "https://doi.org/10.1145/3106237.3117763",
    "date": "2017-08-21",
    "categories": "",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "C#/.NET variant",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Open-source C# projects (up to 138 TCs) plus one Microsoft project (37 TCs)",
    "ind_partner": "Microsoft (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "https://github.com/marko-vasic/ekstaziSharp",
    "included": "TRUE",
    "approach": "Change-based",
    "metrics": "selection count, execution time, cumulative time",
    "open_challenges": "n/a"
  },
  {
    "year": 2017,
    "authors": "Celik, Ahmet; Vasic, Marko; Milicevic, Aleksandar; Gligoric, Milos",
    "title": "Regression test selection across JVM boundaries",
    "bibtex": "celik_regression_2017",
    "abstract": "Modern software development processes recommend that changes be integrated into the main development line of a project multiple times a day. Before a new revision may be integrated, developers practice regression testing to ensure that the latest changes do not break any previously established functionality. The cost of regression testing is high, due to an increase in the number of revisions that are introduced per day, as well as the number of tests developers write per revision. Regression test selection (RTS) optimizes regression testing by skipping tests that are not affected by recent project changes. Existing dynamic RTS techniques support only projects written in a single programming language, which is unfortunate knowing that an open-source project is on average written in several programming languages. We present the first dynamic RTS technique that does not stop at predefined language boundaries. Our technique dynamically detects, at the operating system level, all file artifacts a test depends on. Our technique is, hence, oblivious to the specific means the test uses to actually access the files: be it through spawning a new process, invoking a system call, invoking a library written in a different language, invoking a library that spawns a process which makes a system call, etc. We also provide a set of extension points which allow for a smooth integration with testing frameworks and build systems. We implemented our technique in a tool called RTSLinux as a loadable Linux kernel module and evaluated it on 21 Java projects that escape the JVM by spawning new processes or invoking native code, totaling 2, 050, 791 lines of code. Our results show that RTSLinux, on average, skips 74.17% of tests and saves 52.83% of test execution time compared to executing all tests. Â© 2017 Association for Computing Machinery.",
    "published_in": "ESEC/FSE 2017: Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/3106237.3106297",
    "date": "2017-08-21",
    "categories": "Orchestration technique/Context-specific",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Multi-language projects",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source Java projects (up to 431 TCs)",
    "ind_partner": "Microsoft (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Change-based",
    "metrics": "testing time, reduction rate, dependency discovery, overhead",
    "open_challenges": "Incorporate approach with other programming languages."
  },
  {
    "year": 2018,
    "authors": "Ouriques, JoÃ£o Felipe S.; Cartaxo, Emanuela G.; Machado, PatrÃ­cia D.L.",
    "title": "Test case prioritization techniques for model-based testing: a replicated study",
    "bibtex": "ouriques_test_2018",
    "abstract": "Recently, several test case prioritization (TCP) techniques have been proposed to order test cases for achieving a goal during test execution, particularly, revealing faults sooner. In the model-based testing (MBT) context, such techniques are usually based on heuristics related to structural elements of the model and derived test cases. In this sense, techniques' performance may vary due to a number of factors. While empirical studies comparing the performance of TCP techniques have already been presented in literature, there is still little knowledge, particularly in the MBT context, about which factors may influence the outcomes suggested by a TCP technique. In a previous family of empirical studies focusing on labeled transition systems, we identified that the model layout, i.e., amount of branches, joins, and loops in the model, alone may have little influence on the effectiveness of TCP techniques investigated, whereas characteristics of test cases that actually fail definitely influences this aspect. However, we considered only synthetic artifacts in the study, which reduced the ability of representing properly the reality. In this paper, we present a replication of one of these studies, now with a larger and more representative selection of techniques and considering test suites from industrial systems as experimental objects. Our objective is to find out whether the results remain while increasing the validity in comparison to the original study. Results reinforce that there is no best performer among the investigated techniques and characteristics of test cases that fail represent an important factor, although adaptive random-based techniques are less affected by it.",
    "published_in": "Software Quality Journal",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1007/s11219-017-9398-y",
    "date": "2018-01-23",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Undisclosed systems at Ingenico (up to 48 TCs)",
    "ind_partner": "Ingenico (Brazil)",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "coverage-based, similarity-based, path complexity-based, model-based",
    "metrics": "APFD",
    "open_challenges": "discover influence of other factors on effectiveness; collect supplementary data to improve techniques."
  },
  {
    "year": 2017,
    "authors": "Kwon, Jung-Hyun; Ko, In-Young",
    "title": "Cost-effective regression testing using bloom filters in continuous integration development environments",
    "bibtex": "kwon_cost-effective_2017",
    "abstract": "Regression testing in continuous integration development environments \nmust be cost-effective and should provide fast feedback on test suite \nfailures to the developers. In order to provide faster feedback on \nfailures to developers while using computing resources efficiently, two \ntypes of regression testing techniques have been developed: Regression \nTesting Selection (RTS) and Test Case Prioritization (TCP). One of the \nfactors that reduces the effectiveness of the RTS and TCP techniques is \nthe inclusion of test suites that fail only once over a period. We \npropose an approach based on Bloom filtering to exclude such test suites\n during the RTS process, and to assign such test suites with a lower \npriority during the TCP process. We experimentally evaluate our approach\n using a Google dataset, and demonstrate that cost-effectiveness of the \nproposed RTS and TCP techniques outperforms the state-of-the-art \ntechniques.",
    "published_in": "2017 24th Asia-Pacific Software Engineering Conference (APSEC)",
    "publisher": "IEEE",
    "source": "backward",
    "doi": "https://doi.org/10.1109/APSEC.2017.22",
    "date": "2018-03-05",
    "categories": "Technique analysis",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Selection and prioritization of Test Suites to be executed in a CI environment. The proposed approach is inspired by the work from Elbaum and Penix on Google (Google CI dataset).",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "The Google dataset of testing results\" from Elbaum and Penix",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "A bloom ï¬lter is used together with the window-based method proposed by Elbaum and Penix with the purpose of removing test suites that failed only once over a period. The selected test suites receive a weight that is later used for prioritizing the TS execution (so some notion of orchestration could possibly be considered).",
    "metrics": "Fault detection capability, Efftime (efficiency), and Precision.",
    "open_challenges": "Improve the approach to reduce the false-positive cases."
  },
  {
    "year": 2018,
    "authors": "Garousi, Vahid; Ãzkan, Ramazan; Betin-Can, Aysu",
    "title": "Multi-objective regression test selection in practice: An empirical study in the defense software industry",
    "bibtex": "garousi_multi-objective_2018",
    "abstract": "Context: Executing an entire regression test-suite after every code change is often costly in large software projects. To cope with this challenge, researchers have proposed various regression test-selection techniques. Objective: This paper was motivated by a real industrial need to improve regression-testing practices in the context of a safety-critical industrial software in the defence domain in Turkey. To address our objective, we set up and conducted an âaction-researchâ collaborative project between industry and academia. Method: After a careful literature review, we selected a conceptual multi-objective regression-test selection framework (called MORTO) and adopted it to our industrial context by developing a custom-built genetic algorithm (GA) based on that conceptual framework. GA is able to provide full coverage of the affected (changed) requirements while considering multiple cost and benefit factors of regression testing. e.g., minimizing the number of test cases, and maximizing cumulative number of detected faults by each test suite. Results: The empirical results of applying the approach on the Software Under Test (SUT) demonstrate that this approach yields a more efficient test suite (in terms of costs and benefits) compared to the old (manual) test-selection approach, used in the company, and another applicable approach chosen from the literature. With this new approach, regression selection process in the project under study is not ad-hoc anymore. Furthermore, we have been able to eliminate the subjectivity of regression testing and its dependency on expert opinions. Conclusion: Since the proposed approach has been beneficial in saving the costs of regression testing, it is currently in active use in the company. We believe that other practitioners can apply our approach in their regression-testing contexts too, when applicable. Furthermore, this paper contributes to the body of evidence in regression testing by offering a success story of successful implementation and application of multi-objective regression testing in practice. Â© 2018",
    "published_in": "Information and Software Technology",
    "publisher": "Elsevier",
    "source": "Query",
    "doi": "https://doi.org/10.1016/j.infsof.2018.06.007",
    "date": "2018-06-30",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Defense software",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Defense software system (up to 3588 TCs)",
    "ind_partner": "Government organization (Turkey)",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "TRUE",
    "suppl_url": "https://zenodo.org/record/1149058#.Xto4gfKxVTY",
    "included": "TRUE",
    "approach": "multi-objective genetic algorithm",
    "metrics": "Test execution time, third-party costs, system setup costs, technical resources cost, verification cost",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Haghighatkhah, Alireza; MÃ¤ntylÃ¤,  Mika; Oivo, Markku; Kuvaja, Pasi",
    "title": "Test prioritization in continuous integration environments",
    "bibtex": "haghighatkhah_test_2018",
    "abstract": "Two heuristics namely diversity-based (DBTP) and history-based test prioritization (HBTP) have been separately proposed in the literature. Yet, their combination has not been widely studied in continuous integration (CI) environments. The objective of this study is to catch regression faults earlier, allowing developers to integrate and verify their changes more frequently and continuously. To achieve this, we investigated six open-source projects, each of which included several builds over a large time period. Findings indicate that previous failure knowledge seems to have strong predictive power in CI environments and can be used to effectively prioritize tests. HBTP does not necessarily need to have large data, and its effectiveness improves to a certain degree with larger history interval. DBTP can be used effectively during the early stages, when no historical data is available, and also combined with HBTP to improve its effectiveness. Among the investigated techniques, we found that history-based diversity using NCD Multiset is superior in terms of effectiveness but comes with relatively higher overhead in terms of method execution time. Test prioritization in CI environments can be effectively performed with negligible investment using previous failure knowledge, and its effectiveness can be further improved by considering dissimilarities among the tests.",
    "published_in": "Journal of Systems and Software",
    "publisher": "Elsevier",
    "source": "backward",
    "doi": "https://doi.org/10.1016/j.jss.2018.08.061",
    "date": "2018-08-31",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source Java projects (up to 411 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "combination of diversity-based and history-based",
    "metrics": "APFD, execution time",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Lingming Zhang",
    "title": "Hybrid regression test selection",
    "bibtex": "zhang_hybrid_2018",
    "abstract": "Regression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time.",
    "published_in": "ICSE '18: Proceedings of the 40th International Conference on Software Engineering",
    "publisher": "ACM",
    "source": "backward",
    "doi": "https://doi.org/10.1145/3180155.3180198",
    "date": "2018-09-03",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Large scale Java (as Ekstazi)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source Java projects (up to 16069 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "FALSE",
    "suppl_url": "http://hyrts.org/",
    "included": "TRUE",
    "approach": "A dynamic approach to TCS that combines file-level (like Ekstazi) with method level selection, under different variations, to improve precision while keeping efficiency",
    "metrics": "- Selected test ratio\n\n- E2E testing time",
    "open_challenges": "Apply the idea of hybris also to static RTS techniques"
  },
  {
    "year": 2018,
    "authors": "Miranda, Breno; Cruciani, Emilio; Verdecchia, Roberto; Bertolino, Antonia",
    "title": "FAST Approaches to Scalable Similarity-Based Test Case Prioritization",
    "bibtex": "miranda_fast_2018",
    "abstract": "Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in e ciency with no significant impact on e ectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.",
    "published_in": "2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/3180155.3180210",
    "date": "2018-09-03",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source C and Java projects (up to 670 TCs) + synthetic data for scalability measure",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "FALSE",
    "suppl_url": "https://github.com/icse18-fast/FAST",
    "included": "TRUE",
    "approach": "similarity-based",
    "metrics": "Prioritization time, total testing time, scalability",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Chen, Junjie; Lou, Yiling; Zhang, Lingming Lu; Zhou, Jianyi; Wang, Xiaoleng; Hao, Dan; Zhang, Lingming Lu",
    "title": "Optimizing Test Prioritization via Test Distribution Analysis",
    "bibtex": "chen_optimizing_2018",
    "abstract": "Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice. To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming stateof- the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks fromthe testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.",
    "published_in": "ESEC/FSE 2018: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/3236024.3236053",
    "date": "2018-10-26",
    "categories": "Technique analysis",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "General",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Open-source Java projects (up to 9691 TCs) plus undisclosed projects from Baidu (up to 4139 TCs)",
    "ind_partner": "Baidu (China)",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "TRUE",
    "suppl_url": "https://github.com/JunjieChen/PTP",
    "included": "TRUE",
    "approach": "Analyzes test coverage to predict the optimal TCP technique for a given project.",
    "metrics": "APFDc, FT, LT, AT",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Celik, Ahmet; Lee, Young Chul; Gligoric, Milos",
    "title": "Regression Test Selection for TizenRT",
    "bibtex": "celik_regression_2018",
    "abstract": "Regression testing - running tests after code modifications - is widely practiced in industry, including at Samsung. Regression Test Selection (RTS) optimizes regression testing by skipping tests that are not affected by recent code changes. Recent work has developed robust RTS tools, which mostly target managed languages, e.g., Java and C#, and thus are not applicable to large C projects, e.g., TizenRT, a lightweight RTOS-based platform. We present Selfection, an RTS tool for projects written in C; we discuss the key challenges to develop Selfection and our design decisions. Selfection uses the objdump and readelf tools to statically build a dependency graph of functions from binaries and detect modified code elements. We integrated Selfection in TizenRT and evaluated its benefits if tests are run in an emulator and on a supported hardware platform (ARTIK 053). We used the latest 150 revisions of TizenRT available on GitHub. We measured the benefits of Selfection as the reduction in the number of tests and reduction in test execution time over running all tests at each revision (i.e., RetestAll). Our results show that Selfection can reduce, on average, the number of tests to 4.95% and end-to-end execution time to 7.04% when tests are executed in the emulator, and to 5.74% and 26.82% when tests are executed on the actual hardware. Our results also show that the time taken to maintain the dependency graph and detect modified functions is negligible.",
    "published_in": "ESEC/FSE 2018: Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/3236024.3275527",
    "date": "2018-10-26",
    "categories": "Orchestration technique/Context-specific",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "general (C)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "TizenRT (877 TCs)",
    "ind_partner": "Samsung (South Korea)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "MAYBE",
    "suppl_url": "https://github.com/ahmet-celik/Selfection",
    "included": "TRUE",
    "approach": "Change-based, graph-based",
    "metrics": "Selection rate, testing time, build time",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Zhu, Yuecai; Shihab, Emad; Rigby, Peter C.",
    "title": "Test re-prioritization in continuous testing environments",
    "bibtex": "zhu_test_2018",
    "abstract": "New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests based on their co-failure probability distributions. We named this approach CODYNAQ and in particular, we propose three prioritization variants called CODYNAQSINGLE, CODYNAQDOUBLE and CODYNAQFLEXI. We evaluate our approach on two data sets, CHROME and Google testing data. We find that our co-failure dynamic re-prioritization approach, CODYNAQ, outperforms the default order, FIFOBASELINE, finding the first failure and all failures for a change request by 31% and 62% faster, respectively. CODYNAQ also outperforms GOOGLETCP by finding the first failure 27% faster and all failures 62% faster. Â© 2018 IEEE.",
    "published_in": "2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSME.2018.00016",
    "date": "2018-11-12",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Continuous integration",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "The Google dataset of testing results from Elbaum and Penix plus data scraped from the Chromium project (up to 149 TCs per minute)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Co-failure distributions",
    "metrics": "FirstFail, AllFail, delayed failures (goal: speed up detection of first/all failure)",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Maral Azizi; Hyunsook Do",
    "title": "Retest: A cost effective test case selection technique for modern software development",
    "bibtex": "azizi_retest_2018",
    "abstract": "Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modified version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel language-independent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efficient by selecting a far smaller subset of tests compared to the existing techniques.",
    "published_in": "2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE)",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/ISSRE.2018.00025",
    "date": "2018-11-19",
    "categories": "",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Language-agnostic",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Two open-source .net and C# programs (up to 628 TCs) and two Java subjects from Defects4J (up to 393 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Graph-based",
    "metrics": "selection count, fault detection rate",
    "open_challenges": "automate parameter selection, apply approach on TCP and TSR."
  },
  {
    "year": 2019,
    "authors": "Guo, Bo; Kwon, Young-Woo; Song, Myoungkyu",
    "title": "Decomposing Composite Changes for Code Review and Regression Test Selection in Evolving Software",
    "bibtex": "guo_decomposing_2019",
    "abstract": "Inspecting and testing code changes typically require a significant amount of developer effort. As a system evolves, developers often create composite changes by mixing multiple development issues, as opposed to addressing one independent issue â an atomic change. Inspecting composite changes often becomes time-consuming and error-prone. To test unrelated edits on composite changes, rerunning all regression tests may require excessive time. To address the problem, we present an interactive technique for change decomposition to support code reviews and regression test selection, called ChgCutter. When a developer specifies code change within a diff patch, ChgCutter partitions composite changes into a set of related atomic changes, which is more cohesive and self-contained regarding the issue being addressed. For composite change inspection, it generates an intermediate program version that only includes a related change subset using program dependence relationships. For cost reduction during regression testing, it safely selects only affected tests responsible for changes to an intermediate version. In the evaluation, we apply ChgCutter to 28 composite changes in four open source projects. ChgCutter partitions these changes with 95.7% accuracy, while selecting affected tests with 89.0% accuracy. We conduct a user study with professional software engineers at PayPal and find that ChgCutter is helpful in understanding and validating composite changes, scaling to industry projects. Â© 2019, Springer Science+Business Media, LLC & Science Press, China.",
    "published_in": "Journal of Computer Science and Technology",
    "publisher": "Springer",
    "source": "Query",
    "doi": "https://doi.org/10.1007/s11390-019-1917-9",
    "date": "2019-03-22",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "General (Java)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Generated test suite based on four open-source Java projects (totaling 3456 TCs)",
    "ind_partner": "PayPal (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "TRUE",
    "put_practice": "TRUE",
    "suppl_url": "https://sites.google.com/unomaha.edu/interactively-partitioning",
    "included": "TRUE",
    "approach": "Change-based",
    "metrics": "Precision/Recall",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Zhong, Hua; Zhang, Lingming; Khurshid, Sarfraz",
    "title": "TestSage: Regression test selection for large-scale Web service testing",
    "bibtex": "zhong_testsage:_2019",
    "abstract": "Regression testing is an important but expensive activity in software development. Among various types of tests, web service tests are usually one of the most expensive (due to network communications) but widely adopted types of tests in commercial software development. Regression test selection (RTS) aims to reduce the number of tests which need to be retested by only running tests that are affected by code changes. Although a large number of RTS techniques have been proposed in the past few decades, these techniques have not been adopted on large-scale web service testing. This is because most existing RTS techniques either require direct code dependency between tests and code under test or cannot be applied on large scale systems with enough efficiency. In this paper, we present a novel RTS technique, TestSage, that performs RTS for web service tests on large scale commercial software. With a small overhead, TestSage is able to collect fine grained (function level) dependency between test and service under test that do not directly depend on each other. TestSage has also been successfully applied to large complex systems with over a million functions. We conducted experiments of TestSage on a large scale backend service at Google. Experimental results show that TestSage reduces 34% of testing time when running all AEC (Analysis, Execution and Collection) phases, 50% of testing time while running without collection phase. TestSage has been integrated with internal testing framework at Google and runs day-to-day at the company.",
    "published_in": "2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/ICST.2019.00052",
    "date": "2019-06-06",
    "categories": "Technique analysis",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "TCS for Large-scale Web Service Testing",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Google Assistant (tens of thousands of tests)",
    "ind_partner": "Google (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Trace-based",
    "metrics": "Selection ratio, testing time reduction",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Fu, Ben; Misailovic, Sasa; Gligoric, Milos",
    "title": "Resurgence of Regression Test Selection for C++",
    "bibtex": "fu_resurgence_2019",
    "abstract": "Regression testing - running available tests after each project change - is widely practiced in industry. Despite its widespread use and importance, regression testing is a costly activity. Regression test selection (RTS) optimizes regression testing by selecting only tests affected by project changes. RTS has been extensively studied and several tools have been deployed in large projects. However, work on RTS over the last decade has mostly focused on languages with abstract computing machines (e.g., JVM). Meanwhile development practices (e.g., frequency of commits, testing frameworks, compilers) in C++ projects have dramatically changed and the way we should design and implement RTS tools and the benefits of those tools is unknown. We present a design and implementation of an RTS technique, dubbed RTS++, that targets projects written in C++, which compile to LLVM IR and use the Google Test testing framework. RTS++ uses static analysis of a function call graph to select tests. RTS++ integrates with many existing build systems, including AutoMake, CMake, and Make. We evaluated RTS++ on 11 large open-source projects, totaling 3,811,916 lines of code. To the best of our knowledge, this is the largest evaluation of an RTS technique for C++. We measured the benefits of RTS++ compared to running all available tests (i.e., retest-all). Our results show that RTS++ reduces the number of executed tests and end-to-end testing time by 88% and 61% on average.",
    "published_in": "2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/ICST.2019.00039",
    "date": "2019-06-06",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "C++",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source C++ projects (up to 673 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Change-based",
    "metrics": "selection count, end-to-end time, cumulative time",
    "open_challenges": "n/a"
  },
  {
    "year": 2019,
    "authors": "Eda, Ravi; Do, Hyunsook",
    "title": "An efficient regression testing approach for PHP Web applications using test selection and reusable constraints",
    "bibtex": "eda_efficient_2019",
    "abstract": "Web applications undergo frequent changes. These changes can be due to the addition of new features or the modification of existing features to support customer requests or to patch faults in the system. Given that Web applications have a large surface area subject to attack, changes often include security fixes either in response to malicious attacks or to forestall such attacks. Effective regression testing should ensure that any change does not disable existing features or compromise security. Executing the entire regression test suite takes time and consumes many resources. One approach is to focus regression test efforts only on code paths that were modified in the new version. Such code paths can be identified using tools such as PHP Analysis and Regression Testing Engine (PARTE). In this paper, we extend this approach to test selection where a subset of existing tests that cover the modified code paths can be detected. To further reduce the amount of regression testing needed, we used PARTEâs reusable constraint value information to identify tests that can be reused against the new version without having to modify the input test values. We performed an empirical study to determine whether test selection data combined with reusable constraint values would further improve the turnaround time for regression tests. Results from the experiment conducted on four Hypertext Preprocessor (PHP) web applications demonstrate that this approach is effective in reducing the cost of regression testing of frequently patched Web applications.",
    "published_in": "Software Quality Journal",
    "publisher": "Springer",
    "source": "forward",
    "doi": "https://doi.org/10.1007/s11219-019-09449-2",
    "date": "2019-06-11",
    "categories": "",
    "tcp": "",
    "tcs": "X",
    "tsr": "X",
    "tsa": "",
    "context": "PHP web applications",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source PHP projects (up to 124 TCs)",
    "ind_partner": "",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Change-based",
    "metrics": "Selected tests, obsolete tests, reusable inputs",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Yu, Zhe; Fahid, Fahmid; Menzies, Tim; Rothermel, Gregg; Patrick, Kyle; Cherian, Snehit",
    "title": "TERMINATOR: better automated UI test case prioritization",
    "bibtex": "yu_terminator_2019",
    "abstract": "Automated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process.\nTo mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is \"black box\" in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 \"black box\" test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead.",
    "published_in": "ESEC/FSE 2019: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "backward",
    "doi": "https://doi.org/10.1145/3338906.3340448",
    "date": "2019-08-12",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "UI testing, web app",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Dataset from LexisNexis (2661 TCs)",
    "ind_partner": "LexisNexis (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "https://github.com/ai-se/Data-for-automated-UI-testing-from-LexisNexis",
    "included": "TRUE",
    "approach": "Coverage, history, cost, description, feedback",
    "metrics": "Coverage, APFD, APFDc, failure detection rate",
    "open_challenges": "identify flaky tests, identify fault location, apply approach to different TCP problems"
  },
  {
    "year": 2019,
    "authors": "Correia, Daniel; Abreu, Rui; Santos, Pedro; Nadkarni, Joo",
    "title": "MOTSD: A multi-objective test selection tool using test suite diagnosability",
    "bibtex": "correia_motsd_2019",
    "abstract": "Performing regression testing on large software systems becomes unfeasible as it takes too long to run all the test cases every time a change is made. The main motivation of this work was to provide a faster and earlier feedback loop to the developers at OutSystems when a change is made. The developed tool, MOTSD, implements a multi-objective test selection approach in a C# code base using a test suite diagnosability metric and historical metrics as objectives and it is powered by a particle swarm optimization algorithm. We present implementation challenges, current experimental results and limitations of the tool when applied in an industrial context. Screencast demo link: \\textlessa\\textgreaterhttps://www.youtube.com/watch?v=CYMfQTUu2BE\\textless/a\\textgreater Â© 2019 ACM.",
    "published_in": "ESEC/FSE 2019: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/3338906.3341187",
    "date": "2019-08-12",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "General (C#)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "OutSystems codebase in C# (over 8500 TCs)",
    "ind_partner": "OutSystems (Portugal)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "TRUE",
    "suppl_url": "http://github.com/danielcorreia96/MOTSD",
    "included": "TRUE",
    "approach": "multi-objective, particle swarm optimization",
    "metrics": "Diagnosability",
    "open_challenges": "use data other than coverage (e.g. dependency graph); questions regarding evaluation; common benchmark for this type of tool."
  },
  {
    "year": 2019,
    "authors": "Machalica, Mateusz; Samylkin, Alex; Porth, Meredith; Chandra, Satish",
    "title": "Predictive Test Selection",
    "bibtex": "machalica_predictive_2018",
    "abstract": "Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95% of individual test failures and over 99.9% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.",
    "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSE-SEIP.2019.00018",
    "date": "2019-08-19",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Facebook mobile app repository (undisclosed scale)",
    "ind_partner": "Facebook (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "TRUE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "machine learning-based, history-based",
    "metrics": "Recall, selection rate",
    "open_challenges": "Incorporate more features into the ML model; more sophisticated ML algorithms and models; correlation between tests that are impacted by the same code."
  },
  {
    "year": 2019,
    "authors": "Najafi, Armin; Shang, Weiyi; Rigby, Peter C.",
    "title": "Improving Test Effectiveness Using Test Executions History: An Industrial Experience Report",
    "bibtex": "najafi_improving_2019",
    "abstract": "The cost of software testing has become a burden for software companies in the era of rapid release and continuous integration. Our industrial collaborator Ericsson also faces the challenges of expensive testing processes which are typically part of a complex and specialized testing environment. In order to assist Ericsson with improving the test effectiveness of one of its large subsystems, we adopt test selection and prioritization approaches based on test execution history from prior research. By adopting and simulating those approaches on six months of testing data from our subject system, we confirm the existence of valuable information in the test execution history. In particular, the association between test failures provide the most value to the test selection and prioritization processes. More importantly, during this exercise, we encountered various challenges that are unseen or undiscussed in prior research. We document the challenges, our solutions and the lessons learned as an experience report. Our experiences can be valuable for other software testing practitioners and researchers who would like to adopt existing test effectiveness improvement approaches into their work environment.",
    "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSE-SEIP.2019.00031",
    "date": "2019-08-19",
    "categories": "Ochestration technique",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "General large-scale software",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Undisclosed large-scale system at Ericsson",
    "ind_partner": "Ericsson (Canada)",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "history-based",
    "metrics": "Failure frequency, failure association, cost, execution time, missed failures, execution cost",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Leong, Claire; Singh, Abhayendra; Papadakis, Mike; Le Traon, Yves; Micco, John; Traon, Yves Le; Micco, John",
    "title": "Assessing Transition-Based Test Selection Algorithms at Google",
    "bibtex": "leong_assessing_2019",
    "abstract": "Continuous Integration traditionally relies on testing every code commit with all impacted tests. This practice requires considerable computational resources, which at Google scale, results in delayed test results and high operational costs. To deal with this issue and provide fast feedback, test selection and prioritization methods aim to execute the tests which are most likely to reveal changes in test results as soon as possible. In this paper we present a simulation framework to support the study and evaluation, with real data, of such techniques. We propose a test selection algorithm evaluation method, and detail several practical requirements which are often ignored by related work, such as the detection of transitions, the collection and analysis of data, and the handling of flaky tests. Based on this framework, we design an experiment evaluating five potential regression test selection algorithms, based on simple heuristics and inspired by previous research, though the evaluation technique is applicable to any number of algorithms for future experiments. Our results show that algorithms based on the recent (transition) execution history do not perform as well as expected (given the previously reported results) and that the test selection problem remains largely open. We found that the best performing algorithms are based on the number of times a test has been triggered and the number of distinct authors committing code that triggers particular tests. More research is needed in order to close the gap between the current approaches and the optimal solution.",
    "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSE-SEIP.2019.00019",
    "date": "2019-08-19",
    "categories": "Technique analysis",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Dataset from Google Test Automation Platform",
    "ind_partner": "Google (USA)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "transition-based (apparently a mix of history-based and change-based)",
    "metrics": "Transition count, affected count, author count",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Philip, Adithya Abraham; Bhagwan, Ranjita; Kumar, Rahul; Maddila, Chandra Sekhar; Nagppan, Nachiappan; Nagappan, Nachiappan",
    "title": "FastLane: Test Minimization for Rapidly Deployed Large-Scale Online Services",
    "bibtex": "philip_fastlane:_2019",
    "abstract": "Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases. This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%.",
    "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSE.2019.00054",
    "date": "2019-08-26",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "",
    "tsr": "X",
    "tsa": "",
    "context": "General",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Microsoft Office 365 (tens of thousands of TCs)",
    "ind_partner": "Microsoft (India)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "MAYBE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "machine learning-based",
    "metrics": "Precision/Recall, time saved",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Cruciani, Emilio; Miranda, Breno; Verdecchia, Roberto; Bertolino, Antonia",
    "title": "Scalable Approaches for Test Suite Reduction",
    "bibtex": "cruciani_scalable_2019",
    "abstract": "Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or specification models, that are not commonly available at large scale. We present a family of novel very efficient approaches for similaritybased test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for finding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input).We evaluate four approaches in a version that selects a fixed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some fixed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency. When applied to a suite of more than 500K real world test cases, the most efficient of the four approaches could select B test cases (for varying B values) in less than 10 seconds.",
    "published_in": "2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)",
    "publisher": "IEEE",
    "source": "Query",
    "doi": "https://doi.org/10.1109/ICSE.2019.00055",
    "date": "2019-08-26",
    "categories": "Orchestration technique",
    "tcp": "",
    "tcs": "",
    "tsr": "X",
    "tsa": "",
    "context": "General (Java)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "TRUE",
    "put_practice": "FALSE",
    "suppl_url": "https://zenodo.org/record/2550079",
    "included": "TRUE",
    "approach": "Similarity-based",
    "metrics": "Fault detection loss, test suite reduction, time",
    "open_challenges": ""
  },
  {
    "year": 2019,
    "authors": "Wu, Zhaolin; Yang, Yang; Li, Zheng; Zhao, Ruilian",
    "title": "A Time Window Based Reinforcement Learning Reward for Test Case Prioritization in Continuous Integration",
    "bibtex": "wu_time_2019",
    "abstract": "Continuous integration refers to the practice of merging the working copies of all developers into the mainline frequently. Regression testing for each mergence is characterized by continually changing test suite, limited execution time, and fast feedback, which demands new test optimization techniques. Reinforcement learning is introduced for test case prioritization to save computing resources in continuous integration environment, where a reasonable reward function is highly important for learning strategy, since the process of reinforcement learning is a reward-guided behavior. In this paper, APHFW, a novel reward function is proposed by using partial historical information of test cases effectively for fast feedback and cost reduction. The experiments are based on three open-source data sets, and the results show that the proposed reward function is more cost-effect than other reinforcement learning rewards in continuous integration environment.",
    "published_in": "Internetware '19: Proceedings of the 11th Asia-Pacific Symposium on Internetware",
    "publisher": "ACM",
    "source": "Query",
    "doi": "https://doi.org/10.1145/3361242.3361258",
    "date": "2019-10-28",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Continuous integration in large scale software",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "The Google dataset of testing results\" from Elbaum and Penix (5555 TCs) plus datasets from ABB Robotics (up to 2086 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "TCP by Reinforcement Learning, the paper experiments different windows on which the reward is calculated, in cmparison with Speiker (only recent observation) and He (full history)",
    "metrics": "NAPFD\nTime cost of prioritization function",
    "open_challenges": "use windows of dynamic size"
  },
  {
    "year": 2019,
    "authors": "Land, Kathrin; Neumann, Eva-Maria; Ziegltrum, Simon; Li, Huaxia; Vogel-Heuser, Birgit",
    "title": "An Industrial Evaluation of Test Prioritisation Criteria and Metrics",
    "bibtex": "land_industrial_2019",
    "abstract": "Automated production systems become more and more complex. This makes it\n increasingly difficult to keep track of performed changes and already \nexecuted test cases. This endangers the systems quality as the risk of \nmissing important test cases while planning the test execution is high, \nespecially for testers with little experience. To face this challenge, \ntesters should be supported by an automatic test prioritisation based on\n metrics in selecting the right test cases for the test execution. In \nindustry, many different test prioritisation criteria and strategies are\n used for this purpose. In an industrial interview, experts discussed \nand ranked prioritisation criteria that are currently used within the \nrespective companies. As a result, this paper presents the cactus \nprioritisation model, which graphically resembles the industrial ranking\n and weighting of the criteria. Based on the prioritisation cactus and \nits criteria, a simple prioritisation metric is introduced to determine \nthe utility of each test case regarding the system under test. The test \ncases are prioritised according to their descending utility. \nFurthermore, approaches and metrics to realise the different individual \nprioritisation criteria are proposed.",
    "published_in": "2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/SMC.2019.8914505",
    "date": "2019-11-28",
    "categories": "Case study",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "Mechatronics",
    "ind_motivation": "TRUE",
    "ind_evaluation": "N/A",
    "exp_subjects": "",
    "ind_partner": "Undisclosed industrial partner (Germany)",
    "ind_author": "FALSE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "n/a",
    "metrics": "n/a",
    "open_challenges": "When prioritizing, should the focus be entirely on the safety-critical tests, or should some other tests be sprinkled in-between?"
  },
  {
    "year": 2020,
    "authors": "Noemmer, Raphael; Haas, Roman",
    "title": "An Evaluation of Test Suite Minimization Techniques",
    "bibtex": "noemmer_evaluation_2020",
    "abstract": "As a software project evolves over time, the associated test suite usually grows with it. If test suites are not carefully maintained, this can easily result in massive test execution duration, reducing the benefits of regression testing because faults are found later in development or even after release. Test suite minimization aims to combat long running test suites by removing redundant test cases. Previous work mainly evaluates test suite minimization techniques based on comparably small projects, which are less practically relevant. In this paper, we compare four test suite minimization techniques by applying them to several open source software projects and evaluate the results. We find that the size and execution time of all the test suites can be reduced by over 70% on average. However, there is a substantial loss in fault detection capability of, on average, around 12.5%, restricting the applicability of this form of test suite minimization. Â© Springer Nature Switzerland AG 2020.",
    "published_in": "SWQD 2020: International Conference on Software Quality",
    "publisher": "Springer",
    "source": "Query",
    "doi": "https://doi.org/10.1007/978-3-030-35510-4_4",
    "date": "2019-12-09",
    "categories": "Technique analysis",
    "tcp": "",
    "tcs": "",
    "tsr": "X",
    "tsa": "",
    "context": "Investigates the applicability of TSR\ntechniques in practice",
    "ind_motivation": "FALSE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Open-source Java projects (up to 14770 TCs)",
    "ind_partner": "CQSE (Germany)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "basic greedy and the HGS\nalgorithm (guided by statement coverage only)",
    "metrics": "-test suite reduction achieved\n-Impact on fault detection capability\n-execution time of the reduced test suite (important finding: \"The reduction in number of tests appears to be a bad indicator for the reduction in execution time\")",
    "open_challenges": ""
  },
  {
    "year": 2020,
    "authors": "LÃ¼bke, Daniel",
    "title": "Selecting and Prioritizing Regression Test Suites by Production Usage Risk in Time-Constrained Environments",
    "bibtex": "lubke_selecting_2020",
    "abstract": "Regression Testing is an important quality assurance activity for combating unwanted side-effects, which might have been introduced in a new software release. Selecting and prioritizing regression test cases is a challenge in practice â especially in a world of ever increasing complex- ity, distribution, and size of the software solutions. Current approaches try to minimize the number of regression test cases by analyzing the change and the coverage of the tests with regards to this change. Our approach utilizes usage frequencies from the previous, productive soft- ware version in order to select or prioritize test cases by calculating the Regression Risk of a change. This takes into account that not all features of a software are used the same. We successfully validate our approach in a case study of an industry project which develops a complex process integration platform.",
    "published_in": "SWQD 2020: Software Quality: Quality Intelligence in Software and Systems Engineering",
    "publisher": "Springer",
    "source": "Query",
    "doi": "https://doi.org/10.1007/978-3-030-35510-4_3",
    "date": "2019-12-09",
    "categories": "Orchestration technique",
    "tcp": "X",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Dataset from Terravis (375 TCs)",
    "ind_partner": "Terravis (Switzerland)",
    "ind_author": "FALSE",
    "prac_feedback": "TRUE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "risk-coverage\"-based",
    "metrics": "Accumulated covered regression risk",
    "open_challenges": "combine with other TCP/TCS strategies; study differences between minor and major releases w.r.t. regression risk."
  },
  {
    "year": 2019,
    "authors": "Shi, August; Zhao, Peiyuan; Marinov, Darko",
    "title": "Understanding and improving regression test selection in continuous integration",
    "bibtex": "shi_understanding_2019",
    "abstract": "Developers rely on regression testing in their continuous integration (CI) environment to find changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS. In this paper, we compare module-and class-level RTS techniques in a cloud-based CI environment, Travis. We also develop and evaluate a hybrid RTS technique that combines aspects of the module-and class-level RTS techniques. We evaluate all the techniques on real Travis builds. We find that the RTS techniques do save testing time compared to running all tests (RetestAll), but the percentage of time for a full build using RTS (76.0%) is not as low as found in previous work, due to the extra overhead in a cloud-based CI environment. Moreover, we inspect test failures from RetestAll builds, and although we find that RTS techniques can miss to select failed tests, these test failures are almost all flaky test failures. As such, RTS techniques provide additional value in helping developers avoid wasting time debugging failures not related to the recent code changes. Overall, our results show that RTS can be beneficial for the developers in the CI environment, and RTS not only saves time but also avoids misleading developers by flaky test failures.",
    "published_in": "2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/ISSRE.2019.00031",
    "date": "2020-02-10",
    "categories": "",
    "tcp": "",
    "tcs": "X",
    "tsr": "",
    "tsa": "",
    "context": "Cloud-based continuous integration",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source Java projects (up to 335 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "change-based",
    "metrics": "Selection rate, time savings",
    "open_challenges": "n/a"
  },
  {
    "year": 2020,
    "authors": "Zhou, Zhi Quan; Liu, Chen; Chen, Tsong Yueh; Tse, T. H.; Susilo, Willy",
    "title": "Beating Random Test Case Prioritization",
    "bibtex": "zhou_beating_2020",
    "abstract": "Existing test case prioritization (TCP) techniques have limitations when\n applied to real-world projects, because these techniques require \ncertain information to be made available before they can be applied. For\n example, the family of input-based TCP techniques are based on test \ncase values or test script strings; other techniques use test coverage, \ntest history, program structure, or requirements information. Existing \ntechniques also cannot guarantee to always be more effective than random\n prioritization (RP) that does not have any precondition. As a result, \nRP remains the most applicable and most fundamental TCP technique. This \narticle proposes an extremely simple, effective, and efficient way to \nprioritize test cases through the introduction of a dispersity metric. \nOur technique is as applicable as RP. We conduct empirical studies using\n 43 different versions of 15 real-world projects. Empirical results show\n that our technique is more effective than RP. Our algorithm has a \nlinear computational complexity and, therefore, provides a practical \nsolution to the problem of prioritizing very large test suites (such as \nthose containing hundreds of thousands, or millions, of test cases), \nwhere the execution time of conventional nonlinear prioritization \nalgorithms can be prohibitive. Our technique also provides a practical \nsolution to TCP when neither input-based nor execution-based techniques \nare applicable due to lack of information.",
    "published_in": "IEEE Transactions on Reliability",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/TR.2020.2979815",
    "date": "2020-06-16",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Major open-source multi language projects, including Firefox (480575 TCs) and SQLite (787530 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "Similarity-based",
    "metrics": "Applicability, APFD, F-measure, time till first failure",
    "open_challenges": ""
  },
  {
    "year": 2020,
    "authors": "Peng, Qianyang; Shi, August; Zhang, Lingming",
    "title": "Empirically revisiting and enhancing IR-based test-case prioritization",
    "bibtex": "peng_empirically_2020",
    "abstract": "Test-case prioritization (TCP) aims to detect regression bugs faster via reordering the tests run. While TCP has been studied for over 20 years, it was almost always evaluated using seeded faults/mutants as opposed to using real test failures. In this work, we study the recent change-aware information retrieval (IR) technique for TCP. Prior work has shown it performing better than traditional coverage-based TCP techniques, but it was only evaluated on a small-scale dataset with a cost-unaware metric based on seeded faults/mutants. We extend the prior work by conducting a much larger and more realistic evaluation as well as proposing enhancements that substantially improve the performance. In particular, we evaluate the original technique on a large-scale, real-world software-evolution dataset with real failures using both cost-aware and cost-unaware metrics under various configurations. Also, we design and evaluate hybrid techniques combining the IR features, historical test execution time, and test failure frequencies. Our results show that the change-aware IR technique outperforms stateof-the-art coverage-based techniques in this real-world setting, and our hybrid techniques improve even further upon the original IR technique. Moreover, we show that flaky tests have a substantial impact on evaluating the change-aware TCP techniques based on real test failures.",
    "published_in": "ISSTA 2020: Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis",
    "publisher": "ACM",
    "source": "forward",
    "doi": "https://doi.org/10.1145/3395363.3397383",
    "date": "2020-07-18",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source Java projects (up to 144 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "https://sites.google.com/view/ir-based-tcp",
    "included": "TRUE",
    "approach": "Information retrieval; the idea is that code changes can be used to construct queries which in turn lead to related tests by finding textual similarities (e.g. variable names, function calls).",
    "metrics": "APFD, APFDc",
    "open_challenges": ""
  },
  {
    "year": 2020,
    "authors": "Dirim, Sahin; Sozer, Hasan",
    "title": "Prioritization of Test Cases with Varying Test Costs and Fault Severities for Certification Testing",
    "bibtex": "dirim_prioritization_2020",
    "abstract": "We present an industrial case study on the application of test case prioritization techniques in the context of certification testing in consumer electronics domain. Test execution times and fault severities are subject to high variations in this domain. As a result, most of the existing techniques and metrics turn out to be inappropriate for this application context. We discuss such deficiencies and the room for improvement based on our case study with the certification test suites of 3 Smart TV applications as real experimental objects. We also propose a new metric, LAPFD, which is based on the calculation of the average of the percentage of faults detected. This calculation is weighted according to the cost of test cases and calculated separately per severity class. Then, a lexicographic ordering is performed based on these classes. We compared the baseline (random) ordering of test cases with respect to an alternative ordering based on cost, measured as the test execution time. These alternative orderings are evaluated by using the LAPFD metric. We observed that cost-based ordering of test cases consistently outperformed random ordering. Another observation is that there is a large room for improvement regarding the effectiveness of test case prioritization in this application domain.",
    "published_in": "2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",
    "publisher": "IEEE",
    "source": "forward",
    "doi": "https://doi.org/10.1109/ICSTW50294.2020.00069",
    "date": "2020-08-04",
    "categories": "case study",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "consumer electronics (Smart tv)",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Smart TV applications for Netflix, YouTube and Prime Video (up to 213 TCs â but up to 40 hours)",
    "ind_partner": "Vestel Electronics (Turkey)",
    "ind_author": "TRUE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "MAYBE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "very simple, prioritize test cases based on their execution time (shortest first)",
    "metrics": "LAPFD (variant of APFD the considers cost for each of 4 severity classes)",
    "open_challenges": "consider other TCP techniques as lexographic or history based"
  },
  {
    "year": 2017,
    "authors": "Chi, Zongzheng; Xuan, Jifeng; Ren, Zhilei; Xie, Xiaoyuan; Guo, He",
    "title": "Multi-Level Random Walk for Software Test Suite Reduction",
    "bibtex": "chi_multi-level_2017",
    "abstract": "Which test cases should be selected to save the time of software testing? Due to the large time cost of running all test cases, it is necessary to run representative test cases to shorten the software development cycle. Test suite reduction, an NP-hard problem in software engineering, aims to select a subset of test cases to reduce the time cost of test execution in satisfying test requirements. Recently, search based software engineering provides a new direction to test suite reduction by connecting software engineering problems with computational intelligence methods. In this paper, we propose a multi-level optimization algorithm to simplify the original problem instance of test suite reduction. In each level, we search for local optimal solutions with random walk in potential subsets of the test suite. The problem scale is reduced by locking the intersection of local optima and by discarding shielded test cases with no contribution to test requirements. We compare our algorithm with state-of-the-art methods on test suites of ten large-scale open source projects. Experiments show that our algorithm can more efficiently find optima on five out of six projects, in which Integer Linear Programming (ILP) can find optima; for the other four projects that ILP fails to solve, our algorithm provides the best solutions among heuristics in comparison. Â© 2017 IEEE.",
    "published_in": "IEEE Computational Intelligence Magazine",
    "publisher": "IEEE",
    "source": "query refresh",
    "doi": "https://doi.org/10.1109/MCI.2017.2670460",
    "date": "2017-04-12",
    "categories": "",
    "tcp": "",
    "tcs": "",
    "tsr": "X",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Open-source Java projects (up to 6196 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "http://cstar.whu.edu.cn/p/multi-walk/",
    "included": "TRUE",
    "approach": "search-based",
    "metrics": "reduction rate, runtime",
    "open_challenges": ""
  },
  {
    "year": 2020,
    "authors": "Prado Lima, Jackson A.; Vergilio, Silvia R.",
    "title": "Multi-Armed Bandit Test Case Prioritization in Continuous Integration Environments: A Trade-off Analysis",
    "bibtex": "lima_multi-armed_2020",
    "abstract": "Continuous Integration (CI) practices lead the software to be integrated and tested many times a day, usually subject to a test budget. To deal with this scenario, cost-effective test case prioritization techniques are required. COLEMAN is a Multi-Armed Bandit approach that learns from the test case failure-history the best prioritization order to maximize early fault detection. Reported results show that COLEMAN has reached promising results with different test budgets and spends, in the worst case, less than one second to execute. However, COLEMAN has not been evaluated against a search-based approach. Such an approach can generate near-optimal solutions but is not suitable to the CI budget because it takes too long to execute. Considering this fact, this paper analyses the trade-offs of the COLEMAN solutions in comparison with the near-optimal solutions generated by a Genetic Algorithm (GA). We use measures, which better fit with time constraints: Normalized Average Percentage of Faults Detected (NAPFD), Root-Mean-Square-Error (RMSE), and Prioritization Time. We use seven large-scale real-world software systems, and three different test budgets, 10%, 50%, and 80% of the total time required to execute the test set available for a CI cycle. COLEMAN obtains solutions near to the GA solutions in 90% of the cases, but scenarios with high volatility of test cases and a small number of cycles hamper the prioritization.",
    "published_in": "SAST 20: Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing",
    "publisher": "ACM",
    "source": "query refresh",
    "doi": "https://doi.org/10.1145/3425174.3425210",
    "date": "2020-10-20",
    "categories": "",
    "tcp": "X",
    "tcs": "",
    "tsr": "",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "TRUE",
    "exp_subjects": "Open-source programs (up to 2391 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "reinforcement learning based",
    "metrics": "NAPFD, RMSE (root-mean-square-error), prioritization time",
    "open_challenges": ""
  },
  {
    "year": 2018,
    "authors": "Shi, August; Gyori, Alex; Mahmood, Suleman; Zhao, Peiyuan; Marinov, Darko",
    "title": "Evaluating test-suite reduction in real software evolution",
    "bibtex": "shi_evaluating_2018",
    "abstract": "Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.\n\nWe perform the first extensive study of TSR using real test failures in (failed) builds that occurred for real code changes. We analyze 1478 failed builds from 32 GitHub projects that run their tests on Travis. Each failed build can have multiple faults, so we propose a family of mappings from test failures to faults. We use these mappings to compute Failed-Build Detection Loss (FBDL), the percentage of failed builds where the reduced test suite misses to detect all the faults detected by the original test suite. We find that FBDL can be up to 52.2%, which is higher than suggested by traditional TSR metrics. Moreover, traditional TSR metrics are not good predictors of FBDL, making it difficult for developers to decide whether to use reduced test suites.",
    "published_in": "ISSTA 2018: Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis",
    "publisher": "ACM",
    "source": "author suggestion",
    "doi": "https://doi.org/10.1145/3213846.3213875",
    "date": "2018-07-12",
    "categories": "",
    "tcp": "",
    "tcs": "",
    "tsr": "X",
    "tsa": "",
    "context": "",
    "ind_motivation": "TRUE",
    "ind_evaluation": "FALSE",
    "exp_subjects": "Open-source programs (up to 770 TCs)",
    "ind_partner": "",
    "ind_author": "FALSE",
    "prac_feedback": "FALSE",
    "avai_tool": "FALSE",
    "put_practice": "FALSE",
    "suppl_url": "",
    "included": "TRUE",
    "approach": "coverage-based",
    "metrics": "FBDL (failed build detection loss)",
    "open_challenges": ""
  }
]